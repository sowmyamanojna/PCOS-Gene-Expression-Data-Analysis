{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_recon_vals_pca_ica_nmf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyCsje/9DxOVGQS89JhX2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyamanojna/CS6024-Algorithmic-Approaches-to-Computational-Biology-Project/blob/master/save_recon_vals_pca_ica_nmf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SPy7kQbP4Oy"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle as pkl\r\n",
        "\r\n",
        "import time\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from sklearn.metrics import log_loss\r\n",
        "from sklearn.decomposition import PCA, NMF, FastICA\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSJmGSnwQC0q"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9-ZNQRkQDjM"
      },
      "source": [
        "# Setting up the possible latent dimensions\r\n",
        "# A total of 27 latent dimensions are taken under consideration\r\n",
        "\r\n",
        "k_list = []\r\n",
        "k_list.extend(list(range(2, 10)))\r\n",
        "k_list.extend(list(range(10, 20, 2)))\r\n",
        "k_list.extend(list(range(20, 50, 5)))\r\n",
        "k_list.extend(list(range(50, 61, 10)))\r\n",
        "k_list.append(78)\r\n",
        "k_list.extend(list(range(80, 100, 10)))\r\n",
        "k_list.extend(list(range(100, 176, 25)))\r\n",
        "\r\n",
        "print(\"Latent dimensions:\")\r\n",
        "print(k_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9uPn1yHQLt1"
      },
      "source": [
        "# Read the data. The df_new file has additional information about each sample\r\n",
        "# such as the following:\r\n",
        "#   - PCOS/Control\r\n",
        "#   - Cell type\r\n",
        "#   - Dataset they belong to\r\n",
        "\r\n",
        "common_norm_df = pd.read_csv('/content/drive/MyDrive/aacb_project/datasets/common_normalized.csv', index_col=0)\r\n",
        "df_new = pd.read_csv(\"/content/drive/MyDrive/aacb_project/datasets/control_pcos_celltype_mapping.csv\")\r\n",
        "df_new.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKrcRwrjQPIi"
      },
      "source": [
        "# Rearrange the rows in df_new to ensure that the \r\n",
        "position = []\r\n",
        "values = list(df_new[\"sample_id\"])\r\n",
        "for i,j in enumerate(common_norm_df[\"sample_id\"]):\r\n",
        "    position.append(values.index(j))\r\n",
        "\r\n",
        "df_new = df_new.loc[position]\r\n",
        "df_new = df_new.reset_index()\r\n",
        "df_new = df_new.drop(\"index\", axis=1)\r\n",
        "\r\n",
        "# Merge the two dataframes together\r\n",
        "result = pd.merge(common_norm_df, df_new[df_new.columns[:-1]], how='inner', on='sample_id')\r\n",
        "result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tc_iRk-QR6w"
      },
      "source": [
        "def get_cost_reconstruction(X, model, k_list=k_list):\r\n",
        "    bce_loss = []\r\n",
        "    l2_error = []\r\n",
        "    output = {}\r\n",
        "    \r\n",
        "    print(\"Calculating Reconstruction Error for:\", model.upper())\r\n",
        "    time.sleep(1)\r\n",
        "    \r\n",
        "    if model == \"pca\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            model = PCA(n_components=k, random_state=4)\r\n",
        "            model.fit(X)\r\n",
        "            reduced = model.transform(X)\r\n",
        "            reconstructed = model.inverse_transform(reduced)\r\n",
        "            bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "            l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "            output[k] = reconstructed\r\n",
        "\r\n",
        "    if model == \"ica\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            model = FastICA(n_components=k, random_state=4, max_iter=400)\r\n",
        "            model.fit(X)\r\n",
        "            reduced = model.transform(X)\r\n",
        "            reconstructed = model.inverse_transform(reduced)\r\n",
        "            bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "            l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "            output[k] = reconstructed\r\n",
        "\r\n",
        "    if model == \"nmf\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            model = NMF(n_components=k, random_state=4, max_iter=400)\r\n",
        "            model.fit(X)\r\n",
        "            reduced = model.transform(X)\r\n",
        "            reconstructed = model.inverse_transform(reduced)\r\n",
        "            bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "            l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "            output[k] = reconstructed\r\n",
        "    \r\n",
        "    return bce_loss, l2_error, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNEzOiP_QXf3"
      },
      "source": [
        "model_list = [\"pca\", \"ica\", \"nmf\"]\r\n",
        "bce_loss = []\r\n",
        "l2_error = []\r\n",
        "reconstruction_list = {}\r\n",
        "\r\n",
        "X = common_norm_df[common_norm_df.columns[1:-1]].to_numpy()\r\n",
        "\r\n",
        "for model in model_list:\r\n",
        "    bce, l2, output = get_cost_reconstruction(X, model)\r\n",
        "    bce_loss.append(bce)\r\n",
        "    l2_error.append(l2)\r\n",
        "    reconstruction_list[model] = output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCb5KeEOQZhu"
      },
      "source": [
        "z_dict = {}\r\n",
        "for k in k_list:\r\n",
        "  z_dict[k] = {}\r\n",
        "  for algo in model_list:\r\n",
        "    z_dict[k][algo] = reconstruction_list[algo][k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4y3OJWGQy_z"
      },
      "source": [
        "with open('/content/drive/MyDrive/aacb_project/datasets/z_dict_pca_ica_nmf.p', 'wb') as f:\r\n",
        "  pkl.dump(z_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RutkD8Q5S1AQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}