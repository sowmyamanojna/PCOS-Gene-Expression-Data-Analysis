{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import optimizers, metrics\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input, Dense, Lambda, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tybalt.utils.vae_utils\n",
    "def approx_keras_binary_cross_entropy(x, z, p, epsilon=1e-07):\n",
    "    \"\"\"\n",
    "    Function to approximate Keras `binary_crossentropy()`\n",
    "    https://github.com/keras-team/keras/blob/e6c3f77b0b10b0d76778109a40d6d3282f1cadd0/keras/losses.py#L76\n",
    "    Which is a wrapper for TensorFlow `sigmoid_cross_entropy_with_logits()`\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "    An important step is to clip values of reconstruction\n",
    "    https://github.com/keras-team/keras/blob/a3d160b9467c99cbb27f9aa0382c759f45c8ee66/keras/backend/tensorflow_backend.py#L3071\n",
    "    Arguments:\n",
    "    x - Reconstructed input RNAseq data\n",
    "    z - Input RNAseq data\n",
    "    p - number of features\n",
    "    epsilon - the clipping value to stabilize results (same Keras default)\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    x = np.array(x)\n",
    "    z = np.array(z)\n",
    "\n",
    "    # Add clip to value\n",
    "    x[x < epsilon] = epsilon\n",
    "    x[x > (1 - epsilon)] = (1 - epsilon)\n",
    "\n",
    "    # Perform logit\n",
    "    x = np.log(x / (1 - x))\n",
    "\n",
    "    # Return approximate binary cross entropy\n",
    "    return np.mean(p * np.mean(- x * z + np.log(1 + np.exp(x)), axis=-1))\n",
    "\n",
    "\n",
    "class VariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, var_layer, mean_layer, original_dim, beta, loss,\n",
    "                 **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        self.var_layer = var_layer\n",
    "        self.mean_layer = mean_layer\n",
    "        self.original_dim = original_dim\n",
    "        self.beta = beta\n",
    "        self.loss = loss\n",
    "        super(VariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        if self.loss == 'binary_crossentropy':\n",
    "            recon_loss = self.original_dim * \\\n",
    "                         metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        elif self.loss == 'mse':\n",
    "            recon_loss = self.original_dim * \\\n",
    "                         metrics.mean_squared_error(x_input, x_decoded)\n",
    "\n",
    "        kl_loss = - 0.5 * K.sum(1 + self.var_layer -\n",
    "                                K.square(self.mean_layer) -\n",
    "                                K.exp(self.var_layer), axis=-1)\n",
    "\n",
    "        return K.mean(recon_loss + (K.get_value(self.beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, x_decoded = inputs\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Behavior on each epoch\n",
    "        \"\"\"\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\n",
    "\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, training_data, original_dim, encoder_cbk, decoder_cbk):\n",
    "        self.training_data = training_data\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder_cbk = encoder_cbk\n",
    "        self.decoder_cbk = decoder_cbk\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.xent_loss = []\n",
    "        self.kl_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        recon = self.decoder_cbk.predict(\n",
    "            self.encoder_cbk.predict(self.training_data))\n",
    "        xent_loss = approx_keras_binary_cross_entropy(x=recon,\n",
    "                                                      z=self.training_data,\n",
    "                                                      p=self.original_dim)\n",
    "        full_loss = logs.get('loss')\n",
    "        self.xent_loss.append(xent_loss)\n",
    "        self.kl_loss.append(full_loss - xent_loss)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tybalt.utils.base\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_summary(self):\n",
    "        self.full_model.summary()\n",
    "\n",
    "    def visualize_architecture(self, output_file):\n",
    "        # Visualize the connections of the custom VAE model\n",
    "        plot_model(self.full_model, to_file=output_file)\n",
    "\n",
    "    def visualize_training(self, output_file=None):\n",
    "        # Visualize training performance\n",
    "        history_df = pd.DataFrame(self.hist.history)\n",
    "        ax = history_df.plot()\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Loss')\n",
    "        fig = ax.get_figure()\n",
    "        if output_file:\n",
    "            fig.savefig(output_file)\n",
    "        else:\n",
    "            fig.show()\n",
    "\n",
    "    def get_weights(self, decoder=True):\n",
    "        # Extract weight matrices from encoder or decoder\n",
    "        weights = []\n",
    "        if decoder:\n",
    "            for layer in self.decoder.layers:\n",
    "                weights.append(layer.get_weights())\n",
    "        else:\n",
    "            for layer in self.encoder.layers:\n",
    "                # Encoder weights must be transposed\n",
    "                encoder_weights = layer.get_weights()\n",
    "                encoder_weights = [np.transpose(x) for x in encoder_weights]\n",
    "                weights.append(encoder_weights)\n",
    "        return weights\n",
    "\n",
    "    def save_models(self, encoder_file, decoder_file):\n",
    "        self.encoder.save(encoder_file)\n",
    "        self.decoder.save(decoder_file)\n",
    "\n",
    "\n",
    "class VAE(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "\n",
    "    def _sampling(self, args):\n",
    "        \"\"\"\n",
    "        Function for reparameterization trick to make model differentiable\n",
    "        \"\"\"\n",
    "        # Function with args required for Keras Lambda function\n",
    "        z_mean, z_log_var = args\n",
    "\n",
    "        # Draw epsilon of the same shape from a standard normal distribution\n",
    "        epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                                  stddev=self.epsilon_std)\n",
    "\n",
    "        # The latent vector is non-deterministic and differentiable\n",
    "        # in respect to z_mean and z_log_var\n",
    "        z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "        return z\n",
    "\n",
    "    def initialize_model(self):\n",
    "        \"\"\"\n",
    "        Helper function to run that builds and compiles Keras layers\n",
    "        \"\"\"\n",
    "        self._build_encoder_layer()\n",
    "        self._build_decoder_layer()\n",
    "        self._compile_vae()\n",
    "        self._connect_layers()\n",
    "\n",
    "    def compress(self, df):\n",
    "        # Encode rnaseq into the hidden/latent representation - and save output\n",
    "        # a cVAE expects a list of [rnaseq_df, y_df]\n",
    "        encoded_df = self.encoder.predict_on_batch(df)\n",
    "\n",
    "        if self.model_name == 'cTybalt':\n",
    "            named_index = df[0].index\n",
    "        else:\n",
    "            named_index = df.index\n",
    "\n",
    "        encoded_df = pd.DataFrame(encoded_df,\n",
    "                                  columns=range(1, self.latent_dim + 1),\n",
    "                                  index=named_index)\n",
    "        return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tybalt(VAE):\n",
    "    \"\"\"\n",
    "    Training and evaluation of a tybalt model\n",
    "    Usage: from tybalt.models import Tybalt\n",
    "    \"\"\"\n",
    "    def __init__(self, original_dim, latent_dim, batch_size=50, epochs=50,\n",
    "                 learning_rate=0.0005, kappa=1, epsilon_std=1.0,\n",
    "                 beta=K.variable(0), loss='binary_crossentropy',\n",
    "                 verbose=True):\n",
    "        VAE.__init__(self)\n",
    "        self.model_name = 'Tybalt'\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kappa = kappa\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.beta = beta\n",
    "        self.loss = loss\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _build_encoder_layer(self):\n",
    "        \"\"\"\n",
    "        Function to build the encoder layer connections\n",
    "        \"\"\"\n",
    "        # Input place holder for RNAseq data with specific input size\n",
    "        self.rnaseq_input = Input(shape=(self.original_dim, ))\n",
    "\n",
    "        # Input layer is compressed into a mean and log variance vector of\n",
    "        # size `latent_dim`. Each layer is initialized with glorot uniform\n",
    "        # weights and each step (dense connections, batch norm, and relu\n",
    "        # activation) are funneled separately.\n",
    "        # Each vector are connected to the rnaseq input tensor\n",
    "\n",
    "        # input layer to latent mean layer\n",
    "        z_mean = Dense(self.latent_dim,\n",
    "                       kernel_initializer='glorot_uniform')(self.rnaseq_input)\n",
    "        z_mean_batchnorm = BatchNormalization()(z_mean)\n",
    "        self.z_mean_encoded = Activation('relu')(z_mean_batchnorm)\n",
    "\n",
    "        # input layer to latent standard deviation layer\n",
    "        z_var = Dense(self.latent_dim,\n",
    "                      kernel_initializer='glorot_uniform')(self.rnaseq_input)\n",
    "        z_var_batchnorm = BatchNormalization()(z_var)\n",
    "        self.z_var_encoded = Activation('relu')(z_var_batchnorm)\n",
    "\n",
    "        # return the encoded and randomly sampled z vector\n",
    "        # Takes two keras layers as input to the custom sampling function layer\n",
    "        self.z = Lambda(self._sampling,\n",
    "                        output_shape=(self.latent_dim, ))([self.z_mean_encoded,\n",
    "                                                           self.z_var_encoded])\n",
    "\n",
    "    def _build_decoder_layer(self):\n",
    "        \"\"\"\n",
    "        Function to build the decoder layer connections\n",
    "        \"\"\"\n",
    "        # The decoding layer is much simpler with a single layer glorot uniform\n",
    "        # initialized and sigmoid activation\n",
    "        self.decoder_model = Sequential()\n",
    "        self.decoder_model.add(Dense(self.original_dim, activation='sigmoid',\n",
    "                                     input_dim=self.latent_dim))\n",
    "        self.rnaseq_reconstruct = self.decoder_model(self.z)\n",
    "\n",
    "    def _compile_vae(self):\n",
    "        \"\"\"\n",
    "        Creates the vae layer and compiles all layer connections\n",
    "        \"\"\"\n",
    "        adam = optimizers.Adam(lr=self.learning_rate)\n",
    "        vae_layer = VariationalLayer(var_layer=self.z_var_encoded,\n",
    "                                     mean_layer=self.z_mean_encoded,\n",
    "                                     original_dim=self.original_dim,\n",
    "                                     beta=self.beta, loss=self.loss)(\n",
    "                                [self.rnaseq_input, self.rnaseq_reconstruct])\n",
    "        self.full_model = Model(self.rnaseq_input, vae_layer)\n",
    "        self.full_model.compile(optimizer=adam, loss=None,\n",
    "                                loss_weights=[self.beta])\n",
    "\n",
    "    def _connect_layers(self):\n",
    "        \"\"\"\n",
    "        Make connections between layers to build separate encoder and decoder\n",
    "        \"\"\"\n",
    "        self.encoder = Model(self.rnaseq_input, self.z_mean_encoded)\n",
    "\n",
    "        decoder_input = Input(shape=(self.latent_dim, ))\n",
    "        _x_decoded_mean = self.decoder_model(decoder_input)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    def train_vae(self, train_df, test_df, separate_loss=False):\n",
    "        \"\"\"\n",
    "        Method to train model.\n",
    "        `separate_loss` instantiates a custom Keras callback that tracks the\n",
    "        separate contribution of reconstruction and KL divergence loss. Because\n",
    "        VAEs try to minimize both, it may be informative to track each across\n",
    "        training separately. The callback processes the training data through\n",
    "        the current encoder and decoder and therefore requires additional time\n",
    "        - which is why this is not done by default.\n",
    "        \"\"\"\n",
    "        cbks = [WarmUpCallback(self.beta, self.kappa)]\n",
    "        if separate_loss:\n",
    "            tybalt_loss_cbk = LossCallback(training_data=np.array(train_df),\n",
    "                                           encoder_cbk=self.encoder,\n",
    "                                           decoder_cbk=self.decoder,\n",
    "                                           original_dim=self.original_dim)\n",
    "            cbks += [tybalt_loss_cbk]\n",
    "\n",
    "        self.hist = self.full_model.fit(np.array(train_df),\n",
    "                                        shuffle=True,\n",
    "                                        epochs=self.epochs,\n",
    "                                        batch_size=self.batch_size,\n",
    "                                        verbose=self.verbose,\n",
    "                                        validation_data=(np.array(test_df),\n",
    "                                                         None),\n",
    "                                        callbacks=cbks)\n",
    "        self.history_df = pd.DataFrame(self.hist.history)\n",
    "\n",
    "        if separate_loss:\n",
    "            self.history_df = self.history_df.assign(\n",
    "                                recon=tybalt_loss_cbk.xent_loss)\n",
    "            self.history_df = self.history_df.assign(\n",
    "                                kl=tybalt_loss_cbk.kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>27</th>\n",
       "      <th>36</th>\n",
       "      <th>59</th>\n",
       "      <th>87</th>\n",
       "      <th>94</th>\n",
       "      <th>105</th>\n",
       "      <th>153</th>\n",
       "      <th>164</th>\n",
       "      <th>159</th>\n",
       "      <th>5936_100526737</th>\n",
       "      <th>...</th>\n",
       "      <th>254359</th>\n",
       "      <th>254531</th>\n",
       "      <th>100132341</th>\n",
       "      <th>100287932_100652748</th>\n",
       "      <th>387893</th>\n",
       "      <th>388336</th>\n",
       "      <th>259266</th>\n",
       "      <th>317762</th>\n",
       "      <th>261726</th>\n",
       "      <th>PCOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.125461</td>\n",
       "      <td>0.336077</td>\n",
       "      <td>0.044463</td>\n",
       "      <td>0.267819</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.370576</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.906437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.601428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312354</td>\n",
       "      <td>0.198387</td>\n",
       "      <td>0.77081</td>\n",
       "      <td>0.120213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214607</td>\n",
       "      <td>0.487085</td>\n",
       "      <td>0.589704</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476058</td>\n",
       "      <td>0.631829</td>\n",
       "      <td>0.356537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56419</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.325390</td>\n",
       "      <td>0.850926</td>\n",
       "      <td>0.356499</td>\n",
       "      <td>0.173660</td>\n",
       "      <td>0.430645</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1668 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         27        36        59        87        94       105       153  \\\n",
       "0  0.693258  0.125461  0.336077  0.044463  0.267819  0.467742  0.490196   \n",
       "1  0.214607  0.487085  0.589704  0.104294  0.000000  0.106452  0.000000   \n",
       "\n",
       "        164       159  5936_100526737  ...   254359    254531  100132341  \\\n",
       "0  0.370576  0.008907        0.906437  ...  0.39619  1.000000   0.412466   \n",
       "1  0.476058  0.631829        0.356537  ...  0.56419  0.497418   0.325390   \n",
       "\n",
       "   100287932_100652748    387893    388336    259266   317762    261726  PCOS  \n",
       "0             0.601428  0.000000  0.312354  0.198387  0.77081  0.120213     1  \n",
       "1             0.850926  0.356499  0.173660  0.430645  0.00000  1.000000     1  \n",
       "\n",
       "[2 rows x 1668 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>27</th>\n",
       "      <th>36</th>\n",
       "      <th>59</th>\n",
       "      <th>87</th>\n",
       "      <th>94</th>\n",
       "      <th>105</th>\n",
       "      <th>153</th>\n",
       "      <th>164</th>\n",
       "      <th>159</th>\n",
       "      <th>5936_100526737</th>\n",
       "      <th>...</th>\n",
       "      <th>254359</th>\n",
       "      <th>254531</th>\n",
       "      <th>100132341</th>\n",
       "      <th>100287932_100652748</th>\n",
       "      <th>387893</th>\n",
       "      <th>388336</th>\n",
       "      <th>259266</th>\n",
       "      <th>317762</th>\n",
       "      <th>261726</th>\n",
       "      <th>PCOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.312483</td>\n",
       "      <td>0.288471</td>\n",
       "      <td>0.551965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294203</td>\n",
       "      <td>0.313646</td>\n",
       "      <td>0.635799</td>\n",
       "      <td>0.175577</td>\n",
       "      <td>0.198168</td>\n",
       "      <td>0.025395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285876</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.210907</td>\n",
       "      <td>0.339472</td>\n",
       "      <td>0.674464</td>\n",
       "      <td>0.310908</td>\n",
       "      <td>0.228302</td>\n",
       "      <td>0.416071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.943789</td>\n",
       "      <td>0.471579</td>\n",
       "      <td>0.321965</td>\n",
       "      <td>0.800309</td>\n",
       "      <td>0.963302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.817316</td>\n",
       "      <td>0.596338</td>\n",
       "      <td>0.378297</td>\n",
       "      <td>0.323593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791928</td>\n",
       "      <td>0.314794</td>\n",
       "      <td>0.688564</td>\n",
       "      <td>0.501324</td>\n",
       "      <td>0.704366</td>\n",
       "      <td>0.980853</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.803842</td>\n",
       "      <td>0.400209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1668 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           27        36        59        87        94       105       153  \\\n",
       "127  0.312483  0.288471  0.551965  1.000000  0.294203  0.313646  0.635799   \n",
       "133  0.943789  0.471579  0.321965  0.800309  0.963302  1.000000  0.817316   \n",
       "\n",
       "          164       159  5936_100526737  ...    254359    254531  100132341  \\\n",
       "127  0.175577  0.198168        0.025395  ...  0.285876  0.547722   0.484291   \n",
       "133  0.596338  0.378297        0.323593  ...  0.791928  0.314794   0.688564   \n",
       "\n",
       "     100287932_100652748    387893    388336    259266    317762    261726  \\\n",
       "127             0.210907  0.339472  0.674464  0.310908  0.228302  0.416071   \n",
       "133             0.501324  0.704366  0.980853  0.435757  0.803842  0.400209   \n",
       "\n",
       "     PCOS  \n",
       "127     1  \n",
       "133     1  \n",
       "\n",
       "[2 rows x 1668 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcos_df = pd.read_csv('datasets/common_normalized.csv', index_col=0)\n",
    "pcos_df = pcos_df.drop(['sample_id'], axis=1)\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "\n",
    "pcos_test_df = pcos_df.sample(frac=test_set_percent)\n",
    "pcos_train_df = pcos_df.drop(pcos_test_df.index)\n",
    "\n",
    "display(pcos_train_df.head(2))\n",
    "display(pcos_test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = pcos_df.shape[1]\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Tybalt(original_dim, latent_dim)\n",
    "vae._build_encoder_layer()\n",
    "vae._build_decoder_layer()\n",
    "vae._compile_vae()\n",
    "vae._connect_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "vae.train_vae(pcos_train_df, pcos_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69325843,  0.12546125,  0.33607726, ...,  0.77080958,\n",
       "         0.12021277,  1.        ],\n",
       "       [ 0.21460674,  0.48708487,  0.58970362, ...,  0.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 0.24157303,  0.44649446,  0.54820251, ...,  0.67616876,\n",
       "         0.67340426,  1.        ],\n",
       "       ...,\n",
       "       [ 0.26667339,  0.78317817,  0.4877807 , ...,  0.6485062 ,\n",
       "         0.67157515, -1.        ],\n",
       "       [ 0.13267101,  0.81251818,  0.49090723, ...,  0.50479235,\n",
       "         1.        , -1.        ],\n",
       "       [ 0.21921504,  0.9359994 ,  0.58971787, ...,  0.23880753,\n",
       "         0.87264117, -1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pcos_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
