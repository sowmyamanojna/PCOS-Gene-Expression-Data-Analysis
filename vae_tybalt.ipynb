{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-tybalt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIRi4D6dcXnjdz3IYy7bEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyamanojna/CS6024-Algorithmic-Approaches-to-Computational-Biology-Project/blob/master/vae_tybalt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMiziQtgk38"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import io\r\n",
        "\r\n",
        "import keras\r\n",
        "import tensorflow as tf\r\n",
        "from keras.layers import Input, Dense, Lambda, Layer, Activation\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "from keras import backend as K\r\n",
        "from keras import metrics, optimizers\r\n",
        "from keras.callbacks import Callback\r\n",
        "from tensorflow.keras import losses\r\n",
        "\r\n",
        "import pydot\r\n",
        "import graphviz\r\n",
        "from keras.utils import plot_model\r\n",
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "\r\n",
        "import plotly.express as px\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmkR0amNhErD"
      },
      "source": [
        "%matplotlib inline\r\n",
        "plt.style.use('seaborn-notebook')\r\n",
        "sns.set(style=\"white\", color_codes=True)\r\n",
        "sns.set_context(\"paper\", rc={\"font.size\":14,\"axes.titlesize\":15,\"axes.labelsize\":20,'xtick.labelsize':14, 'ytick.labelsize':14})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQUPJ2QyhHPd"
      },
      "source": [
        "# Function for reparameterization trick to make model differentiable\r\n",
        "def sampling(args):\r\n",
        "    \r\n",
        "    import tensorflow as tf\r\n",
        "    # Function with args required for Keras Lambda function\r\n",
        "    z_mean, z_log_var = args\r\n",
        "\r\n",
        "    # Draw epsilon of the same shape from a standard normal distribution\r\n",
        "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\r\n",
        "                              stddev=epsilon_std)\r\n",
        "    \r\n",
        "    # The latent vector is non-deterministic and differentiable\r\n",
        "    # in respect to z_mean and z_log_var\r\n",
        "    z = z_mean + K.exp(z_log_var / 2) * epsilon\r\n",
        "    return z\r\n",
        "\r\n",
        "class CustomVariationalLayer(Layer):\r\n",
        "    \"\"\"\r\n",
        "    Define a custom layer that learns and performs the training\r\n",
        "    This function is borrowed from:\r\n",
        "    https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        # https://keras.io/layers/writing-your-own-keras-layers/\r\n",
        "        self.is_placeholder = True\r\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def vae_loss(self, x_input, x_decoded):\r\n",
        "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\r\n",
        "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \r\n",
        "                                K.exp(z_log_var_encoded), axis=-1)\r\n",
        "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x = inputs[0]\r\n",
        "        x_decoded = inputs[1]\r\n",
        "        loss = self.vae_loss(x, x_decoded)\r\n",
        "        self.add_loss(loss, inputs=inputs)\r\n",
        "        # We won't actually use the output.\r\n",
        "        return x\r\n",
        "\r\n",
        "class WarmUpCallback(Callback):\r\n",
        "    def __init__(self, beta, kappa):\r\n",
        "        self.beta = beta\r\n",
        "        self.kappa = kappa\r\n",
        "    # Behavior on each epoch\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if K.get_value(self.beta) <= 1:\r\n",
        "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\r\n",
        "\r\n",
        "def get_model_summary(model):\r\n",
        "    stream = io.StringIO()\r\n",
        "    model.summary(print_fn=lambda x: stream.write(x + '\\n'))\r\n",
        "    summary_string = stream.getvalue()\r\n",
        "    stream.close()\r\n",
        "    return summary_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXAlLr7DhZ92"
      },
      "source": [
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4cAazMFi2Er"
      },
      "source": [
        "pcos_df = pd.read_csv('common_normalized.csv')\r\n",
        "pcos_df = pcos_df.drop(['sample_id'], axis=1)\r\n",
        "# Split 10% test set randomly\r\n",
        "test_set_percent = 0.1\r\n",
        "pcos_test_df = pcos_df.sample(frac=test_set_percent)\r\n",
        "pcos_train_df = pcos_df.drop(pcos_test_df.index)\r\n",
        "print(pcos_train_df.head(2))\r\n",
        "print(pcos_test_df.head(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDtGXeR6i245"
      },
      "source": [
        "# Set hyper parameters\r\n",
        "original_dim = pcos_df.shape[1]\r\n",
        "latent_dim = 100\r\n",
        "\r\n",
        "batch_size = 4\r\n",
        "# epochs = 50\r\n",
        "epochs = 10\r\n",
        "learning_rate = 0.0005\r\n",
        "\r\n",
        "epsilon_std = 1.0\r\n",
        "beta = K.variable(0)\r\n",
        "kappa = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9-5mbSYi5T6"
      },
      "source": [
        "# Input place holder for PCOS data with specific input size\r\n",
        "pcos_input = Input(shape=(original_dim, ))\r\n",
        "\r\n",
        "# Input layer is compressed into a mean and log variance vector of size `latent_dim`\r\n",
        "# Each layer is initialized with glorot uniform weights and each step (dense connections,\r\n",
        "# batch norm, and relu activation) are funneled separately\r\n",
        "# Each vector of length `latent_dim` are connected to the input tensor\r\n",
        "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(pcos_input)\r\n",
        "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\r\n",
        "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\r\n",
        "\r\n",
        "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(pcos_input)\r\n",
        "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\r\n",
        "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\r\n",
        "\r\n",
        "# return the encoded and randomly sampled z vector\r\n",
        "# Takes two keras layers as input to the custom sampling function layer with a `latent_dim` output\r\n",
        "z = Lambda(sampling, output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHcYEpui7rZ"
      },
      "source": [
        "# The decoding layer is much simpler with a single layer and sigmoid activation\r\n",
        "decoder_to_reconstruct = Dense(original_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\r\n",
        "pcos_reconstruct = decoder_to_reconstruct(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTZypEmsi-JG"
      },
      "source": [
        "adam = optimizers.Adam(lr=learning_rate)\r\n",
        "vae_layer = CustomVariationalLayer()([pcos_input, pcos_reconstruct])\r\n",
        "vae = Model(pcos_input, vae_layer)\r\n",
        "# vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\r\n",
        "vae.compile(optimizer=adam, loss=losses.BinaryCrossentropy, loss_weights=[beta])\r\n",
        "\r\n",
        "vae_model_summary_string = get_model_summary(vae)\r\n",
        "\r\n",
        "with open('vae_model.txt', 'a') as f:\r\n",
        "  f.write(vae_model_summary_string)\r\n",
        "  f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMEhTp5MjDYX"
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)\r\n",
        "hist = vae.fit(np.asarray(pcos_train_df).astype('float32'),\r\n",
        "               shuffle=True,\r\n",
        "               epochs=epochs,\r\n",
        "               batch_size=batch_size,\r\n",
        "               validation_data=(np.asarray(pcos_test_df).astype('float32'), None),\r\n",
        "               callbacks=[WarmUpCallback(beta, kappa)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDA3W-1jG9C"
      },
      "source": [
        "# Visualize training performance\r\n",
        "history_df = pd.DataFrame(hist.history)\r\n",
        "ax = history_df.plot()\r\n",
        "ax.set_xlabel('Epochs')\r\n",
        "ax.set_ylabel('VAE Loss')\r\n",
        "fig = ax.get_figure()\r\n",
        "fig.savefig(\"hist_plot_file.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pScIcGPjjJ-K"
      },
      "source": [
        "# Model to compress input\r\n",
        "encoder = Model(pcos_input, z_mean_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISGhZX9ejOC8"
      },
      "source": [
        "# Encode into the hidden/latent representation - and save output\r\n",
        "encoded_pcos_df = encoder.predict_on_batch(pcos_df)\r\n",
        "encoded_pcos_df = pd.DataFrame(encoded_pcos_df, index=pcos_df.index)\r\n",
        "\r\n",
        "encoded_pcos_df.columns.name = 'sample_id'\r\n",
        "encoded_pcos_df.columns = encoded_pcos_df.columns + 1\r\n",
        "encoded_file = 'encoded_pcos_onehidden_warmup_batchnorm.tsv'\r\n",
        "encoded_pcos_df.to_csv(encoded_file, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyDVfh2xjQeF"
      },
      "source": [
        "# build a generator that can sample from the learned distribution\r\n",
        "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\r\n",
        "_x_decoded_mean = decoder_to_reconstruct(decoder_input)\r\n",
        "decoder = Model(decoder_input, _x_decoded_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW_XYsodjSxI"
      },
      "source": [
        "encoder_model_file = 'encoder_onehidden_vae.hdf5'\r\n",
        "decoder_model_file = 'decoder_onehidden_vae.hdf5'\r\n",
        "\r\n",
        "encoder.save(encoder_model_file)\r\n",
        "decoder.save(decoder_model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwmJU51pjUfv"
      },
      "source": [
        "# What are the most and least activated nodes\r\n",
        "sum_node_activity = encoded_pcos_df.sum(axis=0).sort_values(ascending=False)\r\n",
        "\r\n",
        "with open('10_most_and_least_activity_nodes.txt', 'a') as f:\r\n",
        "  f.write(sum_node_activity.head(10).to_string())\r\n",
        "  f.write('\\n')\r\n",
        "  f.write(sum_node_activity.tail(10).to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsXQZnuSjWph"
      },
      "source": [
        "# Histogram of node activity for all 100 latent features\r\n",
        "sum_node_activity.hist()\r\n",
        "plt.xlabel('Activation Sum')\r\n",
        "plt.ylabel('Count')\r\n",
        "plt.savefig('node_activity_hist.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qr_zpLUjYnT"
      },
      "source": [
        "# Example of node activation distribution for the first two latent features\r\n",
        "plt.figure(figsize=(6, 6))\r\n",
        "plt.scatter(encoded_pcos_df.iloc[:, 1], encoded_pcos_df.iloc[:, 2])\r\n",
        "plt.xlabel('Latent Feature 1')\r\n",
        "plt.ylabel('Latent Feature 2')\r\n",
        "plt.savefig('node_activation_2_latent.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWVsn6JOjav0"
      },
      "source": [
        "# How well does the model reconstruct the input data\r\n",
        "input_pcos_reconstruct = decoder.predict(np.array(encoded_pcos_df))\r\n",
        "input_pcos_reconstruct = pd.DataFrame(input_pcos_reconstruct, index=pcos_df.index,\r\n",
        "                                        columns=pcos_df.columns)\r\n",
        "input_pcos_reconstruct.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tv2cV5zjcuq"
      },
      "source": [
        "reconstruction_fidelity = pcos_df - input_pcos_reconstruct\r\n",
        "\r\n",
        "gene_mean = reconstruction_fidelity.mean(axis=0)\r\n",
        "gene_abssum = reconstruction_fidelity.abs().sum(axis=0).divide(pcos_df.shape[0])\r\n",
        "gene_summary = pd.DataFrame([gene_mean, gene_abssum], index=['gene mean', 'gene abs(sum)']).T\r\n",
        "gene_summary.sort_values(by='gene abs(sum)', ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmIz6r--jfAb"
      },
      "source": [
        "# Mean of gene reconstruction vs. absolute reconstructed difference per sample\r\n",
        "g = sns.jointplot('gene mean', 'gene abs(sum)', data=gene_summary)\r\n",
        "g.savefig('mean_gene_reconstructed_abs.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtMRwOjijhC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMPzFQsAyN0S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}