{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_weights_dae_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTs5M_9hWIsO"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle as pkl\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras import backend as K\r\n",
        "from keras import optimizers, metrics\r\n",
        "from keras.layers import Input, Dense, Lambda, Activation, Dropout, Layer\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.merge import concatenate\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.regularizers import l1\r\n",
        "from keras import activations\r\n",
        "from keras import backend as K\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.callbacks import Callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCb-Mlz_WC6C",
        "outputId": "92517dc9-380f-4c09-ab39-a6ca41dd772e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTvnfbuXZ0IW"
      },
      "source": [
        "###########################################################\r\n",
        "############## Autoencoder Utility Classes ################\r\n",
        "###########################################################\r\n",
        "\r\n",
        "# From tybalt.utils.base\r\n",
        "class BaseModel():\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def get_summary(self):\r\n",
        "        self.full_model.summary()\r\n",
        "\r\n",
        "    def visualize_architecture(self, output_file):\r\n",
        "        # Visualize the connections of the custom VAE model\r\n",
        "        plot_model(self.full_model, to_file=output_file)\r\n",
        "\r\n",
        "    def visualize_training(self, output_file=None):\r\n",
        "        # Visualize training performance\r\n",
        "        history_df = pd.DataFrame(self.hist.history)\r\n",
        "        ax = history_df.plot()\r\n",
        "        ax.set_xlabel('Epochs')\r\n",
        "        ax.set_ylabel('Loss')\r\n",
        "        fig = ax.get_figure()\r\n",
        "        if output_file:\r\n",
        "            fig.savefig(output_file)\r\n",
        "        else:\r\n",
        "            fig.show()\r\n",
        "\r\n",
        "    def get_weights(self, decoder=True):\r\n",
        "        # Extract weight matrices from encoder or decoder\r\n",
        "        weights = []\r\n",
        "        if decoder:\r\n",
        "            for layer in self.decoder.layers:\r\n",
        "                weights.append(layer.get_weights())\r\n",
        "        else:\r\n",
        "            for layer in self.encoder.layers:\r\n",
        "                # Encoder weights must be transposed\r\n",
        "                encoder_weights = layer.get_weights()\r\n",
        "                encoder_weights = [np.transpose(x) for x in encoder_weights]\r\n",
        "                weights.append(encoder_weights)\r\n",
        "        return weights\r\n",
        "\r\n",
        "    def save_models(self, encoder_file, decoder_file):\r\n",
        "        self.encoder.save(encoder_file)\r\n",
        "        self.decoder.save(decoder_file)\r\n",
        "\r\n",
        "\r\n",
        "# From tybalt.utils.vae_utils\r\n",
        "def approx_keras_binary_cross_entropy(x, z, p, epsilon=1e-07):\r\n",
        "    # Ensure numpy arrays\r\n",
        "    x = np.array(x)\r\n",
        "    z = np.array(z)\r\n",
        "\r\n",
        "    # Add clip to value\r\n",
        "    x[x < epsilon] = epsilon\r\n",
        "    x[x > (1 - epsilon)] = (1 - epsilon)\r\n",
        "\r\n",
        "    # Perform logit\r\n",
        "    x = np.log(x / (1 - x))\r\n",
        "\r\n",
        "    # Return approximate binary cross entropy\r\n",
        "    return np.mean(p * np.mean(- x * z + np.log(1 + np.exp(x)), axis=-1))\r\n",
        "\r\n",
        "\r\n",
        "class WarmUpCallback(Callback):\r\n",
        "    def __init__(self, beta, kappa):\r\n",
        "        self.beta = beta\r\n",
        "        self.kappa = kappa\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if K.get_value(self.beta) <= 1:\r\n",
        "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\r\n",
        "\r\n",
        "\r\n",
        "class LossCallback(Callback):\r\n",
        "    def __init__(self, training_data, original_dim, encoder_cbk, decoder_cbk):\r\n",
        "        self.training_data = training_data\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.encoder_cbk = encoder_cbk\r\n",
        "        self.decoder_cbk = decoder_cbk\r\n",
        "\r\n",
        "    def on_train_begin(self, logs={}):\r\n",
        "        self.xent_loss = []\r\n",
        "        self.kl_loss = []\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        recon = self.decoder_cbk.predict(self.encoder_cbk.predict(self.training_data))\r\n",
        "        xent_loss = approx_keras_binary_cross_entropy(x=recon, z=self.training_data, p=self.original_dim)\r\n",
        "        full_loss = logs.get('loss')\r\n",
        "        self.xent_loss.append(xent_loss)\r\n",
        "        self.kl_loss.append(full_loss - xent_loss)\r\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P9QJ2L7WmIM"
      },
      "source": [
        "###########################################################\r\n",
        "############## Denoising Autoencoder (DAE) ################\r\n",
        "###########################################################\r\n",
        "\r\n",
        "# From tybalt.utils.adage_utils\r\n",
        "class TiedWeightsDecoder(Layer):\r\n",
        "    def __init__(self, output_dim, encoder, activation=None, **kwargs):\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.encoder = encoder\r\n",
        "        self.activation = activations.get(activation)\r\n",
        "        super(TiedWeightsDecoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.kernel = self.encoder.weights\r\n",
        "        super(TiedWeightsDecoder, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x):\r\n",
        "        # Encoder weights: [weight_matrix, bias_term]\r\n",
        "        output = K.dot(x - self.encoder.weights[1], K.transpose(self.encoder.weights[0]))\r\n",
        "        if self.activation is not None:\r\n",
        "            output = self.activation(output)\r\n",
        "        return output\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return (input_shape[0], self.output_dim)\r\n",
        "\r\n",
        "\r\n",
        "class Adage(BaseModel):\r\n",
        "    def __init__(self, original_dim, latent_dim, noise=0.05, batch_size=50,\r\n",
        "                 epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\r\n",
        "                 optimizer='adam', tied_weights=True, verbose=True):\r\n",
        "        BaseModel.__init__(self)\r\n",
        "        self.model_name = 'ADAGE'\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        self.noise = noise\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.sparsity = sparsity\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.loss = loss\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.tied_weights = tied_weights\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def _build_graph(self):\r\n",
        "        # Build the Keras graph for an ADAGE model\r\n",
        "        self.input_rnaseq = Input(shape=(self.original_dim, ))\r\n",
        "        drop = Dropout(self.noise)(self.input_rnaseq)\r\n",
        "        self.encoded = Dense(self.latent_dim, activity_regularizer=l1(self.sparsity))(drop)\r\n",
        "        activation = Activation('relu')(self.encoded)\r\n",
        "        decoded_rnaseq = Dense(self.original_dim, activation='sigmoid')(activation)\r\n",
        "\r\n",
        "        self.full_model = Model(self.input_rnaseq, decoded_rnaseq)\r\n",
        "\r\n",
        "    def _build_tied_weights_graph(self):\r\n",
        "        # Build Keras graph for an ADAGE model with tied weights\r\n",
        "        self.encoded = Dense(self.latent_dim, input_shape=(self.original_dim, ), activity_regularizer=l1(self.sparsity), activation='relu')\r\n",
        "        dropout_layer = Dropout(self.noise)\r\n",
        "        self.tied_decoder = TiedWeightsDecoder(input_shape=(self.latent_dim, ), output_dim=self.original_dim, activation='sigmoid', encoder=self.encoded)\r\n",
        "        self.full_model = Sequential()\r\n",
        "        self.full_model.add(self.encoded)\r\n",
        "        self.full_model.add(dropout_layer)\r\n",
        "        self.full_model.add(self.tied_decoder)\r\n",
        "\r\n",
        "    def _compile_adage(self):\r\n",
        "        # Compile the autoencoder to prepare for training\r\n",
        "        if self.optimizer == 'adadelta':\r\n",
        "            optim = optimizers.Adadelta(lr=self.learning_rate)\r\n",
        "        elif self.optimizer == 'adam':\r\n",
        "            optim = optimizers.Adam(lr=self.learning_rate)\r\n",
        "        self.full_model.compile(optimizer=optim, loss=self.loss)\r\n",
        "\r\n",
        "    def _connect_layers(self):\r\n",
        "        # Separate out the encoder and decoder model\r\n",
        "        encoded_input = Input(shape=(self.latent_dim, ))\r\n",
        "        decoder_layer = self.full_model.layers[-1]\r\n",
        "        self.decoder = Model(encoded_input, decoder_layer(encoded_input))\r\n",
        "\r\n",
        "        if self.tied_weights:\r\n",
        "            # The keras graph is built differently for a tied weight model\r\n",
        "            # Build a model with input and output Tensors of the encoded layer\r\n",
        "            self.encoder = Model(self.encoded.input, self.encoded.output)\r\n",
        "        else:\r\n",
        "            self.encoder = Model(self.input_rnaseq, self.encoded)\r\n",
        "\r\n",
        "    def initialize_model(self):\r\n",
        "        if self.tied_weights:\r\n",
        "            self._build_tied_weights_graph()\r\n",
        "        else:\r\n",
        "            self._build_graph()\r\n",
        "        self._connect_layers()\r\n",
        "        self._compile_adage()\r\n",
        "\r\n",
        "    def train_adage(self, train_df, test_df, adage_comparable_loss=False):\r\n",
        "        self.hist = self.full_model.fit(np.array(train_df), np.array(train_df),\r\n",
        "                                        shuffle=True,\r\n",
        "                                        epochs=self.epochs,\r\n",
        "                                        verbose=self.verbose,\r\n",
        "                                        batch_size=self.batch_size,\r\n",
        "                                        validation_data=(np.array(test_df),\r\n",
        "                                                         np.array(test_df)))\r\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\r\n",
        "\r\n",
        "        # ADAGE loss is a mean over all features - to make this value more\r\n",
        "        # comparable to the VAE reconstruciton loss, multiply by num genes\r\n",
        "        if adage_comparable_loss:\r\n",
        "            self.history_df = self.history_df * self.original_dim\r\n",
        "\r\n",
        "    def compress(self, df):\r\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\r\n",
        "        encoded_df = self.encoder.predict(np.array(df))\r\n",
        "        encoded_df = pd.DataFrame(encoded_df, index=df.index, columns=range(1, self.latent_dim + 1))\r\n",
        "        return encoded_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S1yjAJ4WcIn",
        "outputId": "3f571c9b-cd67-4429-e39e-daa8541255b6"
      },
      "source": [
        "###########################################################\r\n",
        "############## Variational Autoencoder (VAE) ##############\r\n",
        "###########################################################\r\n",
        "\r\n",
        "class VariationalLayer(Layer):\r\n",
        "    def __init__(self, var_layer, mean_layer, original_dim, beta, loss, **kwargs):\r\n",
        "        self.is_placeholder = True\r\n",
        "        self.var_layer = var_layer\r\n",
        "        self.mean_layer = mean_layer\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.beta = beta\r\n",
        "        self.loss = loss\r\n",
        "        super(VariationalLayer, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def vae_loss(self, x_input, x_decoded):\r\n",
        "        if self.loss == 'binary_crossentropy':\r\n",
        "            recon_loss = self.original_dim * \\\r\n",
        "                         metrics.binary_crossentropy(x_input, x_decoded)\r\n",
        "        elif self.loss == 'mse':\r\n",
        "            recon_loss = self.original_dim * \\\r\n",
        "                         metrics.mean_squared_error(x_input, x_decoded)\r\n",
        "\r\n",
        "        kl_loss = - 0.5 * K.sum(1 + self.var_layer -\r\n",
        "                                K.square(self.mean_layer) -\r\n",
        "                                K.exp(self.var_layer), axis=-1)\r\n",
        "\r\n",
        "        return K.mean(recon_loss + (K.get_value(self.beta) * kl_loss))\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x, x_decoded = inputs\r\n",
        "        loss = self.vae_loss(x, x_decoded)\r\n",
        "        self.add_loss(loss, inputs=inputs)\r\n",
        "        # We won't actually use the output.\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class VAE(BaseModel):\r\n",
        "    def __init__(self):\r\n",
        "        BaseModel.__init__(self)\r\n",
        "\r\n",
        "    def _sampling(self, args):\r\n",
        "        # Function with args required for Keras Lambda function\r\n",
        "        z_mean, z_log_var = args\r\n",
        "\r\n",
        "        # Draw epsilon of the same shape from a standard normal distribution\r\n",
        "        epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0., stddev=self.epsilon_std)\r\n",
        "\r\n",
        "        # The latent vector is non-deterministic and differentiable\r\n",
        "        # in respect to z_mean and z_log_var\r\n",
        "        z = z_mean + K.exp(z_log_var / 2) * epsilon\r\n",
        "        return z\r\n",
        "\r\n",
        "    def initialize_model(self):\r\n",
        "        self._build_encoder_layer()\r\n",
        "        self._build_decoder_layer()\r\n",
        "        self._compile_vae()\r\n",
        "        self._connect_layers()\r\n",
        "\r\n",
        "    def compress(self, df):\r\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\r\n",
        "        # a cVAE expects a list of [rnaseq_df, y_df]\r\n",
        "        encoded_df = self.encoder.predict_on_batch(df)\r\n",
        "\r\n",
        "        if self.model_name == 'cTybalt':\r\n",
        "            named_index = df[0].index\r\n",
        "        else:\r\n",
        "            named_index = df.index\r\n",
        "\r\n",
        "        encoded_df = pd.DataFrame(encoded_df, columns=range(1, self.latent_dim + 1), index=named_index)\r\n",
        "        return encoded_df\r\n",
        "\r\n",
        "\r\n",
        "class Tybalt(VAE):\r\n",
        "    def __init__(self, original_dim, latent_dim, batch_size=50, epochs=50,\r\n",
        "                 learning_rate=0.0005, kappa=1, epsilon_std=1.0,\r\n",
        "                 beta=K.variable(0), loss='binary_crossentropy',\r\n",
        "                 verbose=True):\r\n",
        "        VAE.__init__(self)\r\n",
        "        self.model_name = 'Tybalt'\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.kappa = kappa\r\n",
        "        self.epsilon_std = epsilon_std\r\n",
        "        self.beta = beta\r\n",
        "        self.loss = loss\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def _build_encoder_layer(self):\r\n",
        "        # Input place holder for RNAseq data with specific input size\r\n",
        "        self.rnaseq_input = Input(shape=(self.original_dim, ))\r\n",
        "\r\n",
        "        # Input layer is compressed into a mean and log variance vector of\r\n",
        "        # size `latent_dim`. Each layer is initialized with glorot uniform\r\n",
        "        # weights and each step (dense connections, batch norm, and relu\r\n",
        "        # activation) are funneled separately.\r\n",
        "        # Each vector are connected to the rnaseq input tensor\r\n",
        "\r\n",
        "        # input layer to latent mean layer\r\n",
        "        z_mean = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(self.rnaseq_input)\r\n",
        "        z_mean_batchnorm = BatchNormalization()(z_mean)\r\n",
        "        self.z_mean_encoded = Activation('relu')(z_mean_batchnorm)\r\n",
        "\r\n",
        "        # input layer to latent standard deviation layer\r\n",
        "        z_var = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(self.rnaseq_input)\r\n",
        "        z_var_batchnorm = BatchNormalization()(z_var)\r\n",
        "        self.z_var_encoded = Activation('relu')(z_var_batchnorm)\r\n",
        "\r\n",
        "        # return the encoded and randomly sampled z vector\r\n",
        "        # Takes two keras layers as input to the custom sampling function layer\r\n",
        "        self.z = Lambda(self._sampling, output_shape=(self.latent_dim, ))([self.z_mean_encoded, self.z_var_encoded])\r\n",
        "\r\n",
        "    def _build_decoder_layer(self):\r\n",
        "        # The decoding layer is much simpler with a single layer glorot uniform\r\n",
        "        # initialized and sigmoid activation\r\n",
        "        self.decoder_model = Sequential()\r\n",
        "        self.decoder_model.add(Dense(self.original_dim, activation='sigmoid', input_dim=self.latent_dim))\r\n",
        "        self.rnaseq_reconstruct = self.decoder_model(self.z)\r\n",
        "\r\n",
        "    def _compile_vae(self):\r\n",
        "        adam = optimizers.Adam(lr=self.learning_rate)\r\n",
        "        vae_layer = VariationalLayer(var_layer=self.z_var_encoded,\r\n",
        "                                     mean_layer=self.z_mean_encoded,\r\n",
        "                                     original_dim=self.original_dim,\r\n",
        "                                     beta=self.beta, loss=self.loss)([self.rnaseq_input, self.rnaseq_reconstruct])\r\n",
        "        self.full_model = Model(self.rnaseq_input, vae_layer)\r\n",
        "        self.full_model.compile(optimizer=adam, loss=None,\r\n",
        "                                loss_weights=[self.beta])\r\n",
        "\r\n",
        "    def _connect_layers(self):\r\n",
        "        self.encoder = Model(self.rnaseq_input, self.z_mean_encoded)\r\n",
        "\r\n",
        "        decoder_input = Input(shape=(self.latent_dim, ))\r\n",
        "        _x_decoded_mean = self.decoder_model(decoder_input)\r\n",
        "        self.decoder = Model(decoder_input, _x_decoded_mean)\r\n",
        "\r\n",
        "    def train_vae(self, train_df, test_df, separate_loss=False):\r\n",
        "        cbks = [WarmUpCallback(self.beta, self.kappa)]\r\n",
        "        if separate_loss:\r\n",
        "            tybalt_loss_cbk = LossCallback(training_data=np.array(train_df), encoder_cbk=self.encoder, decoder_cbk=self.decoder, original_dim=self.original_dim)\r\n",
        "            cbks += [tybalt_loss_cbk]\r\n",
        "\r\n",
        "        self.hist = self.full_model.fit(np.array(train_df),\r\n",
        "                                        shuffle=True,\r\n",
        "                                        epochs=self.epochs,\r\n",
        "                                        batch_size=self.batch_size,\r\n",
        "                                        verbose=self.verbose,\r\n",
        "                                        validation_data=(np.array(test_df), None),\r\n",
        "                                        callbacks=cbks)\r\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\r\n",
        "\r\n",
        "        if separate_loss:\r\n",
        "            self.history_df = self.history_df.assign(recon=tybalt_loss_cbk.xent_loss)\r\n",
        "            self.history_df = self.history_df.assign(kl=tybalt_loss_cbk.kl_loss)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3aasjR8WTjE",
        "outputId": "eb99eab3-fd04-4c5e-cca7-a004e32d13ce"
      },
      "source": [
        "# Setting up the possible latent dimensions\r\n",
        "# A total of 27 latent dimensions are taken under consideration\r\n",
        "\r\n",
        "k_list = []\r\n",
        "k_list.extend(list(range(2, 10)))\r\n",
        "k_list.extend(list(range(10, 20, 2)))\r\n",
        "k_list.extend(list(range(20, 50, 5)))\r\n",
        "k_list.extend(list(range(50, 61, 10)))\r\n",
        "k_list.append(78)\r\n",
        "k_list.extend(list(range(80, 100, 10)))\r\n",
        "k_list.extend(list(range(100, 176, 25)))\r\n",
        "\r\n",
        "print(\"Latent dimensions:\")\r\n",
        "print(k_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latent dimensions:\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 35, 40, 45, 50, 60, 78, 80, 90, 100, 125, 150, 175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr2I3etObGq3"
      },
      "source": [
        "pcos_df = pd.read_csv('/content/drive/MyDrive/aacb_project/datasets/common_normalized.csv', index_col=0)\r\n",
        "pcos_df = pcos_df.drop([\"sample_id\", \"PCOS\"], axis=1)\r\n",
        "original_dim = pcos_df.shape[1]\r\n",
        "\r\n",
        "# Split 10% test set randomly\r\n",
        "test_set_percent = 0.1\r\n",
        "\r\n",
        "data_type = [\"Training\", \"Testing\"]*len(k_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-EVGbLmbKDR"
      },
      "source": [
        "# VAE Training and Saving Weights\r\n",
        "\r\n",
        "pcos_test_df = pcos_df.sample(frac=test_set_percent)\r\n",
        "pcos_train_df = pcos_df.drop(pcos_test_df.index)\r\n",
        "\r\n",
        "vae_df_hist = []\r\n",
        "vae_k_list_new = []\r\n",
        "vae_encoded_list = []\r\n",
        "vae_decoded_list = []\r\n",
        "vae_weights_list = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    vae = Tybalt(original_dim, latent_dim, batch_size=50, epochs=200,\r\n",
        "                    learning_rate=0.005, kappa=1, epsilon_std=1.0,\r\n",
        "                    beta=K.variable(0), loss='binary_crossentropy',\r\n",
        "                    verbose=False)\r\n",
        "    vae._build_encoder_layer()\r\n",
        "    vae._build_decoder_layer()\r\n",
        "    vae._compile_vae()\r\n",
        "    vae._connect_layers()\r\n",
        "\r\n",
        "    vae.train_vae(pcos_train_df, pcos_test_df, separate_loss=False)\r\n",
        "\r\n",
        "    # Append the loss dataframe\r\n",
        "    vae_df_hist.append(vae.history_df)\r\n",
        "\r\n",
        "    # Append the encoded and decoded matrix for the training data\r\n",
        "    vae_encoded_list.append(vae.encoder.predict(pcos_train_df))\r\n",
        "    vae_decoded_list.append(vae.decoder.predict(vae_encoded_list[-1]))\r\n",
        "    \r\n",
        "    # Append the encoded and decoded matrix for the testing data\r\n",
        "    vae_encoded_list.append(vae.encoder.predict(pcos_test_df))\r\n",
        "    vae_decoded_list.append(vae.decoder.predict(vae_encoded_list[-1]))\r\n",
        "\r\n",
        "    # Append the weights\r\n",
        "    vae_weights_list.append(vae.get_weights()[1])\r\n",
        "\r\n",
        "    vae_k_list_new.append(latent_dim)\r\n",
        "    vae_k_list_new.append(latent_dim)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCd1YLkuin-v"
      },
      "source": [
        "vae_z_dict = {}\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  vae_z_dict[k] = vae_weights_list[i][0]\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/aacb_project/datasets/z_dict_vae.p', 'wb') as f:\r\n",
        "  pkl.dump(vae_z_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nb2mpEfcbpE",
        "outputId": "89d7fbab-3667-4be4-dfcc-a2c54bee5d89"
      },
      "source": [
        "# DAE Training and Saving Weights\r\n",
        "\r\n",
        "pcos_test_df = pcos_df.sample(frac=test_set_percent)\r\n",
        "pcos_train_df = pcos_df.drop(pcos_test_df.index)\r\n",
        "\r\n",
        "dae_df_hist = []\r\n",
        "dae_k_list_new = []\r\n",
        "dae_encoded_list = []\r\n",
        "dae_decoded_list = []\r\n",
        "dae_weights_list = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    dae = Adage(original_dim, latent_dim, noise=0.05, batch_size=50,\r\n",
        "                    epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\r\n",
        "                    optimizer='adam', tied_weights=True, verbose=False)\r\n",
        "    dae._build_graph()\r\n",
        "    dae._build_tied_weights_graph()\r\n",
        "    dae._compile_adage()\r\n",
        "    dae._connect_layers()\r\n",
        "    dae.initialize_model()\r\n",
        "    dae.train_adage(pcos_train_df, pcos_test_df)\r\n",
        "\r\n",
        "    # Append the loss dataframe\r\n",
        "    dae_df_hist.append(dae.history_df)\r\n",
        "\r\n",
        "    # Append the encoded and decoded matrix for the training data\r\n",
        "    dae_encoded_list.append(dae.encoder.predict(pcos_train_df))\r\n",
        "    dae_decoded_list.append(dae.decoder.predict(dae_encoded_list[-1]))\r\n",
        "    \r\n",
        "    # Append the encoded and decoded matrix for the testing data\r\n",
        "    dae_encoded_list.append(dae.encoder.predict(pcos_test_df))\r\n",
        "    dae_decoded_list.append(dae.decoder.predict(dae_encoded_list[-1]))\r\n",
        "\r\n",
        "    # Append the weights \r\n",
        "    # The weights are of dimensions (orig_dim, latent_dim)\r\n",
        "    dae_weights_list.append(dae.get_weights()[1])\r\n",
        "\r\n",
        "    dae_k_list_new.append(latent_dim)\r\n",
        "    dae_k_list_new.append(latent_dim)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [06:33<00:00, 14.06s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lafKrQQOeoCA"
      },
      "source": [
        "dae_z_dict = {}\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  dae_z_dict[k] = dae_weights_list[i][0].transpose()\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/aacb_project/datasets/z_dict_dae.p', 'wb') as f:\r\n",
        "  pkl.dump(dae_z_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rG_Rgs8lqsf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}