{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save-weights-pcos-control.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m92eUxT4P21B"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle as pkl\r\n",
        "\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from sklearn.metrics import log_loss\r\n",
        "from sklearn.decomposition import PCA, NMF, FastICA\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras import backend as K\r\n",
        "from keras import optimizers, metrics\r\n",
        "from keras.layers import Input, Dense, Lambda, Activation, Dropout, Layer\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.merge import concatenate\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.regularizers import l1\r\n",
        "from keras import activations\r\n",
        "from keras import backend as K\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.callbacks import Callback"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1egTogAGaRdw",
        "outputId": "ae801cdf-77f5-48a2-c50f-ed7b5712297d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1UXRk4iFLHW"
      },
      "source": [
        "###########################################################\r\n",
        "###################### PCA, ICA, NMF ######################\r\n",
        "###########################################################\r\n",
        "\r\n",
        "def get_cost_reconstruction(X, model, k_list):\r\n",
        "    bce_loss = []\r\n",
        "    l2_error = []\r\n",
        "    output = {}\r\n",
        "    \r\n",
        "    print(\"Calculating Reconstruction Error for:\", model.upper())\r\n",
        "    time.sleep(1)\r\n",
        "    \r\n",
        "    if model == \"pca\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            if k <= len(X):\r\n",
        "              model = PCA(n_components=k, random_state=4)\r\n",
        "              model.fit(X)\r\n",
        "              reduced = model.transform(X)\r\n",
        "              reconstructed = model.inverse_transform(reduced)\r\n",
        "              bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "              l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "              output[k] = model.components_\r\n",
        "\r\n",
        "    if model == \"ica\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            if k <= len(X):\r\n",
        "              model = FastICA(n_components=k, random_state=4, max_iter=400)\r\n",
        "              model.fit(X)\r\n",
        "              reduced = model.transform(X)\r\n",
        "              reconstructed = model.inverse_transform(reduced)\r\n",
        "              bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "              l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "              output[k] = model.components_\r\n",
        "\r\n",
        "    if model == \"nmf\":\r\n",
        "        for k in tqdm(k_list):\r\n",
        "            if k <= len(X):\r\n",
        "              model = NMF(n_components=k, random_state=4, max_iter=400)\r\n",
        "              model.fit(X)\r\n",
        "              reduced = model.transform(X)\r\n",
        "              reconstructed = model.inverse_transform(reduced)\r\n",
        "              bce_loss.append(log_loss(X.reshape(-1,).astype(int), reconstructed.reshape(-1,)))\r\n",
        "              l2_error.append(np.linalg.norm(X-reconstructed))\r\n",
        "              output[k] = model.components_\r\n",
        "    \r\n",
        "    return bce_loss, l2_error, output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c938vg5KQkCq"
      },
      "source": [
        "###########################################################\r\n",
        "############## Autoencoder Utility Classes ################\r\n",
        "###########################################################\r\n",
        "\r\n",
        "# From tybalt.utils.base\r\n",
        "class BaseModel():\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def get_summary(self):\r\n",
        "        self.full_model.summary()\r\n",
        "\r\n",
        "    def visualize_architecture(self, output_file):\r\n",
        "        # Visualize the connections of the custom VAE model\r\n",
        "        plot_model(self.full_model, to_file=output_file)\r\n",
        "\r\n",
        "    def visualize_training(self, output_file=None):\r\n",
        "        # Visualize training performance\r\n",
        "        history_df = pd.DataFrame(self.hist.history)\r\n",
        "        ax = history_df.plot()\r\n",
        "        ax.set_xlabel('Epochs')\r\n",
        "        ax.set_ylabel('Loss')\r\n",
        "        fig = ax.get_figure()\r\n",
        "        if output_file:\r\n",
        "            fig.savefig(output_file)\r\n",
        "        else:\r\n",
        "            fig.show()\r\n",
        "\r\n",
        "    def get_weights(self, decoder=True):\r\n",
        "        # Extract weight matrices from encoder or decoder\r\n",
        "        weights = []\r\n",
        "        if decoder:\r\n",
        "            for layer in self.decoder.layers:\r\n",
        "                weights.append(layer.get_weights())\r\n",
        "        else:\r\n",
        "            for layer in self.encoder.layers:\r\n",
        "                # Encoder weights must be transposed\r\n",
        "                encoder_weights = layer.get_weights()\r\n",
        "                encoder_weights = [np.transpose(x) for x in encoder_weights]\r\n",
        "                weights.append(encoder_weights)\r\n",
        "        return weights\r\n",
        "\r\n",
        "    def save_models(self, encoder_file, decoder_file):\r\n",
        "        self.encoder.save(encoder_file)\r\n",
        "        self.decoder.save(decoder_file)\r\n",
        "\r\n",
        "\r\n",
        "# From tybalt.utils.vae_utils\r\n",
        "def approx_keras_binary_cross_entropy(x, z, p, epsilon=1e-07):\r\n",
        "    # Ensure numpy arrays\r\n",
        "    x = np.array(x)\r\n",
        "    z = np.array(z)\r\n",
        "\r\n",
        "    # Add clip to value\r\n",
        "    x[x < epsilon] = epsilon\r\n",
        "    x[x > (1 - epsilon)] = (1 - epsilon)\r\n",
        "\r\n",
        "    # Perform logit\r\n",
        "    x = np.log(x / (1 - x))\r\n",
        "\r\n",
        "    # Return approximate binary cross entropy\r\n",
        "    return np.mean(p * np.mean(- x * z + np.log(1 + np.exp(x)), axis=-1))\r\n",
        "\r\n",
        "\r\n",
        "class WarmUpCallback(Callback):\r\n",
        "    def __init__(self, beta, kappa):\r\n",
        "        self.beta = beta\r\n",
        "        self.kappa = kappa\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if K.get_value(self.beta) <= 1:\r\n",
        "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\r\n",
        "\r\n",
        "\r\n",
        "class LossCallback(Callback):\r\n",
        "    def __init__(self, training_data, original_dim, encoder_cbk, decoder_cbk):\r\n",
        "        self.training_data = training_data\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.encoder_cbk = encoder_cbk\r\n",
        "        self.decoder_cbk = decoder_cbk\r\n",
        "\r\n",
        "    def on_train_begin(self, logs={}):\r\n",
        "        self.xent_loss = []\r\n",
        "        self.kl_loss = []\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        recon = self.decoder_cbk.predict(self.encoder_cbk.predict(self.training_data))\r\n",
        "        xent_loss = approx_keras_binary_cross_entropy(x=recon, z=self.training_data, p=self.original_dim)\r\n",
        "        full_loss = logs.get('loss')\r\n",
        "        self.xent_loss.append(xent_loss)\r\n",
        "        self.kl_loss.append(full_loss - xent_loss)\r\n",
        "        return"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClU-ZKk4QlOS"
      },
      "source": [
        "###########################################################\r\n",
        "############## Denoising Autoencoder (DAE) ################\r\n",
        "###########################################################\r\n",
        "\r\n",
        "# From tybalt.utils.adage_utils\r\n",
        "class TiedWeightsDecoder(Layer):\r\n",
        "    def __init__(self, output_dim, encoder, activation=None, **kwargs):\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.encoder = encoder\r\n",
        "        self.activation = activations.get(activation)\r\n",
        "        super(TiedWeightsDecoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.kernel = self.encoder.weights\r\n",
        "        super(TiedWeightsDecoder, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x):\r\n",
        "        # Encoder weights: [weight_matrix, bias_term]\r\n",
        "        output = K.dot(x - self.encoder.weights[1], K.transpose(self.encoder.weights[0]))\r\n",
        "        if self.activation is not None:\r\n",
        "            output = self.activation(output)\r\n",
        "        return output\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return (input_shape[0], self.output_dim)\r\n",
        "\r\n",
        "\r\n",
        "class Adage(BaseModel):\r\n",
        "    def __init__(self, original_dim, latent_dim, noise=0.05, batch_size=50,\r\n",
        "                 epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\r\n",
        "                 optimizer='adam', tied_weights=True, verbose=True):\r\n",
        "        BaseModel.__init__(self)\r\n",
        "        self.model_name = 'ADAGE'\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        self.noise = noise\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.sparsity = sparsity\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.loss = loss\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.tied_weights = tied_weights\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def _build_graph(self):\r\n",
        "        # Build the Keras graph for an ADAGE model\r\n",
        "        self.input_rnaseq = Input(shape=(self.original_dim, ))\r\n",
        "        drop = Dropout(self.noise)(self.input_rnaseq)\r\n",
        "        self.encoded = Dense(self.latent_dim, activity_regularizer=l1(self.sparsity))(drop)\r\n",
        "        activation = Activation('relu')(self.encoded)\r\n",
        "        decoded_rnaseq = Dense(self.original_dim, activation='sigmoid')(activation)\r\n",
        "\r\n",
        "        self.full_model = Model(self.input_rnaseq, decoded_rnaseq)\r\n",
        "\r\n",
        "    def _build_tied_weights_graph(self):\r\n",
        "        # Build Keras graph for an ADAGE model with tied weights\r\n",
        "        self.encoded = Dense(self.latent_dim, input_shape=(self.original_dim, ), activity_regularizer=l1(self.sparsity), activation='relu')\r\n",
        "        dropout_layer = Dropout(self.noise)\r\n",
        "        self.tied_decoder = TiedWeightsDecoder(input_shape=(self.latent_dim, ), output_dim=self.original_dim, activation='sigmoid', encoder=self.encoded)\r\n",
        "        self.full_model = Sequential()\r\n",
        "        self.full_model.add(self.encoded)\r\n",
        "        self.full_model.add(dropout_layer)\r\n",
        "        self.full_model.add(self.tied_decoder)\r\n",
        "\r\n",
        "    def _compile_adage(self):\r\n",
        "        # Compile the autoencoder to prepare for training\r\n",
        "        if self.optimizer == 'adadelta':\r\n",
        "            optim = optimizers.Adadelta(lr=self.learning_rate)\r\n",
        "        elif self.optimizer == 'adam':\r\n",
        "            optim = optimizers.Adam(lr=self.learning_rate)\r\n",
        "        self.full_model.compile(optimizer=optim, loss=self.loss)\r\n",
        "\r\n",
        "    def _connect_layers(self):\r\n",
        "        # Separate out the encoder and decoder model\r\n",
        "        encoded_input = Input(shape=(self.latent_dim, ))\r\n",
        "        decoder_layer = self.full_model.layers[-1]\r\n",
        "        self.decoder = Model(encoded_input, decoder_layer(encoded_input))\r\n",
        "\r\n",
        "        if self.tied_weights:\r\n",
        "            # The keras graph is built differently for a tied weight model\r\n",
        "            # Build a model with input and output Tensors of the encoded layer\r\n",
        "            self.encoder = Model(self.encoded.input, self.encoded.output)\r\n",
        "        else:\r\n",
        "            self.encoder = Model(self.input_rnaseq, self.encoded)\r\n",
        "\r\n",
        "    def initialize_model(self):\r\n",
        "        if self.tied_weights:\r\n",
        "            self._build_tied_weights_graph()\r\n",
        "        else:\r\n",
        "            self._build_graph()\r\n",
        "        self._connect_layers()\r\n",
        "        self._compile_adage()\r\n",
        "\r\n",
        "    def train_adage(self, train_df, test_df, adage_comparable_loss=False):\r\n",
        "        self.hist = self.full_model.fit(np.array(train_df), np.array(train_df),\r\n",
        "                                        shuffle=True,\r\n",
        "                                        epochs=self.epochs,\r\n",
        "                                        verbose=self.verbose,\r\n",
        "                                        batch_size=self.batch_size,\r\n",
        "                                        validation_data=(np.array(test_df),\r\n",
        "                                                         np.array(test_df)))\r\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\r\n",
        "\r\n",
        "        # ADAGE loss is a mean over all features - to make this value more\r\n",
        "        # comparable to the VAE reconstruciton loss, multiply by num genes\r\n",
        "        if adage_comparable_loss:\r\n",
        "            self.history_df = self.history_df * self.original_dim\r\n",
        "\r\n",
        "    def compress(self, df):\r\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\r\n",
        "        encoded_df = self.encoder.predict(np.array(df))\r\n",
        "        encoded_df = pd.DataFrame(encoded_df, index=df.index, columns=range(1, self.latent_dim + 1))\r\n",
        "        return encoded_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8wae39dQq_Z",
        "outputId": "15e20002-90b4-4eb9-8c1f-7213fe814be5"
      },
      "source": [
        "###########################################################\r\n",
        "############## Variational Autoencoder (VAE) ##############\r\n",
        "###########################################################\r\n",
        "\r\n",
        "class VariationalLayer(Layer):\r\n",
        "    def __init__(self, var_layer, mean_layer, original_dim, beta, loss, **kwargs):\r\n",
        "        self.is_placeholder = True\r\n",
        "        self.var_layer = var_layer\r\n",
        "        self.mean_layer = mean_layer\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.beta = beta\r\n",
        "        self.loss = loss\r\n",
        "        super(VariationalLayer, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def vae_loss(self, x_input, x_decoded):\r\n",
        "        if self.loss == 'binary_crossentropy':\r\n",
        "            recon_loss = self.original_dim * \\\r\n",
        "                         metrics.binary_crossentropy(x_input, x_decoded)\r\n",
        "        elif self.loss == 'mse':\r\n",
        "            recon_loss = self.original_dim * \\\r\n",
        "                         metrics.mean_squared_error(x_input, x_decoded)\r\n",
        "\r\n",
        "        kl_loss = - 0.5 * K.sum(1 + self.var_layer -\r\n",
        "                                K.square(self.mean_layer) -\r\n",
        "                                K.exp(self.var_layer), axis=-1)\r\n",
        "\r\n",
        "        return K.mean(recon_loss + (K.get_value(self.beta) * kl_loss))\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x, x_decoded = inputs\r\n",
        "        loss = self.vae_loss(x, x_decoded)\r\n",
        "        self.add_loss(loss, inputs=inputs)\r\n",
        "        # We won't actually use the output.\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class VAE(BaseModel):\r\n",
        "    def __init__(self):\r\n",
        "        BaseModel.__init__(self)\r\n",
        "\r\n",
        "    def _sampling(self, args):\r\n",
        "        # Function with args required for Keras Lambda function\r\n",
        "        z_mean, z_log_var = args\r\n",
        "\r\n",
        "        # Draw epsilon of the same shape from a standard normal distribution\r\n",
        "        epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0., stddev=self.epsilon_std)\r\n",
        "\r\n",
        "        # The latent vector is non-deterministic and differentiable\r\n",
        "        # in respect to z_mean and z_log_var\r\n",
        "        z = z_mean + K.exp(z_log_var / 2) * epsilon\r\n",
        "        return z\r\n",
        "\r\n",
        "    def initialize_model(self):\r\n",
        "        self._build_encoder_layer()\r\n",
        "        self._build_decoder_layer()\r\n",
        "        self._compile_vae()\r\n",
        "        self._connect_layers()\r\n",
        "\r\n",
        "    def compress(self, df):\r\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\r\n",
        "        # a cVAE expects a list of [rnaseq_df, y_df]\r\n",
        "        encoded_df = self.encoder.predict_on_batch(df)\r\n",
        "\r\n",
        "        if self.model_name == 'cTybalt':\r\n",
        "            named_index = df[0].index\r\n",
        "        else:\r\n",
        "            named_index = df.index\r\n",
        "\r\n",
        "        encoded_df = pd.DataFrame(encoded_df, columns=range(1, self.latent_dim + 1), index=named_index)\r\n",
        "        return encoded_df\r\n",
        "\r\n",
        "\r\n",
        "class Tybalt(VAE):\r\n",
        "    def __init__(self, original_dim, latent_dim, batch_size=50, epochs=50,\r\n",
        "                 learning_rate=0.0005, kappa=1, epsilon_std=1.0,\r\n",
        "                 beta=K.variable(0), loss='binary_crossentropy',\r\n",
        "                 verbose=True):\r\n",
        "        VAE.__init__(self)\r\n",
        "        self.model_name = 'Tybalt'\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.kappa = kappa\r\n",
        "        self.epsilon_std = epsilon_std\r\n",
        "        self.beta = beta\r\n",
        "        self.loss = loss\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def _build_encoder_layer(self):\r\n",
        "        # Input place holder for RNAseq data with specific input size\r\n",
        "        self.rnaseq_input = Input(shape=(self.original_dim, ))\r\n",
        "\r\n",
        "        # Input layer is compressed into a mean and log variance vector of\r\n",
        "        # size `latent_dim`. Each layer is initialized with glorot uniform\r\n",
        "        # weights and each step (dense connections, batch norm, and relu\r\n",
        "        # activation) are funneled separately.\r\n",
        "        # Each vector are connected to the rnaseq input tensor\r\n",
        "\r\n",
        "        # input layer to latent mean layer\r\n",
        "        z_mean = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(self.rnaseq_input)\r\n",
        "        z_mean_batchnorm = BatchNormalization()(z_mean)\r\n",
        "        self.z_mean_encoded = Activation('relu')(z_mean_batchnorm)\r\n",
        "\r\n",
        "        # input layer to latent standard deviation layer\r\n",
        "        z_var = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(self.rnaseq_input)\r\n",
        "        z_var_batchnorm = BatchNormalization()(z_var)\r\n",
        "        self.z_var_encoded = Activation('relu')(z_var_batchnorm)\r\n",
        "\r\n",
        "        # return the encoded and randomly sampled z vector\r\n",
        "        # Takes two keras layers as input to the custom sampling function layer\r\n",
        "        self.z = Lambda(self._sampling, output_shape=(self.latent_dim, ))([self.z_mean_encoded, self.z_var_encoded])\r\n",
        "\r\n",
        "    def _build_decoder_layer(self):\r\n",
        "        # The decoding layer is much simpler with a single layer glorot uniform\r\n",
        "        # initialized and sigmoid activation\r\n",
        "        self.decoder_model = Sequential()\r\n",
        "        self.decoder_model.add(Dense(self.original_dim, activation='sigmoid', input_dim=self.latent_dim))\r\n",
        "        self.rnaseq_reconstruct = self.decoder_model(self.z)\r\n",
        "\r\n",
        "    def _compile_vae(self):\r\n",
        "        adam = optimizers.Adam(lr=self.learning_rate)\r\n",
        "        vae_layer = VariationalLayer(var_layer=self.z_var_encoded,\r\n",
        "                                     mean_layer=self.z_mean_encoded,\r\n",
        "                                     original_dim=self.original_dim,\r\n",
        "                                     beta=self.beta, loss=self.loss)([self.rnaseq_input, self.rnaseq_reconstruct])\r\n",
        "        self.full_model = Model(self.rnaseq_input, vae_layer)\r\n",
        "        self.full_model.compile(optimizer=adam, loss=None,\r\n",
        "                                loss_weights=[self.beta])\r\n",
        "\r\n",
        "    def _connect_layers(self):\r\n",
        "        self.encoder = Model(self.rnaseq_input, self.z_mean_encoded)\r\n",
        "\r\n",
        "        decoder_input = Input(shape=(self.latent_dim, ))\r\n",
        "        _x_decoded_mean = self.decoder_model(decoder_input)\r\n",
        "        self.decoder = Model(decoder_input, _x_decoded_mean)\r\n",
        "\r\n",
        "    def train_vae(self, train_df, test_df, separate_loss=False):\r\n",
        "        cbks = [WarmUpCallback(self.beta, self.kappa)]\r\n",
        "        if separate_loss:\r\n",
        "            tybalt_loss_cbk = LossCallback(training_data=np.array(train_df), encoder_cbk=self.encoder, decoder_cbk=self.decoder, original_dim=self.original_dim)\r\n",
        "            cbks += [tybalt_loss_cbk]\r\n",
        "\r\n",
        "        self.hist = self.full_model.fit(np.array(train_df),\r\n",
        "                                        shuffle=True,\r\n",
        "                                        epochs=self.epochs,\r\n",
        "                                        batch_size=self.batch_size,\r\n",
        "                                        verbose=self.verbose,\r\n",
        "                                        validation_data=(np.array(test_df), None),\r\n",
        "                                        callbacks=cbks)\r\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\r\n",
        "\r\n",
        "        if separate_loss:\r\n",
        "            self.history_df = self.history_df.assign(recon=tybalt_loss_cbk.xent_loss)\r\n",
        "            self.history_df = self.history_df.assign(kl=tybalt_loss_cbk.kl_loss)\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJUpDjvFabLS"
      },
      "source": [
        "common_norm_df = pd.read_csv('/content/drive/MyDrive/aacb_project/datasets/common_normalized.csv', index_col=0)\r\n",
        "df_new = pd.read_csv(\"/content/drive/MyDrive/aacb_project/datasets/control_pcos_celltype_mapping.csv\")\r\n",
        "\r\n",
        "normal = [\"control\", \"obese\", \"pioglitazone\", \"lean\", \"valproic\"]\r\n",
        "pcos = [\"PCOS\", \"PCOS_lean\", \"PCOS_obese\", \"PCOS_insulin_res\"]\r\n",
        "replacement = {}\r\n",
        "for i in normal:\r\n",
        "    replacement[i] = 0\r\n",
        "for i in pcos:\r\n",
        "    replacement[i] = 1\r\n",
        "\r\n",
        "position = []\r\n",
        "values = list(df_new[\"sample_id\"])\r\n",
        "for i,j in enumerate(common_norm_df[\"sample_id\"]):\r\n",
        "    position.append(values.index(j))\r\n",
        "\r\n",
        "df_new = df_new.loc[position]\r\n",
        "df_new = df_new.reset_index()\r\n",
        "df_new = df_new.drop(\"index\", axis=1)\r\n",
        "\r\n",
        "result = pd.merge(common_norm_df, df_new[df_new.columns[:-1]], how='inner', on='sample_id')\r\n",
        "gene_feature_ids = result.columns[1:-3]\r\n",
        "\r\n",
        "result[\"PCOS\"] = df_new[\"PCOS/Control\"]\r\n",
        "result.drop([\"PCOS/Control\"], axis=1, inplace=True)\r\n",
        "result.replace({\"PCOS\":replacement}, inplace=True)\r\n",
        "\r\n",
        "gene_feature_ids = result.columns[1:-3]\r\n",
        "\r\n",
        "test_set_percent = 0.1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXwZ4ua5Ekxr",
        "outputId": "e0ab00b1-56ae-4d03-c590-f5d06eaba73d"
      },
      "source": [
        "cell_type_groups = [pd.DataFrame(y) for x, y in result.groupby(['cell_type'], as_index=False)]\r\n",
        "cell_type_dfs = [x[['PCOS', 'cell_type']] for x in cell_type_groups]\r\n",
        "cell_type_pcos_controls = []\r\n",
        "for cell_type_df in cell_type_groups:\r\n",
        "    cell_type = list(cell_type_df['cell_type'])[0]\r\n",
        "    numPCOS = list(cell_type_df['PCOS']).count(1)\r\n",
        "    numControl = list(cell_type_df['PCOS']).count(0)\r\n",
        "    cell_type_pcos_controls.append(pd.DataFrame([[cell_type, numPCOS, numControl]], columns=['cell_type', 'PCOS', 'Control']))\r\n",
        "cell_type_pcos_controls_df = pd.concat(cell_type_pcos_controls).reset_index(drop=True)\r\n",
        "print(cell_type_pcos_controls_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 cell_type  PCOS  Control\n",
            "0                  adipose     8        7\n",
            "1                  cumulus    12       11\n",
            "2              endothelial     3        4\n",
            "3               epithelial     4        3\n",
            "4                granulosa     7        3\n",
            "5              mesenchymal     3        4\n",
            "6                  stromal     4        4\n",
            "7                    theca    10       16\n",
            "8  vastus_lateralis_muscle    26       46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnNoESmhEpat",
        "outputId": "83a440fb-69ef-4b1f-f5e0-6a9fb214e12a"
      },
      "source": [
        "pcos = (result.query(\"PCOS == 1\").sample_id.tolist())\r\n",
        "control = (result.query(\"PCOS == 0\").sample_id.tolist())\r\n",
        "print(\"PCOS samples:\", len(pcos))\r\n",
        "print(\"Control samples:\", len(control))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCOS samples: 77\n",
            "Control samples: 98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxKwIAI1KRm6",
        "outputId": "f23b301b-9688-4b4e-b13d-e1d76d8f9150"
      },
      "source": [
        "X1 = common_norm_df[common_norm_df['sample_id'].isin(pcos)]\r\n",
        "X_pcos_df = X1[X1.columns[1:-1]]\r\n",
        "X_pcos = X_pcos_df.to_numpy()\r\n",
        "X2 = common_norm_df[common_norm_df['sample_id'].isin(control)]\r\n",
        "X_control_df = X2[X2.columns[1:-1]]\r\n",
        "X_control = X_control_df.to_numpy()\r\n",
        "print(X_pcos.shape, X_pcos_df.shape, X_control.shape, X_control_df.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(77, 1667) (77, 1667) (98, 1667) (98, 1667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoR64hVeaaYs",
        "outputId": "dd040a02-3a6e-4b4b-8c21-bc968de3f89a"
      },
      "source": [
        "# Setting up the possible latent dimensions\r\n",
        "# A total of 27 latent dimensions are taken under consideration\r\n",
        "\r\n",
        "k_list = []\r\n",
        "k_list.extend(list(range(2, 10)))\r\n",
        "k_list.extend(list(range(10, 20, 2)))\r\n",
        "k_list.extend(list(range(20, 50, 5)))\r\n",
        "k_list.extend(list(range(50, 61, 10)))\r\n",
        "k_list.append(78)\r\n",
        "k_list.extend(list(range(80, 100, 10)))\r\n",
        "k_list.extend(list(range(100, 176, 25)))\r\n",
        "\r\n",
        "print(\"Latent dimensions:\")\r\n",
        "print(k_list)\r\n",
        "\r\n",
        "model_list = [\"pca\", \"ica\", \"nmf\", \"dae\", \"vae\"]\r\n",
        "print(model_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latent dimensions:\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 35, 40, 45, 50, 60, 78, 80, 90, 100, 125, 150, 175]\n",
            "['pca', 'ica', 'nmf', 'dae', 'vae']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B6uTv9ZFNbq"
      },
      "source": [
        "z_dict_pcos = {}\r\n",
        "z_dict_control = {}\r\n",
        "for algo in model_list:\r\n",
        "  z_dict_pcos[algo] = {}\r\n",
        "  z_dict_control[algo] = {}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G84-DEeoEPBX",
        "outputId": "f4ef41b0-0344-4654-a746-32af878684cc"
      },
      "source": [
        "# PCA, ICA, NMF Training and Saving Weights\r\n",
        "\r\n",
        "weight_list_pcos = {}\r\n",
        "weight_list_control = {}\r\n",
        "\r\n",
        "print(\"=\"*50)\r\n",
        "print(\"PCOS group\")\r\n",
        "for model in [\"pca\", \"ica\", \"nmf\"]:\r\n",
        "    bce_p, l2_p, output_p = get_cost_reconstruction(X_pcos, model, k_list)\r\n",
        "    weight_list_pcos[model] = output_p\r\n",
        "print(\"=\"*50)\r\n",
        "\r\n",
        "for algo in ['pca', 'ica', 'nmf']:\r\n",
        "  for k in k_list:\r\n",
        "    if k in weight_list_pcos[algo]:\r\n",
        "      z_dict_pcos[algo][k] = weight_list_pcos[algo][k]\r\n",
        "    else:\r\n",
        "      z_dict_pcos[algo][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]\r\n",
        "\r\n",
        "print(\"Control group\")\r\n",
        "for model in [\"pca\", \"ica\", \"nmf\"]:\r\n",
        "    bce_c, l2_c, output_c = get_cost_reconstruction(X_control, model, k_list)\r\n",
        "    weight_list_control[model] = output_c\r\n",
        "print(\"=\"*50)\r\n",
        "\r\n",
        "for algo in ['pca', 'ica', 'nmf']:\r\n",
        "  for k in k_list:\r\n",
        "    if k in weight_list_control[algo]:\r\n",
        "      z_dict_control[algo][k] = weight_list_control[algo][k]\r\n",
        "    else:\r\n",
        "      z_dict_control[algo][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PCOS group\n",
            "Calculating Reconstruction Error for: PCA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 20.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating Reconstruction Error for: ICA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:02<00:00,  9.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating Reconstruction Error for: NMF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:28<00:00,  1.03s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Control group\n",
            "Calculating Reconstruction Error for: PCA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 14.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating Reconstruction Error for: ICA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:03<00:00,  7.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Calculating Reconstruction Error for: NMF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [01:03<00:00,  2.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pOHbysyTvon",
        "outputId": "f4a1e6fd-43ec-4df2-937f-d40ed1c6e180"
      },
      "source": [
        "# VAE Training and Saving Weights\r\n",
        "\r\n",
        "# PCOS\r\n",
        "\r\n",
        "original_dim = X_pcos_df.shape[1]\r\n",
        "pcos_test_df = X_pcos_df.sample(frac=test_set_percent)\r\n",
        "pcos_train_df = X_pcos_df.drop(pcos_test_df.index)\r\n",
        "\r\n",
        "vae_weights_list = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    if latent_dim <= len(X_pcos_df):\r\n",
        "      vae = Tybalt(original_dim, latent_dim, batch_size=50, epochs=200,\r\n",
        "                      learning_rate=0.005, kappa=1, epsilon_std=1.0,\r\n",
        "                      beta=K.variable(0), loss='binary_crossentropy',\r\n",
        "                      verbose=False)\r\n",
        "      vae._build_encoder_layer()\r\n",
        "      vae._build_decoder_layer()\r\n",
        "      vae._compile_vae()\r\n",
        "      vae._connect_layers()\r\n",
        "      vae.train_vae(pcos_train_df, pcos_test_df, separate_loss=False)\r\n",
        "      vae_weights_list.append(vae.get_weights()[1])\r\n",
        "\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  if i < len(vae_weights_list):\r\n",
        "    z_dict_pcos['vae'][k] = vae_weights_list[i][0]\r\n",
        "  else:\r\n",
        "    z_dict_pcos['vae'][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]\r\n",
        "\r\n",
        "\r\n",
        "# Control\r\n",
        "\r\n",
        "original_dim = X_control_df.shape[1]\r\n",
        "control_test_df = X_control_df.sample(frac=test_set_percent)\r\n",
        "control_train_df = X_control_df.drop(control_test_df.index)\r\n",
        "\r\n",
        "vae_weights_list2 = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    if latent_dim <= len(X_control_df):\r\n",
        "      vae = Tybalt(original_dim, latent_dim, batch_size=50, epochs=200,\r\n",
        "                      learning_rate=0.005, kappa=1, epsilon_std=1.0,\r\n",
        "                      beta=K.variable(0), loss='binary_crossentropy',\r\n",
        "                      verbose=False)\r\n",
        "      vae._build_encoder_layer()\r\n",
        "      vae._build_decoder_layer()\r\n",
        "      vae._compile_vae()\r\n",
        "      vae._connect_layers()\r\n",
        "      vae.train_vae(control_train_df, control_test_df, separate_loss=False)\r\n",
        "      vae_weights_list2.append(vae.get_weights()[1])\r\n",
        "\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  if i < len(vae_weights_list2):\r\n",
        "    z_dict_control['vae'][k] = vae_weights_list2[i][0]\r\n",
        "  else:\r\n",
        "    z_dict_control['vae'][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/28 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_1:0' shape=() dtype=float32> beta\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 1/28 [00:03<01:42,  3.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_2:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/28 [00:07<01:41,  3.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_3:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 3/28 [00:12<01:43,  4.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_4:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 4/28 [00:17<01:44,  4.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_5:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 5/28 [00:22<01:47,  4.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_6:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 21%|██▏       | 6/28 [00:28<01:51,  5.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_7:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 7/28 [00:35<01:55,  5.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_8:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 29%|██▊       | 8/28 [00:42<01:59,  5.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_9:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 9/28 [00:49<02:02,  6.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_10:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 10/28 [00:57<02:03,  6.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_11:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 11/28 [01:06<02:05,  7.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_12:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 12/28 [01:15<02:05,  7.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_13:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▋     | 13/28 [01:25<02:05,  8.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_14:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 14/28 [01:35<02:04,  8.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_15:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▎    | 15/28 [01:45<02:02,  9.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_16:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 16/28 [01:56<01:59,  9.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_17:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 17/28 [02:08<01:56, 10.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_18:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 18/28 [02:21<01:51, 11.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_19:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 19/28 [02:34<01:46, 11.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_20:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████▏  | 20/28 [02:48<01:39, 12.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_21:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [03:03<00:00,  6.57s/it]\n",
            "  0%|          | 0/28 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_22:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 1/28 [00:13<06:12, 13.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_23:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 2/28 [00:28<06:02, 13.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_24:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 3/28 [00:42<05:54, 14.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_25:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 4/28 [00:58<05:49, 14.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_26:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 5/28 [01:14<05:44, 14.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_27:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 21%|██▏       | 6/28 [01:30<05:37, 15.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_28:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 7/28 [01:47<05:32, 15.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_29:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 29%|██▊       | 8/28 [02:04<05:26, 16.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_30:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 9/28 [02:22<05:19, 16.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_31:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 10/28 [02:41<05:11, 17.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_32:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 11/28 [03:00<05:02, 17.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_33:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 12/28 [03:19<04:53, 18.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_34:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▋     | 13/28 [03:39<04:42, 18.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_35:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 14/28 [04:00<04:30, 19.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_36:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▎    | 15/28 [04:21<04:19, 19.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_37:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 16/28 [04:43<04:05, 20.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_38:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 17/28 [05:06<03:52, 21.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_39:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 18/28 [05:29<03:39, 21.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_40:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 19/28 [05:54<03:24, 22.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_41:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████▏  | 20/28 [06:19<03:08, 23.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_42:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 21/28 [06:46<02:50, 24.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_43:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 79%|███████▊  | 22/28 [07:13<02:31, 25.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_44:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 23/28 [07:41<02:10, 26.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_45:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [08:09<00:00, 17.48s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbw0acW5V5Uq",
        "outputId": "ea205cad-76b9-49a9-a15a-0499f5248876"
      },
      "source": [
        "# DAE Training and Saving Weights\r\n",
        "\r\n",
        "# PCOS\r\n",
        "\r\n",
        "original_dim = X_pcos_df.shape[1]\r\n",
        "pcos_test_df = X_pcos_df.sample(frac=test_set_percent)\r\n",
        "pcos_train_df = X_pcos_df.drop(pcos_test_df.index)\r\n",
        "\r\n",
        "dae_weights_list = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    if latent_dim <= len(X_pcos_df):\r\n",
        "      dae = Adage(original_dim, latent_dim, noise=0.05, batch_size=50,\r\n",
        "                      epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\r\n",
        "                      optimizer='adam', tied_weights=True, verbose=False)\r\n",
        "      dae._build_graph()\r\n",
        "      dae._build_tied_weights_graph()\r\n",
        "      dae._compile_adage()\r\n",
        "      dae._connect_layers()\r\n",
        "      dae.initialize_model()\r\n",
        "      dae.train_adage(pcos_train_df, pcos_test_df)\r\n",
        "      dae_weights_list.append(dae.get_weights()[1])\r\n",
        "\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  if i < len(dae_weights_list):\r\n",
        "    z_dict_pcos['dae'][k] = dae_weights_list[i][0].transpose()\r\n",
        "  else:\r\n",
        "    z_dict_pcos['dae'][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]\r\n",
        "\r\n",
        "\r\n",
        "# Control\r\n",
        "\r\n",
        "original_dim = X_control_df.shape[1]\r\n",
        "control_test_df = X_control_df.sample(frac=test_set_percent)\r\n",
        "control_train_df = X_control_df.drop(control_test_df.index)\r\n",
        "\r\n",
        "dae_weights_list2 = []\r\n",
        "\r\n",
        "for latent_dim in tqdm(k_list):\r\n",
        "    if latent_dim <= len(X_control_df):\r\n",
        "      dae = Adage(original_dim, latent_dim, noise=0.05, batch_size=50,\r\n",
        "                      epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\r\n",
        "                      optimizer='adam', tied_weights=True, verbose=False)\r\n",
        "      dae._build_graph()\r\n",
        "      dae._build_tied_weights_graph()\r\n",
        "      dae._compile_adage()\r\n",
        "      dae._connect_layers()\r\n",
        "      dae.initialize_model()\r\n",
        "      dae.train_adage(pcos_train_df, pcos_test_df)\r\n",
        "      dae_weights_list2.append(dae.get_weights()[1])\r\n",
        "\r\n",
        "for i in range(len(k_list)):\r\n",
        "  k = k_list[i]\r\n",
        "  if i < len(dae_weights_list2):\r\n",
        "    z_dict_control['dae'][k] = dae_weights_list2[i][0].transpose()\r\n",
        "  else:\r\n",
        "    z_dict_control['dae'][k] = [[[] for _ in range(k)] for _ in range(len(gene_feature_ids))]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [04:50<00:00, 10.36s/it]\n",
            "100%|██████████| 28/28 [06:39<00:00, 14.25s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBw0KYeAPYij"
      },
      "source": [
        "with open('/content/drive/MyDrive/aacb_project/datasets/z_dict_pcos.p', 'wb') as f1:\r\n",
        "  pkl.dump(z_dict_pcos, f1)\r\n",
        "with open('/content/drive/MyDrive/aacb_project/datasets/z_dict_control.p', 'wb') as f2:\r\n",
        "  pkl.dump(z_dict_control, f2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCGsztuACNMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}