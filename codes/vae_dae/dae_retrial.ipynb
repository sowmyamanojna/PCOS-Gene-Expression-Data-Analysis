{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "dae_retrial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyamanojna/CS6024-Algorithmic-Approaches-to-Computational-Biology-Project/blob/master/codes/vae_dae/dae_retrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Wu_G_5oPsD",
        "outputId": "270de80a-e699-456a-cc64-2196ba466cf0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Lambda, Activation, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l1\n",
        "\n",
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Layer\n",
        "from keras import metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F41c_qN6oV6Y",
        "outputId": "beabbba4-38dc-4641-dbae-a4f2e4de6a48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CugEKHULoPsH"
      },
      "source": [
        "# From tybalt.utils.adage_utils\n",
        "class TiedWeightsDecoder(Layer):\n",
        "    \"\"\"\n",
        "    Transpose the encoder weights to apply decoding of compressed latent space\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim, encoder, activation=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder = encoder\n",
        "        self.activation = activations.get(activation)\n",
        "        super(TiedWeightsDecoder, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.encoder.weights\n",
        "        super(TiedWeightsDecoder, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Encoder weights: [weight_matrix, bias_term]\n",
        "        output = K.dot(x - self.encoder.weights[1],\n",
        "                       K.transpose(self.encoder.weights[0]))\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuCeI2MroPsJ"
      },
      "source": [
        "# From tybalt.utils.base\n",
        "class BaseModel():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_summary(self):\n",
        "        self.full_model.summary()\n",
        "\n",
        "    def visualize_architecture(self, output_file):\n",
        "        # Visualize the connections of the custom VAE model\n",
        "        plot_model(self.full_model, to_file=output_file)\n",
        "\n",
        "    def visualize_training(self, output_file=None):\n",
        "        # Visualize training performance\n",
        "        history_df = pd.DataFrame(self.hist.history)\n",
        "        ax = history_df.plot()\n",
        "        ax.set_xlabel('Epochs')\n",
        "        ax.set_ylabel('Loss')\n",
        "        fig = ax.get_figure()\n",
        "        if output_file:\n",
        "            fig.savefig(output_file)\n",
        "        else:\n",
        "            fig.show()\n",
        "\n",
        "    def get_weights(self, decoder=True):\n",
        "        # Extract weight matrices from encoder or decoder\n",
        "        weights = []\n",
        "        if decoder:\n",
        "            for layer in self.decoder.layers:\n",
        "                weights.append(layer.get_weights())\n",
        "        else:\n",
        "            for layer in self.encoder.layers:\n",
        "                # Encoder weights must be transposed\n",
        "                encoder_weights = layer.get_weights()\n",
        "                encoder_weights = [np.transpose(x) for x in encoder_weights]\n",
        "                weights.append(encoder_weights)\n",
        "        return weights\n",
        "\n",
        "    def save_models(self, encoder_file, decoder_file):\n",
        "        self.encoder.save(encoder_file)\n",
        "        self.decoder.save(decoder_file)\n",
        "\n",
        "\n",
        "class VAE(BaseModel):\n",
        "    def __init__(self):\n",
        "        BaseModel.__init__(self)\n",
        "\n",
        "    def _sampling(self, args):\n",
        "        \"\"\"\n",
        "        Function for reparameterization trick to make model differentiable\n",
        "        \"\"\"\n",
        "        # Function with args required for Keras Lambda function\n",
        "        z_mean, z_log_var = args\n",
        "\n",
        "        # Draw epsilon of the same shape from a standard normal distribution\n",
        "        epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
        "                                  stddev=self.epsilon_std)\n",
        "\n",
        "        # The latent vector is non-deterministic and differentiable\n",
        "        # in respect to z_mean and z_log_var\n",
        "        z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "        return z\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"\n",
        "        Helper function to run that builds and compiles Keras layers\n",
        "        \"\"\"\n",
        "        self._build_encoder_layer()\n",
        "        self._build_decoder_layer()\n",
        "        self._compile_vae()\n",
        "        self._connect_layers()\n",
        "\n",
        "    def compress(self, df):\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\n",
        "        # a cVAE expects a list of [rnaseq_df, y_df]\n",
        "        encoded_df = self.encoder.predict_on_batch(df)\n",
        "\n",
        "        if self.model_name == 'cTybalt':\n",
        "            named_index = df[0].index\n",
        "        else:\n",
        "            named_index = df.index\n",
        "\n",
        "        encoded_df = pd.DataFrame(encoded_df,\n",
        "                                  columns=range(1, self.latent_dim + 1),\n",
        "                                  index=named_index)\n",
        "        return encoded_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIJTv8peoPsK"
      },
      "source": [
        "# From tybalt.utils.vae_utils\n",
        "def approx_keras_binary_cross_entropy(x, z, p, epsilon=1e-07):\n",
        "    \"\"\"\n",
        "    Function to approximate Keras `binary_crossentropy()`\n",
        "    https://github.com/keras-team/keras/blob/e6c3f77b0b10b0d76778109a40d6d3282f1cadd0/keras/losses.py#L76\n",
        "    Which is a wrapper for TensorFlow `sigmoid_cross_entropy_with_logits()`\n",
        "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
        "    An important step is to clip values of reconstruction\n",
        "    https://github.com/keras-team/keras/blob/a3d160b9467c99cbb27f9aa0382c759f45c8ee66/keras/backend/tensorflow_backend.py#L3071\n",
        "    Arguments:\n",
        "    x - Reconstructed input RNAseq data\n",
        "    z - Input RNAseq data\n",
        "    p - number of features\n",
        "    epsilon - the clipping value to stabilize results (same Keras default)\n",
        "    \"\"\"\n",
        "    # Ensure numpy arrays\n",
        "    x = np.array(x)\n",
        "    z = np.array(z)\n",
        "\n",
        "    # Add clip to value\n",
        "    x[x < epsilon] = epsilon\n",
        "    x[x > (1 - epsilon)] = (1 - epsilon)\n",
        "\n",
        "    # Perform logit\n",
        "    x = np.log(x / (1 - x))\n",
        "\n",
        "    # Return approximate binary cross entropy\n",
        "    return np.mean(p * np.mean(- x * z + np.log(1 + np.exp(x)), axis=-1))\n",
        "\n",
        "\n",
        "class VariationalLayer(Layer):\n",
        "    \"\"\"\n",
        "    Define a custom layer that learns and performs the training\n",
        "    \"\"\"\n",
        "    def __init__(self, var_layer, mean_layer, original_dim, beta, loss,\n",
        "                 **kwargs):\n",
        "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
        "        self.is_placeholder = True\n",
        "        self.var_layer = var_layer\n",
        "        self.mean_layer = mean_layer\n",
        "        self.original_dim = original_dim\n",
        "        self.beta = beta\n",
        "        self.loss = loss\n",
        "        super(VariationalLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def vae_loss(self, x_input, x_decoded):\n",
        "        if self.loss == 'binary_crossentropy':\n",
        "            recon_loss = self.original_dim * \\\n",
        "                         metrics.binary_crossentropy(x_input, x_decoded)\n",
        "        elif self.loss == 'mse':\n",
        "            recon_loss = self.original_dim * \\\n",
        "                         metrics.mean_squared_error(x_input, x_decoded)\n",
        "\n",
        "        kl_loss = - 0.5 * K.sum(1 + self.var_layer -\n",
        "                                K.square(self.mean_layer) -\n",
        "                                K.exp(self.var_layer), axis=-1)\n",
        "\n",
        "        return K.mean(recon_loss + (K.get_value(self.beta) * kl_loss))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, x_decoded = inputs\n",
        "        loss = self.vae_loss(x, x_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # We won't actually use the output.\n",
        "        return x\n",
        "\n",
        "\n",
        "class WarmUpCallback(Callback):\n",
        "    def __init__(self, beta, kappa):\n",
        "        self.beta = beta\n",
        "        self.kappa = kappa\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \"\"\"\n",
        "        Behavior on each epoch\n",
        "        \"\"\"\n",
        "        if K.get_value(self.beta) <= 1:\n",
        "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\n",
        "\n",
        "\n",
        "class LossCallback(Callback):\n",
        "    def __init__(self, training_data, original_dim, encoder_cbk, decoder_cbk):\n",
        "        self.training_data = training_data\n",
        "        self.original_dim = original_dim\n",
        "        self.encoder_cbk = encoder_cbk\n",
        "        self.decoder_cbk = decoder_cbk\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.xent_loss = []\n",
        "        self.kl_loss = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        recon = self.decoder_cbk.predict(\n",
        "            self.encoder_cbk.predict(self.training_data))\n",
        "        xent_loss = approx_keras_binary_cross_entropy(x=recon,\n",
        "                                                      z=self.training_data,\n",
        "                                                      p=self.original_dim)\n",
        "        full_loss = logs.get('loss')\n",
        "        self.xent_loss.append(xent_loss)\n",
        "        self.kl_loss.append(full_loss - xent_loss)\n",
        "        return"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogxhU4x_oPsM"
      },
      "source": [
        "class Adage(BaseModel):\n",
        "    \"\"\"\n",
        "    Training and evaluation of an ADAGE model\n",
        "    Usage: from tybalt.models import Adage\n",
        "    \"\"\"\n",
        "    def __init__(self, original_dim, latent_dim, noise=0.05, batch_size=50,\n",
        "                 epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\n",
        "                 optimizer='adam', tied_weights=True, verbose=True):\n",
        "        BaseModel.__init__(self)\n",
        "        self.model_name = 'ADAGE'\n",
        "        self.original_dim = original_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.noise = noise\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.sparsity = sparsity\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.tied_weights = tied_weights\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _build_graph(self):\n",
        "        # Build the Keras graph for an ADAGE model\n",
        "        self.input_rnaseq = Input(shape=(self.original_dim, ))\n",
        "        drop = Dropout(self.noise)(self.input_rnaseq)\n",
        "        self.encoded = Dense(self.latent_dim,\n",
        "                             activity_regularizer=l1(self.sparsity))(drop)\n",
        "        activation = Activation('relu')(self.encoded)\n",
        "        decoded_rnaseq = Dense(self.original_dim,\n",
        "                               activation='sigmoid')(activation)\n",
        "\n",
        "        self.full_model = Model(self.input_rnaseq, decoded_rnaseq)\n",
        "\n",
        "    def _build_tied_weights_graph(self):\n",
        "        # Build Keras graph for an ADAGE model with tied weights\n",
        "        self.encoded = Dense(self.latent_dim,\n",
        "                             input_shape=(self.original_dim, ),\n",
        "                             activity_regularizer=l1(self.sparsity),\n",
        "                             activation='relu')\n",
        "        dropout_layer = Dropout(self.noise)\n",
        "        self.tied_decoder = TiedWeightsDecoder(input_shape=(self.latent_dim, ),\n",
        "                                               output_dim=self.original_dim,\n",
        "                                               activation='sigmoid',\n",
        "                                               encoder=self.encoded)\n",
        "        self.full_model = Sequential()\n",
        "        self.full_model.add(self.encoded)\n",
        "        self.full_model.add(dropout_layer)\n",
        "        self.full_model.add(self.tied_decoder)\n",
        "\n",
        "    def _compile_adage(self):\n",
        "        # Compile the autoencoder to prepare for training\n",
        "        if self.optimizer == 'adadelta':\n",
        "            optim = optimizers.Adadelta(lr=self.learning_rate)\n",
        "        elif self.optimizer == 'adam':\n",
        "            optim = optimizers.Adam(lr=self.learning_rate)\n",
        "        self.full_model.compile(optimizer=optim, loss=self.loss)\n",
        "\n",
        "    def _connect_layers(self):\n",
        "        # Separate out the encoder and decoder model\n",
        "        encoded_input = Input(shape=(self.latent_dim, ))\n",
        "        decoder_layer = self.full_model.layers[-1]\n",
        "        self.decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "        if self.tied_weights:\n",
        "            # The keras graph is built differently for a tied weight model\n",
        "            # Build a model with input and output Tensors of the encoded layer\n",
        "            self.encoder = Model(self.encoded.input, self.encoded.output)\n",
        "        else:\n",
        "            self.encoder = Model(self.input_rnaseq, self.encoded)\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"\n",
        "        Helper function to run that builds and compiles Keras layers\n",
        "        \"\"\"\n",
        "        if self.tied_weights:\n",
        "            self._build_tied_weights_graph()\n",
        "        else:\n",
        "            self._build_graph()\n",
        "        self._connect_layers()\n",
        "        self._compile_adage()\n",
        "\n",
        "    def train_adage(self, train_df, test_df, adage_comparable_loss=False):\n",
        "        self.hist = self.full_model.fit(np.array(train_df), np.array(train_df),\n",
        "                                        shuffle=True,\n",
        "                                        epochs=self.epochs,\n",
        "                                        verbose=self.verbose,\n",
        "                                        batch_size=self.batch_size,\n",
        "                                        validation_data=(np.array(test_df),\n",
        "                                                         np.array(test_df)))\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\n",
        "\n",
        "        # ADAGE loss is a mean over all features - to make this value more\n",
        "        # comparable to the VAE reconstruciton loss, multiply by num genes\n",
        "        if adage_comparable_loss:\n",
        "            self.history_df = self.history_df * self.original_dim\n",
        "\n",
        "    def compress(self, df):\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\n",
        "        encoded_df = self.encoder.predict(np.array(df))\n",
        "        encoded_df = pd.DataFrame(encoded_df, index=df.index,\n",
        "                                  columns=range(1, self.latent_dim + 1))\n",
        "        return encoded_df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "dcfyRJk1oPsN",
        "outputId": "f11ff9a3-a4be-4db7-d79a-3f2e4d59fcd8"
      },
      "source": [
        "pcos_df = pd.read_csv('/content/drive/MyDrive/aacb_project/datasets/common_normalized.csv', index_col=0)\n",
        "pcos_df = pcos_df.drop(['sample_id'], axis=1)\n",
        "\n",
        "# Split 10% test set randomly\n",
        "test_set_percent = 0.1\n",
        "\n",
        "pcos_test_df = pcos_df.sample(frac=test_set_percent)\n",
        "pcos_train_df = pcos_df.drop(pcos_test_df.index)\n",
        "\n",
        "display(pcos_train_df.head(2))\n",
        "display(pcos_test_df.head(2))\n",
        "\n",
        "original_dim = pcos_df.shape[1]\n",
        "latent_dim = 2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>27</th>\n",
              "      <th>36</th>\n",
              "      <th>59</th>\n",
              "      <th>87</th>\n",
              "      <th>94</th>\n",
              "      <th>105</th>\n",
              "      <th>153</th>\n",
              "      <th>159</th>\n",
              "      <th>164</th>\n",
              "      <th>226</th>\n",
              "      <th>288</th>\n",
              "      <th>290</th>\n",
              "      <th>311</th>\n",
              "      <th>330</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>345</th>\n",
              "      <th>355</th>\n",
              "      <th>359</th>\n",
              "      <th>377</th>\n",
              "      <th>382</th>\n",
              "      <th>389</th>\n",
              "      <th>392</th>\n",
              "      <th>394</th>\n",
              "      <th>405</th>\n",
              "      <th>408</th>\n",
              "      <th>420</th>\n",
              "      <th>430</th>\n",
              "      <th>443</th>\n",
              "      <th>463</th>\n",
              "      <th>476</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>515</th>\n",
              "      <th>533</th>\n",
              "      <th>552</th>\n",
              "      <th>567</th>\n",
              "      <th>572</th>\n",
              "      <th>582</th>\n",
              "      <th>586</th>\n",
              "      <th>...</th>\n",
              "      <th>92558</th>\n",
              "      <th>92815</th>\n",
              "      <th>92822</th>\n",
              "      <th>93164</th>\n",
              "      <th>93487</th>\n",
              "      <th>93974</th>\n",
              "      <th>6248_84301</th>\n",
              "      <th>8693_100528030</th>\n",
              "      <th>100506581</th>\n",
              "      <th>112399</th>\n",
              "      <th>112479</th>\n",
              "      <th>374655</th>\n",
              "      <th>375035</th>\n",
              "      <th>375057</th>\n",
              "      <th>113251</th>\n",
              "      <th>114088</th>\n",
              "      <th>114791</th>\n",
              "      <th>114882</th>\n",
              "      <th>116228</th>\n",
              "      <th>116285</th>\n",
              "      <th>116985</th>\n",
              "      <th>116986</th>\n",
              "      <th>51463_653519</th>\n",
              "      <th>118491</th>\n",
              "      <th>118987</th>\n",
              "      <th>120227</th>\n",
              "      <th>645644</th>\n",
              "      <th>100129482</th>\n",
              "      <th>100529257_55333</th>\n",
              "      <th>253512</th>\n",
              "      <th>122704</th>\n",
              "      <th>253959</th>\n",
              "      <th>254359</th>\n",
              "      <th>254531</th>\n",
              "      <th>100132341</th>\n",
              "      <th>387893</th>\n",
              "      <th>388336</th>\n",
              "      <th>259266</th>\n",
              "      <th>261726</th>\n",
              "      <th>PCOS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.693258</td>\n",
              "      <td>0.125461</td>\n",
              "      <td>0.336077</td>\n",
              "      <td>0.044463</td>\n",
              "      <td>0.267819</td>\n",
              "      <td>0.467742</td>\n",
              "      <td>0.490196</td>\n",
              "      <td>0.008907</td>\n",
              "      <td>0.370576</td>\n",
              "      <td>0.953515</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060236</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>0.078014</td>\n",
              "      <td>0.1193</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712054</td>\n",
              "      <td>0.648867</td>\n",
              "      <td>0.535433</td>\n",
              "      <td>0.593551</td>\n",
              "      <td>0.651320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.132791</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.263318</td>\n",
              "      <td>0.725872</td>\n",
              "      <td>0.701149</td>\n",
              "      <td>0.395953</td>\n",
              "      <td>0.302829</td>\n",
              "      <td>0.83933</td>\n",
              "      <td>0.349876</td>\n",
              "      <td>0.835372</td>\n",
              "      <td>0.502399</td>\n",
              "      <td>0.800414</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.609797</td>\n",
              "      <td>0.146067</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.915703</td>\n",
              "      <td>0.768041</td>\n",
              "      <td>0.925659</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855740</td>\n",
              "      <td>0.130112</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.086751</td>\n",
              "      <td>0.288095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.560748</td>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.261745</td>\n",
              "      <td>0.209231</td>\n",
              "      <td>0.061603</td>\n",
              "      <td>0.295745</td>\n",
              "      <td>0.489703</td>\n",
              "      <td>0.481948</td>\n",
              "      <td>0.113924</td>\n",
              "      <td>0.100254</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396190</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.412466</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312354</td>\n",
              "      <td>0.198387</td>\n",
              "      <td>0.120213</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.446494</td>\n",
              "      <td>0.548203</td>\n",
              "      <td>0.247916</td>\n",
              "      <td>0.033477</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.137255</td>\n",
              "      <td>0.457245</td>\n",
              "      <td>0.529493</td>\n",
              "      <td>0.864979</td>\n",
              "      <td>0.321063</td>\n",
              "      <td>0.212610</td>\n",
              "      <td>0.564342</td>\n",
              "      <td>0.124113</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.119891</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.205272</td>\n",
              "      <td>0.253348</td>\n",
              "      <td>0.610282</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.764284</td>\n",
              "      <td>0.762722</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>0.062224</td>\n",
              "      <td>0.227353</td>\n",
              "      <td>0.002710</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.338057</td>\n",
              "      <td>0.135464</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.452107</td>\n",
              "      <td>0.908008</td>\n",
              "      <td>0.264473</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.250620</td>\n",
              "      <td>0.634656</td>\n",
              "      <td>0.218459</td>\n",
              "      <td>0.446225</td>\n",
              "      <td>0.29352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356108</td>\n",
              "      <td>0.253378</td>\n",
              "      <td>0.390449</td>\n",
              "      <td>0.322581</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.691409</td>\n",
              "      <td>0.534772</td>\n",
              "      <td>0.061511</td>\n",
              "      <td>0.709024</td>\n",
              "      <td>0.009294</td>\n",
              "      <td>0.578838</td>\n",
              "      <td>0.166392</td>\n",
              "      <td>0.323344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183544</td>\n",
              "      <td>0.887850</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.677912</td>\n",
              "      <td>0.556818</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.618792</td>\n",
              "      <td>0.622154</td>\n",
              "      <td>0.575527</td>\n",
              "      <td>0.753191</td>\n",
              "      <td>0.338673</td>\n",
              "      <td>0.245172</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.480964</td>\n",
              "      <td>0.317073</td>\n",
              "      <td>0.618557</td>\n",
              "      <td>0.229993</td>\n",
              "      <td>0.712381</td>\n",
              "      <td>0.686747</td>\n",
              "      <td>0.252062</td>\n",
              "      <td>0.371943</td>\n",
              "      <td>0.097902</td>\n",
              "      <td>0.517742</td>\n",
              "      <td>0.673404</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1668 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         27        36        59        87  ...    388336    259266    261726  PCOS\n",
              "0  0.693258  0.125461  0.336077  0.044463  ...  0.312354  0.198387  0.120213     1\n",
              "2  0.241573  0.446494  0.548203  0.247916  ...  0.097902  0.517742  0.673404     1\n",
              "\n",
              "[2 rows x 1668 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>27</th>\n",
              "      <th>36</th>\n",
              "      <th>59</th>\n",
              "      <th>87</th>\n",
              "      <th>94</th>\n",
              "      <th>105</th>\n",
              "      <th>153</th>\n",
              "      <th>159</th>\n",
              "      <th>164</th>\n",
              "      <th>226</th>\n",
              "      <th>288</th>\n",
              "      <th>290</th>\n",
              "      <th>311</th>\n",
              "      <th>330</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>345</th>\n",
              "      <th>355</th>\n",
              "      <th>359</th>\n",
              "      <th>377</th>\n",
              "      <th>382</th>\n",
              "      <th>389</th>\n",
              "      <th>392</th>\n",
              "      <th>394</th>\n",
              "      <th>405</th>\n",
              "      <th>408</th>\n",
              "      <th>420</th>\n",
              "      <th>430</th>\n",
              "      <th>443</th>\n",
              "      <th>463</th>\n",
              "      <th>476</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>515</th>\n",
              "      <th>533</th>\n",
              "      <th>552</th>\n",
              "      <th>567</th>\n",
              "      <th>572</th>\n",
              "      <th>582</th>\n",
              "      <th>586</th>\n",
              "      <th>...</th>\n",
              "      <th>92558</th>\n",
              "      <th>92815</th>\n",
              "      <th>92822</th>\n",
              "      <th>93164</th>\n",
              "      <th>93487</th>\n",
              "      <th>93974</th>\n",
              "      <th>6248_84301</th>\n",
              "      <th>8693_100528030</th>\n",
              "      <th>100506581</th>\n",
              "      <th>112399</th>\n",
              "      <th>112479</th>\n",
              "      <th>374655</th>\n",
              "      <th>375035</th>\n",
              "      <th>375057</th>\n",
              "      <th>113251</th>\n",
              "      <th>114088</th>\n",
              "      <th>114791</th>\n",
              "      <th>114882</th>\n",
              "      <th>116228</th>\n",
              "      <th>116285</th>\n",
              "      <th>116985</th>\n",
              "      <th>116986</th>\n",
              "      <th>51463_653519</th>\n",
              "      <th>118491</th>\n",
              "      <th>118987</th>\n",
              "      <th>120227</th>\n",
              "      <th>645644</th>\n",
              "      <th>100129482</th>\n",
              "      <th>100529257_55333</th>\n",
              "      <th>253512</th>\n",
              "      <th>122704</th>\n",
              "      <th>253959</th>\n",
              "      <th>254359</th>\n",
              "      <th>254531</th>\n",
              "      <th>100132341</th>\n",
              "      <th>387893</th>\n",
              "      <th>388336</th>\n",
              "      <th>259266</th>\n",
              "      <th>261726</th>\n",
              "      <th>PCOS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.323642</td>\n",
              "      <td>0.429946</td>\n",
              "      <td>0.290348</td>\n",
              "      <td>0.397461</td>\n",
              "      <td>0.175832</td>\n",
              "      <td>0.220503</td>\n",
              "      <td>0.796616</td>\n",
              "      <td>0.410946</td>\n",
              "      <td>0.275616</td>\n",
              "      <td>0.513240</td>\n",
              "      <td>0.657831</td>\n",
              "      <td>0.373378</td>\n",
              "      <td>0.494698</td>\n",
              "      <td>0.364469</td>\n",
              "      <td>0.405464</td>\n",
              "      <td>0.134784</td>\n",
              "      <td>0.293410</td>\n",
              "      <td>0.358117</td>\n",
              "      <td>0.083670</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.616571</td>\n",
              "      <td>0.432667</td>\n",
              "      <td>0.389987</td>\n",
              "      <td>0.323798</td>\n",
              "      <td>0.513705</td>\n",
              "      <td>0.238584</td>\n",
              "      <td>0.537117</td>\n",
              "      <td>0.325763</td>\n",
              "      <td>0.533384</td>\n",
              "      <td>0.654008</td>\n",
              "      <td>0.695983</td>\n",
              "      <td>0.515053</td>\n",
              "      <td>0.605268</td>\n",
              "      <td>0.369145</td>\n",
              "      <td>0.511993</td>\n",
              "      <td>0.248162</td>\n",
              "      <td>0.385627</td>\n",
              "      <td>0.657507</td>\n",
              "      <td>0.703432</td>\n",
              "      <td>0.12770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524407</td>\n",
              "      <td>0.080363</td>\n",
              "      <td>0.665824</td>\n",
              "      <td>0.363019</td>\n",
              "      <td>0.322438</td>\n",
              "      <td>0.559233</td>\n",
              "      <td>0.626507</td>\n",
              "      <td>0.590883</td>\n",
              "      <td>0.445137</td>\n",
              "      <td>0.569982</td>\n",
              "      <td>0.321235</td>\n",
              "      <td>0.163824</td>\n",
              "      <td>0.248510</td>\n",
              "      <td>0.447173</td>\n",
              "      <td>0.207168</td>\n",
              "      <td>0.42011</td>\n",
              "      <td>0.381446</td>\n",
              "      <td>0.413615</td>\n",
              "      <td>0.226999</td>\n",
              "      <td>0.244083</td>\n",
              "      <td>0.261896</td>\n",
              "      <td>0.362091</td>\n",
              "      <td>0.855041</td>\n",
              "      <td>0.741901</td>\n",
              "      <td>0.227154</td>\n",
              "      <td>0.434468</td>\n",
              "      <td>0.497738</td>\n",
              "      <td>0.557024</td>\n",
              "      <td>0.405206</td>\n",
              "      <td>0.257075</td>\n",
              "      <td>0.271987</td>\n",
              "      <td>0.249465</td>\n",
              "      <td>0.343650</td>\n",
              "      <td>0.531724</td>\n",
              "      <td>0.308010</td>\n",
              "      <td>0.511222</td>\n",
              "      <td>0.570499</td>\n",
              "      <td>0.078504</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.466677</td>\n",
              "      <td>0.467885</td>\n",
              "      <td>0.165163</td>\n",
              "      <td>0.271106</td>\n",
              "      <td>0.228080</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.763003</td>\n",
              "      <td>0.532205</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.559899</td>\n",
              "      <td>0.616100</td>\n",
              "      <td>0.204181</td>\n",
              "      <td>0.646334</td>\n",
              "      <td>0.563318</td>\n",
              "      <td>0.507199</td>\n",
              "      <td>0.193048</td>\n",
              "      <td>0.260161</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.069762</td>\n",
              "      <td>0.36215</td>\n",
              "      <td>0.547915</td>\n",
              "      <td>0.253726</td>\n",
              "      <td>0.516156</td>\n",
              "      <td>0.735041</td>\n",
              "      <td>0.694097</td>\n",
              "      <td>0.228282</td>\n",
              "      <td>0.476916</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.948941</td>\n",
              "      <td>0.692339</td>\n",
              "      <td>0.482900</td>\n",
              "      <td>0.574182</td>\n",
              "      <td>0.500809</td>\n",
              "      <td>0.208416</td>\n",
              "      <td>0.214867</td>\n",
              "      <td>0.696690</td>\n",
              "      <td>0.551836</td>\n",
              "      <td>0.499549</td>\n",
              "      <td>0.804834</td>\n",
              "      <td>0.52542</td>\n",
              "      <td>...</td>\n",
              "      <td>0.119984</td>\n",
              "      <td>0.357739</td>\n",
              "      <td>0.624343</td>\n",
              "      <td>0.682360</td>\n",
              "      <td>0.528296</td>\n",
              "      <td>0.067914</td>\n",
              "      <td>0.954496</td>\n",
              "      <td>0.502764</td>\n",
              "      <td>0.436648</td>\n",
              "      <td>0.807462</td>\n",
              "      <td>0.706774</td>\n",
              "      <td>0.227728</td>\n",
              "      <td>0.475126</td>\n",
              "      <td>0.554299</td>\n",
              "      <td>0.701643</td>\n",
              "      <td>0.28921</td>\n",
              "      <td>0.726533</td>\n",
              "      <td>0.603753</td>\n",
              "      <td>0.166050</td>\n",
              "      <td>0.515188</td>\n",
              "      <td>0.423397</td>\n",
              "      <td>0.572920</td>\n",
              "      <td>0.712672</td>\n",
              "      <td>0.541823</td>\n",
              "      <td>0.134790</td>\n",
              "      <td>0.533768</td>\n",
              "      <td>0.176014</td>\n",
              "      <td>0.651214</td>\n",
              "      <td>0.256511</td>\n",
              "      <td>0.161921</td>\n",
              "      <td>0.554520</td>\n",
              "      <td>0.581196</td>\n",
              "      <td>0.318367</td>\n",
              "      <td>0.842679</td>\n",
              "      <td>0.303518</td>\n",
              "      <td>0.626343</td>\n",
              "      <td>0.135349</td>\n",
              "      <td>0.651245</td>\n",
              "      <td>0.332059</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1668 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           27        36        59        87  ...    388336    259266    261726  PCOS\n",
              "111  0.323642  0.429946  0.290348  0.397461  ...  0.570499  0.078504  0.000000    -1\n",
              "51   0.466677  0.467885  0.165163  0.271106  ...  0.135349  0.651245  0.332059     0\n",
              "\n",
              "[2 rows x 1668 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FWycs3NoPsN",
        "outputId": "3ae6f6e6-629e-41cf-9a63-37a5c15b6f8e"
      },
      "source": [
        "dae = Adage(original_dim, latent_dim, noise=0.05, batch_size=50,\n",
        "                 epochs=100, sparsity=0, learning_rate=0.0005, loss='mse',\n",
        "                 optimizer='adam', tied_weights=True, verbose=True)\n",
        "dae._build_graph()\n",
        "dae._build_tied_weights_graph()\n",
        "dae._compile_adage()\n",
        "dae._connect_layers()\n",
        "dae.initialize_model()\n",
        "dae.train_adage(pcos_train_df, pcos_test_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 157 samples, validate on 18 samples\n",
            "Epoch 1/100\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0796\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 0s 162us/step - loss: 0.0824 - val_loss: 0.0795\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 0s 182us/step - loss: 0.0823 - val_loss: 0.0795\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 0s 171us/step - loss: 0.0823 - val_loss: 0.0795\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 0s 191us/step - loss: 0.0822 - val_loss: 0.0794\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 0s 126us/step - loss: 0.0821 - val_loss: 0.0794\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 0s 141us/step - loss: 0.0820 - val_loss: 0.0793\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 0s 157us/step - loss: 0.0818 - val_loss: 0.0791\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 0s 141us/step - loss: 0.0816 - val_loss: 0.0790\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 0s 139us/step - loss: 0.0815 - val_loss: 0.0788\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 0s 156us/step - loss: 0.0811 - val_loss: 0.0786\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 0s 146us/step - loss: 0.0809 - val_loss: 0.0784\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 0s 148us/step - loss: 0.0806 - val_loss: 0.0782\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 0s 168us/step - loss: 0.0802 - val_loss: 0.0780\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 0s 181us/step - loss: 0.0799 - val_loss: 0.0778\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 0s 167us/step - loss: 0.0796 - val_loss: 0.0775\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 0s 168us/step - loss: 0.0792 - val_loss: 0.0773\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 0s 160us/step - loss: 0.0789 - val_loss: 0.0771\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 0s 153us/step - loss: 0.0786 - val_loss: 0.0769\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 0s 143us/step - loss: 0.0783 - val_loss: 0.0767\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 0s 150us/step - loss: 0.0781 - val_loss: 0.0765\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 0s 138us/step - loss: 0.0779 - val_loss: 0.0763\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 0s 149us/step - loss: 0.0776 - val_loss: 0.0761\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 0s 170us/step - loss: 0.0778 - val_loss: 0.0760\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 0s 150us/step - loss: 0.0773 - val_loss: 0.0758\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 0s 139us/step - loss: 0.0772 - val_loss: 0.0757\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 0s 132us/step - loss: 0.0772 - val_loss: 0.0756\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 0s 148us/step - loss: 0.0769 - val_loss: 0.0755\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 0s 148us/step - loss: 0.0766 - val_loss: 0.0755\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 0s 171us/step - loss: 0.0766 - val_loss: 0.0754\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 0s 134us/step - loss: 0.0766 - val_loss: 0.0754\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 0s 141us/step - loss: 0.0764 - val_loss: 0.0753\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 0s 134us/step - loss: 0.0766 - val_loss: 0.0752\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 0s 200us/step - loss: 0.0763 - val_loss: 0.0752\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 0s 171us/step - loss: 0.0763 - val_loss: 0.0751\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 0s 149us/step - loss: 0.0762 - val_loss: 0.0751\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 0s 143us/step - loss: 0.0761 - val_loss: 0.0751\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 0s 140us/step - loss: 0.0761 - val_loss: 0.0750\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 0s 154us/step - loss: 0.0759 - val_loss: 0.0750\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 0s 137us/step - loss: 0.0762 - val_loss: 0.0750\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 0s 142us/step - loss: 0.0760 - val_loss: 0.0750\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 0s 141us/step - loss: 0.0760 - val_loss: 0.0749\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 0s 164us/step - loss: 0.0759 - val_loss: 0.0749\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 0s 154us/step - loss: 0.0760 - val_loss: 0.0749\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 0s 137us/step - loss: 0.0758 - val_loss: 0.0749\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 0s 142us/step - loss: 0.0760 - val_loss: 0.0748\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 0s 162us/step - loss: 0.0758 - val_loss: 0.0748\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 0s 171us/step - loss: 0.0758 - val_loss: 0.0748\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 0s 179us/step - loss: 0.0758 - val_loss: 0.0747\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 0s 159us/step - loss: 0.0757 - val_loss: 0.0748\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 0s 159us/step - loss: 0.0756 - val_loss: 0.0747\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 0s 150us/step - loss: 0.0756 - val_loss: 0.0747\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 0s 143us/step - loss: 0.0756 - val_loss: 0.0747\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 0s 196us/step - loss: 0.0759 - val_loss: 0.0746\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 0s 155us/step - loss: 0.0755 - val_loss: 0.0746\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 0s 169us/step - loss: 0.0756 - val_loss: 0.0746\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 0s 145us/step - loss: 0.0754 - val_loss: 0.0746\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 0s 155us/step - loss: 0.0756 - val_loss: 0.0746\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 0s 146us/step - loss: 0.0755 - val_loss: 0.0746\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 0s 186us/step - loss: 0.0752 - val_loss: 0.0745\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 0s 153us/step - loss: 0.0752 - val_loss: 0.0745\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 0s 149us/step - loss: 0.0752 - val_loss: 0.0745\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 0s 171us/step - loss: 0.0754 - val_loss: 0.0744\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 0s 187us/step - loss: 0.0750 - val_loss: 0.0744\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 0s 160us/step - loss: 0.0750 - val_loss: 0.0744\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 0s 167us/step - loss: 0.0749 - val_loss: 0.0743\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 0s 167us/step - loss: 0.0750 - val_loss: 0.0743\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 0s 164us/step - loss: 0.0749 - val_loss: 0.0743\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 0s 157us/step - loss: 0.0746 - val_loss: 0.0742\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 0s 143us/step - loss: 0.0748 - val_loss: 0.0742\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 0s 144us/step - loss: 0.0748 - val_loss: 0.0741\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 0s 170us/step - loss: 0.0750 - val_loss: 0.0741\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 0s 211us/step - loss: 0.0745 - val_loss: 0.0740\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 0s 152us/step - loss: 0.0746 - val_loss: 0.0739\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 0s 149us/step - loss: 0.0745 - val_loss: 0.0739\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 0s 169us/step - loss: 0.0745 - val_loss: 0.0739\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 0s 166us/step - loss: 0.0747 - val_loss: 0.0738\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 0s 158us/step - loss: 0.0744 - val_loss: 0.0738\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 0s 174us/step - loss: 0.0746 - val_loss: 0.0738\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 0s 165us/step - loss: 0.0742 - val_loss: 0.0738\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 0s 162us/step - loss: 0.0743 - val_loss: 0.0737\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 0s 158us/step - loss: 0.0743 - val_loss: 0.0737\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 0s 151us/step - loss: 0.0738 - val_loss: 0.0737\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 0s 151us/step - loss: 0.0742 - val_loss: 0.0737\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 0s 154us/step - loss: 0.0739 - val_loss: 0.0736\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 0s 178us/step - loss: 0.0744 - val_loss: 0.0736\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 0s 157us/step - loss: 0.0742 - val_loss: 0.0736\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 0s 152us/step - loss: 0.0739 - val_loss: 0.0735\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 0s 161us/step - loss: 0.0737 - val_loss: 0.0735\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 0s 152us/step - loss: 0.0738 - val_loss: 0.0734\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 0s 155us/step - loss: 0.0737 - val_loss: 0.0734\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 0s 155us/step - loss: 0.0743 - val_loss: 0.0734\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 0s 164us/step - loss: 0.0738 - val_loss: 0.0733\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 0s 154us/step - loss: 0.0737 - val_loss: 0.0733\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 0s 148us/step - loss: 0.0739 - val_loss: 0.0733\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 0s 152us/step - loss: 0.0737 - val_loss: 0.0732\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 0s 143us/step - loss: 0.0737 - val_loss: 0.0732\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 0s 154us/step - loss: 0.0740 - val_loss: 0.0732\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 0s 192us/step - loss: 0.0735 - val_loss: 0.0732\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 0s 165us/step - loss: 0.0741 - val_loss: 0.0732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FoKw1bto4Bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "bee8826f-1d7c-4947-a541-8de1075cb57c"
      },
      "source": [
        "plt.figure()\n",
        "ax = dae.history_df.plot()\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('DAE Loss')\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"hist_plot_file.png\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN5nBCQFpgD1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}