{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "vae_retrial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyamanojna/CS6024-Algorithmic-Approaches-to-Computational-Biology-Project/blob/master/codes/vae_dae/vae_retrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKTFGrLYdT2b",
        "outputId": "e157da4d-9014-4acb-964b-514d17156c9d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import optimizers, metrics\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Input, Dense, Lambda, Activation, Dropout, Layer\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCMdyI3YdVOq",
        "outputId": "e334098f-6c5d-4f91-bbd2-c8173805b7b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zs8l2gkdT2g"
      },
      "source": [
        "# From tybalt.utils.vae_utils\n",
        "def approx_keras_binary_cross_entropy(x, z, p, epsilon=1e-07):\n",
        "    \"\"\"\n",
        "    Function to approximate Keras `binary_crossentropy()`\n",
        "    https://github.com/keras-team/keras/blob/e6c3f77b0b10b0d76778109a40d6d3282f1cadd0/keras/losses.py#L76\n",
        "    Which is a wrapper for TensorFlow `sigmoid_cross_entropy_with_logits()`\n",
        "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
        "    An important step is to clip values of reconstruction\n",
        "    https://github.com/keras-team/keras/blob/a3d160b9467c99cbb27f9aa0382c759f45c8ee66/keras/backend/tensorflow_backend.py#L3071\n",
        "    Arguments:\n",
        "    x - Reconstructed input RNAseq data\n",
        "    z - Input RNAseq data\n",
        "    p - number of features\n",
        "    epsilon - the clipping value to stabilize results (same Keras default)\n",
        "    \"\"\"\n",
        "    # Ensure numpy arrays\n",
        "    x = np.array(x)\n",
        "    z = np.array(z)\n",
        "\n",
        "    # Add clip to value\n",
        "    x[x < epsilon] = epsilon\n",
        "    x[x > (1 - epsilon)] = (1 - epsilon)\n",
        "\n",
        "    # Perform logit\n",
        "    x = np.log(x / (1 - x))\n",
        "\n",
        "    # Return approximate binary cross entropy\n",
        "    return np.mean(p * np.mean(- x * z + np.log(1 + np.exp(x)), axis=-1))\n",
        "\n",
        "\n",
        "class VariationalLayer(Layer):\n",
        "    \"\"\"\n",
        "    Define a custom layer that learns and performs the training\n",
        "    \"\"\"\n",
        "    def __init__(self, var_layer, mean_layer, original_dim, beta, loss,\n",
        "                 **kwargs):\n",
        "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
        "        self.is_placeholder = True\n",
        "        self.var_layer = var_layer\n",
        "        self.mean_layer = mean_layer\n",
        "        self.original_dim = original_dim\n",
        "        self.beta = beta\n",
        "        self.loss = loss\n",
        "        super(VariationalLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def vae_loss(self, x_input, x_decoded):\n",
        "        if self.loss == 'binary_crossentropy':\n",
        "            recon_loss = self.original_dim * \\\n",
        "                         metrics.binary_crossentropy(x_input, x_decoded)\n",
        "        elif self.loss == 'mse':\n",
        "            recon_loss = self.original_dim * \\\n",
        "                         metrics.mean_squared_error(x_input, x_decoded)\n",
        "\n",
        "        kl_loss = - 0.5 * K.sum(1 + self.var_layer -\n",
        "                                K.square(self.mean_layer) -\n",
        "                                K.exp(self.var_layer), axis=-1)\n",
        "\n",
        "        return K.mean(recon_loss + (K.get_value(self.beta) * kl_loss))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, x_decoded = inputs\n",
        "        loss = self.vae_loss(x, x_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # We won't actually use the output.\n",
        "        return x\n",
        "\n",
        "\n",
        "class WarmUpCallback(Callback):\n",
        "    def __init__(self, beta, kappa):\n",
        "        self.beta = beta\n",
        "        self.kappa = kappa\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \"\"\"\n",
        "        Behavior on each epoch\n",
        "        \"\"\"\n",
        "        if K.get_value(self.beta) <= 1:\n",
        "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\n",
        "\n",
        "\n",
        "class LossCallback(Callback):\n",
        "    def __init__(self, training_data, original_dim, encoder_cbk, decoder_cbk):\n",
        "        self.training_data = training_data\n",
        "        self.original_dim = original_dim\n",
        "        self.encoder_cbk = encoder_cbk\n",
        "        self.decoder_cbk = decoder_cbk\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.xent_loss = []\n",
        "        self.kl_loss = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        recon = self.decoder_cbk.predict(\n",
        "            self.encoder_cbk.predict(self.training_data))\n",
        "        xent_loss = approx_keras_binary_cross_entropy(x=recon,\n",
        "                                                      z=self.training_data,\n",
        "                                                      p=self.original_dim)\n",
        "        full_loss = logs.get('loss')\n",
        "        self.xent_loss.append(xent_loss)\n",
        "        self.kl_loss.append(full_loss - xent_loss)\n",
        "        return"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuVXCvSRdT2j"
      },
      "source": [
        "# From tybalt.utils.base\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "class BaseModel():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_summary(self):\n",
        "        self.full_model.summary()\n",
        "\n",
        "    def visualize_architecture(self, output_file):\n",
        "        # Visualize the connections of the custom VAE model\n",
        "        plot_model(self.full_model, to_file=output_file)\n",
        "\n",
        "    def visualize_training(self, output_file=None):\n",
        "        # Visualize training performance\n",
        "        history_df = pd.DataFrame(self.hist.history)\n",
        "        ax = history_df.plot()\n",
        "        ax.set_xlabel('Epochs')\n",
        "        ax.set_ylabel('Loss')\n",
        "        fig = ax.get_figure()\n",
        "        if output_file:\n",
        "            fig.savefig(output_file)\n",
        "        else:\n",
        "            fig.show()\n",
        "\n",
        "    def get_weights(self, decoder=True):\n",
        "        # Extract weight matrices from encoder or decoder\n",
        "        weights = []\n",
        "        if decoder:\n",
        "            for layer in self.decoder.layers:\n",
        "                weights.append(layer.get_weights())\n",
        "        else:\n",
        "            for layer in self.encoder.layers:\n",
        "                # Encoder weights must be transposed\n",
        "                encoder_weights = layer.get_weights()\n",
        "                encoder_weights = [np.transpose(x) for x in encoder_weights]\n",
        "                weights.append(encoder_weights)\n",
        "        return weights\n",
        "\n",
        "    def save_models(self, encoder_file, decoder_file):\n",
        "        self.encoder.save(encoder_file)\n",
        "        self.decoder.save(decoder_file)\n",
        "\n",
        "\n",
        "class VAE(BaseModel):\n",
        "    def __init__(self):\n",
        "        BaseModel.__init__(self)\n",
        "\n",
        "    def _sampling(self, args):\n",
        "        \"\"\"\n",
        "        Function for reparameterization trick to make model differentiable\n",
        "        \"\"\"\n",
        "        # Function with args required for Keras Lambda function\n",
        "        z_mean, z_log_var = args\n",
        "\n",
        "        # Draw epsilon of the same shape from a standard normal distribution\n",
        "        epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
        "                                  stddev=self.epsilon_std)\n",
        "\n",
        "        # The latent vector is non-deterministic and differentiable\n",
        "        # in respect to z_mean and z_log_var\n",
        "        z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "        return z\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"\n",
        "        Helper function to run that builds and compiles Keras layers\n",
        "        \"\"\"\n",
        "        self._build_encoder_layer()\n",
        "        self._build_decoder_layer()\n",
        "        self._compile_vae()\n",
        "        self._connect_layers()\n",
        "\n",
        "    def compress(self, df):\n",
        "        # Encode rnaseq into the hidden/latent representation - and save output\n",
        "        # a cVAE expects a list of [rnaseq_df, y_df]\n",
        "        encoded_df = self.encoder.predict_on_batch(df)\n",
        "\n",
        "        if self.model_name == 'cTybalt':\n",
        "            named_index = df[0].index\n",
        "        else:\n",
        "            named_index = df.index\n",
        "\n",
        "        encoded_df = pd.DataFrame(encoded_df,\n",
        "                                  columns=range(1, self.latent_dim + 1),\n",
        "                                  index=named_index)\n",
        "        return encoded_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSPN84F-dT2l",
        "outputId": "c2911728-b66a-4a50-d596-1c6f66f437ce"
      },
      "source": [
        "class Tybalt(VAE):\n",
        "    \"\"\"\n",
        "    Training and evaluation of a tybalt model\n",
        "    Usage: from tybalt.models import Tybalt\n",
        "    \"\"\"\n",
        "    def __init__(self, original_dim, latent_dim, batch_size=50, epochs=50,\n",
        "                 learning_rate=0.0005, kappa=1, epsilon_std=1.0,\n",
        "                 beta=K.variable(0), loss='binary_crossentropy',\n",
        "                 verbose=True):\n",
        "        VAE.__init__(self)\n",
        "        self.model_name = 'Tybalt'\n",
        "        self.original_dim = original_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.kappa = kappa\n",
        "        self.epsilon_std = epsilon_std\n",
        "        self.beta = beta\n",
        "        self.loss = loss\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _build_encoder_layer(self):\n",
        "        \"\"\"\n",
        "        Function to build the encoder layer connections\n",
        "        \"\"\"\n",
        "        # Input place holder for RNAseq data with specific input size\n",
        "        self.rnaseq_input = Input(shape=(self.original_dim, ))\n",
        "\n",
        "        # Input layer is compressed into a mean and log variance vector of\n",
        "        # size `latent_dim`. Each layer is initialized with glorot uniform\n",
        "        # weights and each step (dense connections, batch norm, and relu\n",
        "        # activation) are funneled separately.\n",
        "        # Each vector are connected to the rnaseq input tensor\n",
        "\n",
        "        # input layer to latent mean layer\n",
        "        z_mean = Dense(self.latent_dim,\n",
        "                       kernel_initializer='glorot_uniform')(self.rnaseq_input)\n",
        "        z_mean_batchnorm = BatchNormalization()(z_mean)\n",
        "        self.z_mean_encoded = Activation('relu')(z_mean_batchnorm)\n",
        "\n",
        "        # input layer to latent standard deviation layer\n",
        "        z_var = Dense(self.latent_dim,\n",
        "                      kernel_initializer='glorot_uniform')(self.rnaseq_input)\n",
        "        z_var_batchnorm = BatchNormalization()(z_var)\n",
        "        self.z_var_encoded = Activation('relu')(z_var_batchnorm)\n",
        "\n",
        "        # return the encoded and randomly sampled z vector\n",
        "        # Takes two keras layers as input to the custom sampling function layer\n",
        "        self.z = Lambda(self._sampling,\n",
        "                        output_shape=(self.latent_dim, ))([self.z_mean_encoded,\n",
        "                                                           self.z_var_encoded])\n",
        "\n",
        "    def _build_decoder_layer(self):\n",
        "        \"\"\"\n",
        "        Function to build the decoder layer connections\n",
        "        \"\"\"\n",
        "        # The decoding layer is much simpler with a single layer glorot uniform\n",
        "        # initialized and sigmoid activation\n",
        "        self.decoder_model = Sequential()\n",
        "        self.decoder_model.add(Dense(self.original_dim, activation='sigmoid',\n",
        "                                     input_dim=self.latent_dim))\n",
        "        self.rnaseq_reconstruct = self.decoder_model(self.z)\n",
        "\n",
        "    def _compile_vae(self):\n",
        "        \"\"\"\n",
        "        Creates the vae layer and compiles all layer connections\n",
        "        \"\"\"\n",
        "        adam = optimizers.Adam(lr=self.learning_rate)\n",
        "        vae_layer = VariationalLayer(var_layer=self.z_var_encoded,\n",
        "                                     mean_layer=self.z_mean_encoded,\n",
        "                                     original_dim=self.original_dim,\n",
        "                                     beta=self.beta, loss=self.loss)(\n",
        "                                [self.rnaseq_input, self.rnaseq_reconstruct])\n",
        "        self.full_model = Model(self.rnaseq_input, vae_layer)\n",
        "        self.full_model.compile(optimizer=adam, loss=None,\n",
        "                                loss_weights=[self.beta])\n",
        "\n",
        "    def _connect_layers(self):\n",
        "        \"\"\"\n",
        "        Make connections between layers to build separate encoder and decoder\n",
        "        \"\"\"\n",
        "        self.encoder = Model(self.rnaseq_input, self.z_mean_encoded)\n",
        "\n",
        "        decoder_input = Input(shape=(self.latent_dim, ))\n",
        "        _x_decoded_mean = self.decoder_model(decoder_input)\n",
        "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
        "\n",
        "    def train_vae(self, train_df, test_df, separate_loss=False):\n",
        "        \"\"\"\n",
        "        Method to train model.\n",
        "        `separate_loss` instantiates a custom Keras callback that tracks the\n",
        "        separate contribution of reconstruction and KL divergence loss. Because\n",
        "        VAEs try to minimize both, it may be informative to track each across\n",
        "        training separately. The callback processes the training data through\n",
        "        the current encoder and decoder and therefore requires additional time\n",
        "        - which is why this is not done by default.\n",
        "        \"\"\"\n",
        "        cbks = [WarmUpCallback(self.beta, self.kappa)]\n",
        "        if separate_loss:\n",
        "            tybalt_loss_cbk = LossCallback(training_data=np.array(train_df),\n",
        "                                           encoder_cbk=self.encoder,\n",
        "                                           decoder_cbk=self.decoder,\n",
        "                                           original_dim=self.original_dim)\n",
        "            cbks += [tybalt_loss_cbk]\n",
        "\n",
        "        self.hist = self.full_model.fit(np.array(train_df),\n",
        "                                        shuffle=True,\n",
        "                                        epochs=self.epochs,\n",
        "                                        batch_size=self.batch_size,\n",
        "                                        verbose=self.verbose,\n",
        "                                        validation_data=(np.array(test_df),\n",
        "                                                         None),\n",
        "                                        callbacks=cbks)\n",
        "        self.history_df = pd.DataFrame(self.hist.history)\n",
        "\n",
        "        if separate_loss:\n",
        "            self.history_df = self.history_df.assign(\n",
        "                                recon=tybalt_loss_cbk.xent_loss)\n",
        "            self.history_df = self.history_df.assign(\n",
        "                                kl=tybalt_loss_cbk.kl_loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHdiKm9TdT2m"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "zDFFMkw1dT2o",
        "outputId": "195c90e3-61ed-4431-c9b1-a09b55b11b1c"
      },
      "source": [
        "pcos_df = pd.read_csv('/content/drive/MyDrive/aacb_project/datasets/common_normalized.csv', index_col=0)\n",
        "pcos_df = pcos_df.drop(['sample_id'], axis=1)\n",
        "\n",
        "# Split 10% test set randomly\n",
        "test_set_percent = 0.1\n",
        "\n",
        "pcos_test_df = pcos_df.sample(frac=test_set_percent)\n",
        "pcos_train_df = pcos_df.drop(pcos_test_df.index)\n",
        "\n",
        "display(pcos_train_df.head(2))\n",
        "display(pcos_test_df.head(2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>27</th>\n",
              "      <th>36</th>\n",
              "      <th>59</th>\n",
              "      <th>87</th>\n",
              "      <th>94</th>\n",
              "      <th>105</th>\n",
              "      <th>153</th>\n",
              "      <th>159</th>\n",
              "      <th>164</th>\n",
              "      <th>226</th>\n",
              "      <th>288</th>\n",
              "      <th>290</th>\n",
              "      <th>311</th>\n",
              "      <th>330</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>345</th>\n",
              "      <th>355</th>\n",
              "      <th>359</th>\n",
              "      <th>377</th>\n",
              "      <th>382</th>\n",
              "      <th>389</th>\n",
              "      <th>392</th>\n",
              "      <th>394</th>\n",
              "      <th>405</th>\n",
              "      <th>408</th>\n",
              "      <th>420</th>\n",
              "      <th>430</th>\n",
              "      <th>443</th>\n",
              "      <th>463</th>\n",
              "      <th>476</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>515</th>\n",
              "      <th>533</th>\n",
              "      <th>552</th>\n",
              "      <th>567</th>\n",
              "      <th>572</th>\n",
              "      <th>582</th>\n",
              "      <th>586</th>\n",
              "      <th>...</th>\n",
              "      <th>92558</th>\n",
              "      <th>92815</th>\n",
              "      <th>92822</th>\n",
              "      <th>93164</th>\n",
              "      <th>93487</th>\n",
              "      <th>93974</th>\n",
              "      <th>6248_84301</th>\n",
              "      <th>8693_100528030</th>\n",
              "      <th>100506581</th>\n",
              "      <th>112399</th>\n",
              "      <th>112479</th>\n",
              "      <th>374655</th>\n",
              "      <th>375035</th>\n",
              "      <th>375057</th>\n",
              "      <th>113251</th>\n",
              "      <th>114088</th>\n",
              "      <th>114791</th>\n",
              "      <th>114882</th>\n",
              "      <th>116228</th>\n",
              "      <th>116285</th>\n",
              "      <th>116985</th>\n",
              "      <th>116986</th>\n",
              "      <th>51463_653519</th>\n",
              "      <th>118491</th>\n",
              "      <th>118987</th>\n",
              "      <th>120227</th>\n",
              "      <th>645644</th>\n",
              "      <th>100129482</th>\n",
              "      <th>100529257_55333</th>\n",
              "      <th>253512</th>\n",
              "      <th>122704</th>\n",
              "      <th>253959</th>\n",
              "      <th>254359</th>\n",
              "      <th>254531</th>\n",
              "      <th>100132341</th>\n",
              "      <th>387893</th>\n",
              "      <th>388336</th>\n",
              "      <th>259266</th>\n",
              "      <th>261726</th>\n",
              "      <th>PCOS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.693258</td>\n",
              "      <td>0.125461</td>\n",
              "      <td>0.336077</td>\n",
              "      <td>0.044463</td>\n",
              "      <td>0.267819</td>\n",
              "      <td>0.467742</td>\n",
              "      <td>0.490196</td>\n",
              "      <td>0.008907</td>\n",
              "      <td>0.370576</td>\n",
              "      <td>0.953515</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.060236</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>0.078014</td>\n",
              "      <td>0.119300</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712054</td>\n",
              "      <td>0.648867</td>\n",
              "      <td>0.535433</td>\n",
              "      <td>0.593551</td>\n",
              "      <td>0.651320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.132791</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.263318</td>\n",
              "      <td>0.725872</td>\n",
              "      <td>0.701149</td>\n",
              "      <td>0.395953</td>\n",
              "      <td>0.302829</td>\n",
              "      <td>0.839330</td>\n",
              "      <td>0.349876</td>\n",
              "      <td>0.835372</td>\n",
              "      <td>0.502399</td>\n",
              "      <td>0.800414</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.609797</td>\n",
              "      <td>0.146067</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.915703</td>\n",
              "      <td>0.768041</td>\n",
              "      <td>0.925659</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855740</td>\n",
              "      <td>0.130112</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.086751</td>\n",
              "      <td>0.288095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.560748</td>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.261745</td>\n",
              "      <td>0.209231</td>\n",
              "      <td>0.061603</td>\n",
              "      <td>0.295745</td>\n",
              "      <td>0.489703</td>\n",
              "      <td>0.481948</td>\n",
              "      <td>0.113924</td>\n",
              "      <td>0.100254</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.39619</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.412466</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312354</td>\n",
              "      <td>0.198387</td>\n",
              "      <td>0.120213</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.214607</td>\n",
              "      <td>0.487085</td>\n",
              "      <td>0.589704</td>\n",
              "      <td>0.104294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.631829</td>\n",
              "      <td>0.476058</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.038093</td>\n",
              "      <td>0.799387</td>\n",
              "      <td>0.131206</td>\n",
              "      <td>0.118719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025316</td>\n",
              "      <td>0.216724</td>\n",
              "      <td>0.023437</td>\n",
              "      <td>0.286346</td>\n",
              "      <td>0.619005</td>\n",
              "      <td>0.946925</td>\n",
              "      <td>0.854954</td>\n",
              "      <td>0.337931</td>\n",
              "      <td>0.352278</td>\n",
              "      <td>0.149398</td>\n",
              "      <td>0.636856</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.184211</td>\n",
              "      <td>0.238965</td>\n",
              "      <td>0.808785</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.561854</td>\n",
              "      <td>0.916302</td>\n",
              "      <td>0.265155</td>\n",
              "      <td>0.285360</td>\n",
              "      <td>0.808202</td>\n",
              "      <td>0.291843</td>\n",
              "      <td>0.515512</td>\n",
              "      <td>0.084831</td>\n",
              "      <td>...</td>\n",
              "      <td>0.677019</td>\n",
              "      <td>0.142455</td>\n",
              "      <td>0.070225</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.512184</td>\n",
              "      <td>0.694845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.536028</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.809129</td>\n",
              "      <td>0.158155</td>\n",
              "      <td>0.586751</td>\n",
              "      <td>0.424603</td>\n",
              "      <td>0.644419</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.949077</td>\n",
              "      <td>0.994318</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.427987</td>\n",
              "      <td>0.049664</td>\n",
              "      <td>0.763077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.080092</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082278</td>\n",
              "      <td>0.407360</td>\n",
              "      <td>0.418699</td>\n",
              "      <td>0.437113</td>\n",
              "      <td>0.367855</td>\n",
              "      <td>0.56419</td>\n",
              "      <td>0.497418</td>\n",
              "      <td>0.325390</td>\n",
              "      <td>0.356499</td>\n",
              "      <td>0.173660</td>\n",
              "      <td>0.430645</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1668 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         27        36        59        87  ...    388336    259266    261726  PCOS\n",
              "0  0.693258  0.125461  0.336077  0.044463  ...  0.312354  0.198387  0.120213     1\n",
              "1  0.214607  0.487085  0.589704  0.104294  ...  0.173660  0.430645  1.000000     1\n",
              "\n",
              "[2 rows x 1668 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>27</th>\n",
              "      <th>36</th>\n",
              "      <th>59</th>\n",
              "      <th>87</th>\n",
              "      <th>94</th>\n",
              "      <th>105</th>\n",
              "      <th>153</th>\n",
              "      <th>159</th>\n",
              "      <th>164</th>\n",
              "      <th>226</th>\n",
              "      <th>288</th>\n",
              "      <th>290</th>\n",
              "      <th>311</th>\n",
              "      <th>330</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>345</th>\n",
              "      <th>355</th>\n",
              "      <th>359</th>\n",
              "      <th>377</th>\n",
              "      <th>382</th>\n",
              "      <th>389</th>\n",
              "      <th>392</th>\n",
              "      <th>394</th>\n",
              "      <th>405</th>\n",
              "      <th>408</th>\n",
              "      <th>420</th>\n",
              "      <th>430</th>\n",
              "      <th>443</th>\n",
              "      <th>463</th>\n",
              "      <th>476</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>515</th>\n",
              "      <th>533</th>\n",
              "      <th>552</th>\n",
              "      <th>567</th>\n",
              "      <th>572</th>\n",
              "      <th>582</th>\n",
              "      <th>586</th>\n",
              "      <th>...</th>\n",
              "      <th>92558</th>\n",
              "      <th>92815</th>\n",
              "      <th>92822</th>\n",
              "      <th>93164</th>\n",
              "      <th>93487</th>\n",
              "      <th>93974</th>\n",
              "      <th>6248_84301</th>\n",
              "      <th>8693_100528030</th>\n",
              "      <th>100506581</th>\n",
              "      <th>112399</th>\n",
              "      <th>112479</th>\n",
              "      <th>374655</th>\n",
              "      <th>375035</th>\n",
              "      <th>375057</th>\n",
              "      <th>113251</th>\n",
              "      <th>114088</th>\n",
              "      <th>114791</th>\n",
              "      <th>114882</th>\n",
              "      <th>116228</th>\n",
              "      <th>116285</th>\n",
              "      <th>116985</th>\n",
              "      <th>116986</th>\n",
              "      <th>51463_653519</th>\n",
              "      <th>118491</th>\n",
              "      <th>118987</th>\n",
              "      <th>120227</th>\n",
              "      <th>645644</th>\n",
              "      <th>100129482</th>\n",
              "      <th>100529257_55333</th>\n",
              "      <th>253512</th>\n",
              "      <th>122704</th>\n",
              "      <th>253959</th>\n",
              "      <th>254359</th>\n",
              "      <th>254531</th>\n",
              "      <th>100132341</th>\n",
              "      <th>387893</th>\n",
              "      <th>388336</th>\n",
              "      <th>259266</th>\n",
              "      <th>261726</th>\n",
              "      <th>PCOS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.129505</td>\n",
              "      <td>0.181005</td>\n",
              "      <td>0.924609</td>\n",
              "      <td>0.148352</td>\n",
              "      <td>0.667476</td>\n",
              "      <td>0.308571</td>\n",
              "      <td>0.399707</td>\n",
              "      <td>0.072805</td>\n",
              "      <td>0.283274</td>\n",
              "      <td>0.166885</td>\n",
              "      <td>0.092105</td>\n",
              "      <td>0.233102</td>\n",
              "      <td>0.950314</td>\n",
              "      <td>0.087576</td>\n",
              "      <td>0.165382</td>\n",
              "      <td>0.101295</td>\n",
              "      <td>0.850775</td>\n",
              "      <td>0.391562</td>\n",
              "      <td>0.709706</td>\n",
              "      <td>0.504050</td>\n",
              "      <td>0.032042</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.201385</td>\n",
              "      <td>0.237093</td>\n",
              "      <td>0.532035</td>\n",
              "      <td>0.320378</td>\n",
              "      <td>0.174917</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>0.527110</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.118644</td>\n",
              "      <td>0.394805</td>\n",
              "      <td>0.304545</td>\n",
              "      <td>0.886182</td>\n",
              "      <td>0.691932</td>\n",
              "      <td>0.732815</td>\n",
              "      <td>0.247788</td>\n",
              "      <td>0.526245</td>\n",
              "      <td>0.343293</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>0.147849</td>\n",
              "      <td>0.537037</td>\n",
              "      <td>0.190012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.535132</td>\n",
              "      <td>0.519948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080819</td>\n",
              "      <td>0.066770</td>\n",
              "      <td>0.055517</td>\n",
              "      <td>0.548400</td>\n",
              "      <td>0.675879</td>\n",
              "      <td>0.822034</td>\n",
              "      <td>0.850120</td>\n",
              "      <td>0.210145</td>\n",
              "      <td>0.366419</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.363089</td>\n",
              "      <td>0.227196</td>\n",
              "      <td>0.156654</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>0.559778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.689993</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.788043</td>\n",
              "      <td>0.178251</td>\n",
              "      <td>0.496886</td>\n",
              "      <td>0.900915</td>\n",
              "      <td>0.715411</td>\n",
              "      <td>0.473233</td>\n",
              "      <td>0.188396</td>\n",
              "      <td>0.245631</td>\n",
              "      <td>0.196678</td>\n",
              "      <td>0.648442</td>\n",
              "      <td>0.184979</td>\n",
              "      <td>0.385505</td>\n",
              "      <td>0.644742</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.239703</td>\n",
              "      <td>0.649477</td>\n",
              "      <td>0.078745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.266449</td>\n",
              "      <td>0.146422</td>\n",
              "      <td>0.431584</td>\n",
              "      <td>0.503799</td>\n",
              "      <td>0.520344</td>\n",
              "      <td>0.364528</td>\n",
              "      <td>0.485421</td>\n",
              "      <td>0.039343</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.253401</td>\n",
              "      <td>0.356205</td>\n",
              "      <td>0.028227</td>\n",
              "      <td>0.092454</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.126762</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>0.363461</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150554</td>\n",
              "      <td>0.572913</td>\n",
              "      <td>0.207787</td>\n",
              "      <td>0.080381</td>\n",
              "      <td>0.646698</td>\n",
              "      <td>0.186215</td>\n",
              "      <td>0.823966</td>\n",
              "      <td>0.269807</td>\n",
              "      <td>0.660864</td>\n",
              "      <td>0.426235</td>\n",
              "      <td>0.069061</td>\n",
              "      <td>0.588288</td>\n",
              "      <td>0.249296</td>\n",
              "      <td>0.386684</td>\n",
              "      <td>0.219280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.917404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272666</td>\n",
              "      <td>0.824473</td>\n",
              "      <td>0.250663</td>\n",
              "      <td>0.764054</td>\n",
              "      <td>0.260222</td>\n",
              "      <td>0.250448</td>\n",
              "      <td>0.476643</td>\n",
              "      <td>0.015934</td>\n",
              "      <td>0.282625</td>\n",
              "      <td>0.342973</td>\n",
              "      <td>0.180708</td>\n",
              "      <td>0.435645</td>\n",
              "      <td>0.233691</td>\n",
              "      <td>0.417996</td>\n",
              "      <td>0.158665</td>\n",
              "      <td>0.343397</td>\n",
              "      <td>0.55412</td>\n",
              "      <td>0.640270</td>\n",
              "      <td>0.462755</td>\n",
              "      <td>0.439467</td>\n",
              "      <td>0.057574</td>\n",
              "      <td>0.888190</td>\n",
              "      <td>0.531518</td>\n",
              "      <td>0.344512</td>\n",
              "      <td>0.179327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.575808</td>\n",
              "      <td>0.457257</td>\n",
              "      <td>0.239723</td>\n",
              "      <td>0.203369</td>\n",
              "      <td>0.214708</td>\n",
              "      <td>0.173022</td>\n",
              "      <td>0.689886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071049</td>\n",
              "      <td>0.281503</td>\n",
              "      <td>0.916737</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1668 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           27        36        59        87  ...    388336    259266    261726  PCOS\n",
              "23   0.129505  0.181005  0.924609  0.148352  ...  0.184979  0.385505  0.644742     0\n",
              "109  0.239703  0.649477  0.078745  0.000000  ...  0.071049  0.281503  0.916737    -1\n",
              "\n",
              "[2 rows x 1668 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_66nYg9dT2r"
      },
      "source": [
        "original_dim = pcos_df.shape[1]\n",
        "latent_dim = 2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukdd8eJIdT2s",
        "outputId": "bb4b8eed-569d-4e3b-fa82-ccc43a572476"
      },
      "source": [
        "vae = Tybalt(original_dim, latent_dim, batch_size=50, epochs=2000,\n",
        "                 learning_rate=0.005, kappa=1, epsilon_std=1.0,\n",
        "                 beta=K.variable(0), loss='binary_crossentropy',\n",
        "                 verbose=True)\n",
        "vae._build_encoder_layer()\n",
        "vae._build_decoder_layer()\n",
        "vae._compile_vae()\n",
        "vae._connect_layers()\n",
        "\n",
        "vae.train_vae(pcos_train_df, pcos_test_df, separate_loss=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_3:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training_utils.py:819: UserWarning: Output variational_layer_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to variational_layer_3.\n",
            "  'be expecting any data to be passed to {0}.'.format(name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 157 samples, validate on 18 samples\n",
            "Epoch 1/2000\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 1157.1926 - val_loss: 1155.6572\n",
            "Epoch 2/2000\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1155.1016 - val_loss: 1157.8860\n",
            "Epoch 3/2000\n",
            "157/157 [==============================] - 0s 168us/step - loss: 1153.1345 - val_loss: 1155.7478\n",
            "Epoch 4/2000\n",
            "157/157 [==============================] - 0s 168us/step - loss: 1151.2876 - val_loss: 1159.0933\n",
            "Epoch 5/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1148.9696 - val_loss: 1200.1495\n",
            "Epoch 6/2000\n",
            "157/157 [==============================] - 0s 166us/step - loss: 1147.2134 - val_loss: 1832.7251\n",
            "Epoch 7/2000\n",
            "157/157 [==============================] - 0s 163us/step - loss: 1146.4955 - val_loss: 1863.6591\n",
            "Epoch 8/2000\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1145.5831 - val_loss: 2320.9868\n",
            "Epoch 9/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1142.4007 - val_loss: 2254.2249\n",
            "Epoch 10/2000\n",
            "157/157 [==============================] - 0s 170us/step - loss: 1142.0565 - val_loss: 2112.6047\n",
            "Epoch 11/2000\n",
            "157/157 [==============================] - 0s 160us/step - loss: 1139.6902 - val_loss: 1837.1597\n",
            "Epoch 12/2000\n",
            "157/157 [==============================] - 0s 174us/step - loss: 1139.1964 - val_loss: 1624.7134\n",
            "Epoch 13/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1136.8727 - val_loss: 1751.1968\n",
            "Epoch 14/2000\n",
            "157/157 [==============================] - 0s 158us/step - loss: 1135.3315 - val_loss: 1317.0249\n",
            "Epoch 15/2000\n",
            "157/157 [==============================] - 0s 165us/step - loss: 1134.2385 - val_loss: 1261.4132\n",
            "Epoch 16/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1133.8324 - val_loss: 1201.5861\n",
            "Epoch 17/2000\n",
            "157/157 [==============================] - 0s 166us/step - loss: 1132.2491 - val_loss: 1205.3438\n",
            "Epoch 18/2000\n",
            "157/157 [==============================] - 0s 170us/step - loss: 1132.1703 - val_loss: 1155.3169\n",
            "Epoch 19/2000\n",
            "157/157 [==============================] - 0s 161us/step - loss: 1128.1856 - val_loss: 1152.5695\n",
            "Epoch 20/2000\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1130.9149 - val_loss: 1181.7645\n",
            "Epoch 21/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1128.7931 - val_loss: 1152.4542\n",
            "Epoch 22/2000\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1129.6382 - val_loss: 1138.8420\n",
            "Epoch 23/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1126.3015 - val_loss: 1150.7444\n",
            "Epoch 24/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1125.4165 - val_loss: 1125.3257\n",
            "Epoch 25/2000\n",
            "157/157 [==============================] - 0s 184us/step - loss: 1128.0940 - val_loss: 1135.5326\n",
            "Epoch 26/2000\n",
            "157/157 [==============================] - 0s 164us/step - loss: 1126.2209 - val_loss: 1123.5728\n",
            "Epoch 27/2000\n",
            "157/157 [==============================] - 0s 174us/step - loss: 1126.0417 - val_loss: 1115.9556\n",
            "Epoch 28/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1123.2147 - val_loss: 1130.8502\n",
            "Epoch 29/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1122.4292 - val_loss: 1136.5002\n",
            "Epoch 30/2000\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1125.0487 - val_loss: 1129.7003\n",
            "Epoch 31/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1123.6686 - val_loss: 1124.8882\n",
            "Epoch 32/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1121.7907 - val_loss: 1115.5060\n",
            "Epoch 33/2000\n",
            "157/157 [==============================] - 0s 167us/step - loss: 1123.7850 - val_loss: 1120.8214\n",
            "Epoch 34/2000\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1122.0169 - val_loss: 1119.7173\n",
            "Epoch 35/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1122.2067 - val_loss: 1129.3685\n",
            "Epoch 36/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1122.8434 - val_loss: 1135.9648\n",
            "Epoch 37/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1121.5110 - val_loss: 1130.8353\n",
            "Epoch 38/2000\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1122.1685 - val_loss: 1124.9553\n",
            "Epoch 39/2000\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1121.5332 - val_loss: 1132.0688\n",
            "Epoch 40/2000\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1121.5948 - val_loss: 1135.9508\n",
            "Epoch 41/2000\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1118.6728 - val_loss: 1123.6063\n",
            "Epoch 42/2000\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1119.0662 - val_loss: 1126.3593\n",
            "Epoch 43/2000\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1118.2532 - val_loss: 1133.8618\n",
            "Epoch 44/2000\n",
            "157/157 [==============================] - 0s 184us/step - loss: 1120.4073 - val_loss: 1124.8856\n",
            "Epoch 45/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1118.9254 - val_loss: 1143.1946\n",
            "Epoch 46/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1117.0538 - val_loss: 1127.7142\n",
            "Epoch 47/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1119.7526 - val_loss: 1120.7133\n",
            "Epoch 48/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1117.5135 - val_loss: 1127.1747\n",
            "Epoch 49/2000\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1117.7466 - val_loss: 1129.2280\n",
            "Epoch 50/2000\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1119.0443 - val_loss: 1140.0142\n",
            "Epoch 51/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1118.6033 - val_loss: 1135.3232\n",
            "Epoch 52/2000\n",
            "157/157 [==============================] - 0s 172us/step - loss: 1117.0925 - val_loss: 1133.5092\n",
            "Epoch 53/2000\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1117.7736 - val_loss: 1131.4268\n",
            "Epoch 54/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1116.5898 - val_loss: 1122.6277\n",
            "Epoch 55/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1118.7849 - val_loss: 1121.4175\n",
            "Epoch 56/2000\n",
            "157/157 [==============================] - 0s 169us/step - loss: 1116.1356 - val_loss: 1118.1619\n",
            "Epoch 57/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1117.4719 - val_loss: 1124.9412\n",
            "Epoch 58/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1117.6069 - val_loss: 1116.2754\n",
            "Epoch 59/2000\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1118.3052 - val_loss: 1118.5305\n",
            "Epoch 60/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1116.1249 - val_loss: 1124.5101\n",
            "Epoch 61/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1116.0668 - val_loss: 1124.1675\n",
            "Epoch 62/2000\n",
            "157/157 [==============================] - 0s 167us/step - loss: 1118.9323 - val_loss: 1120.4729\n",
            "Epoch 63/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1115.7535 - val_loss: 1118.6829\n",
            "Epoch 64/2000\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1113.5443 - val_loss: 1122.1714\n",
            "Epoch 65/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1118.1222 - val_loss: 1116.4889\n",
            "Epoch 66/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1116.1145 - val_loss: 1129.2139\n",
            "Epoch 67/2000\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1115.6774 - val_loss: 1137.1505\n",
            "Epoch 68/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1112.4279 - val_loss: 1128.2651\n",
            "Epoch 69/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1114.2368 - val_loss: 1122.2394\n",
            "Epoch 70/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1117.1906 - val_loss: 1127.7605\n",
            "Epoch 71/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1114.0839 - val_loss: 1132.8932\n",
            "Epoch 72/2000\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1114.6786 - val_loss: 1118.0178\n",
            "Epoch 73/2000\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1113.7385 - val_loss: 1133.0511\n",
            "Epoch 74/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1114.8985 - val_loss: 1124.4089\n",
            "Epoch 75/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1113.1068 - val_loss: 1129.1978\n",
            "Epoch 76/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1113.1825 - val_loss: 1132.4650\n",
            "Epoch 77/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1115.7784 - val_loss: 1129.6516\n",
            "Epoch 78/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1112.0437 - val_loss: 1129.5693\n",
            "Epoch 79/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1111.0741 - val_loss: 1122.5117\n",
            "Epoch 80/2000\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1114.2342 - val_loss: 1125.4788\n",
            "Epoch 81/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1112.6432 - val_loss: 1123.1898\n",
            "Epoch 82/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1113.6317 - val_loss: 1124.1655\n",
            "Epoch 83/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1113.2051 - val_loss: 1119.5562\n",
            "Epoch 84/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1113.2306 - val_loss: 1114.4143\n",
            "Epoch 85/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1114.5217 - val_loss: 1119.1006\n",
            "Epoch 86/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1112.6608 - val_loss: 1111.9712\n",
            "Epoch 87/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1112.5836 - val_loss: 1110.2119\n",
            "Epoch 88/2000\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1113.2182 - val_loss: 1108.8431\n",
            "Epoch 89/2000\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1111.2491 - val_loss: 1112.3424\n",
            "Epoch 90/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1112.5544 - val_loss: 1111.0209\n",
            "Epoch 91/2000\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1112.6375 - val_loss: 1116.0039\n",
            "Epoch 92/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1112.9295 - val_loss: 1113.5770\n",
            "Epoch 93/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1114.2286 - val_loss: 1111.7648\n",
            "Epoch 94/2000\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1111.4542 - val_loss: 1117.4806\n",
            "Epoch 95/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1111.6757 - val_loss: 1117.8784\n",
            "Epoch 96/2000\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1113.4356 - val_loss: 1112.5385\n",
            "Epoch 97/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1111.5516 - val_loss: 1122.2974\n",
            "Epoch 98/2000\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1112.0770 - val_loss: 1118.0924\n",
            "Epoch 99/2000\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1110.5823 - val_loss: 1118.2904\n",
            "Epoch 100/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1111.2882 - val_loss: 1121.3744\n",
            "Epoch 101/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1108.6586 - val_loss: 1121.1151\n",
            "Epoch 102/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1110.2171 - val_loss: 1115.3071\n",
            "Epoch 103/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1111.5561 - val_loss: 1116.5973\n",
            "Epoch 104/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1112.4797 - val_loss: 1119.1445\n",
            "Epoch 105/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1110.3154 - val_loss: 1119.7421\n",
            "Epoch 106/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1110.9598 - val_loss: 1111.4961\n",
            "Epoch 107/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1110.9163 - val_loss: 1111.9576\n",
            "Epoch 108/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1111.9739 - val_loss: 1110.8572\n",
            "Epoch 109/2000\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1109.9019 - val_loss: 1111.8306\n",
            "Epoch 110/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1110.9449 - val_loss: 1112.4065\n",
            "Epoch 111/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1110.2254 - val_loss: 1115.0006\n",
            "Epoch 112/2000\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1111.1004 - val_loss: 1115.5428\n",
            "Epoch 113/2000\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1108.9522 - val_loss: 1113.9381\n",
            "Epoch 114/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1112.4798 - val_loss: 1114.4541\n",
            "Epoch 115/2000\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1111.2285 - val_loss: 1113.9663\n",
            "Epoch 116/2000\n",
            "157/157 [==============================] - 0s 170us/step - loss: 1110.1942 - val_loss: 1115.4537\n",
            "Epoch 117/2000\n",
            "157/157 [==============================] - 0s 167us/step - loss: 1110.2183 - val_loss: 1118.6923\n",
            "Epoch 118/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1112.5299 - val_loss: 1107.3798\n",
            "Epoch 119/2000\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1110.2579 - val_loss: 1108.4247\n",
            "Epoch 120/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1109.0991 - val_loss: 1107.6113\n",
            "Epoch 121/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1111.0799 - val_loss: 1106.9983\n",
            "Epoch 122/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1111.1225 - val_loss: 1118.4907\n",
            "Epoch 123/2000\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1109.8495 - val_loss: 1114.8875\n",
            "Epoch 124/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1109.5948 - val_loss: 1111.8384\n",
            "Epoch 125/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1109.5660 - val_loss: 1110.2123\n",
            "Epoch 126/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1108.2481 - val_loss: 1112.3955\n",
            "Epoch 127/2000\n",
            "157/157 [==============================] - 0s 159us/step - loss: 1109.6707 - val_loss: 1113.3069\n",
            "Epoch 128/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1110.4890 - val_loss: 1109.8833\n",
            "Epoch 129/2000\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1109.2285 - val_loss: 1111.9550\n",
            "Epoch 130/2000\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1108.7027 - val_loss: 1110.5806\n",
            "Epoch 131/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1110.3145 - val_loss: 1108.8702\n",
            "Epoch 132/2000\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1112.7005 - val_loss: 1115.7393\n",
            "Epoch 133/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1110.7280 - val_loss: 1115.4493\n",
            "Epoch 134/2000\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1109.1039 - val_loss: 1110.2827\n",
            "Epoch 135/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1108.5034 - val_loss: 1115.2399\n",
            "Epoch 136/2000\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1111.1370 - val_loss: 1111.3611\n",
            "Epoch 137/2000\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1109.4586 - val_loss: 1112.5874\n",
            "Epoch 138/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1109.3259 - val_loss: 1111.4028\n",
            "Epoch 139/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1109.3392 - val_loss: 1117.2621\n",
            "Epoch 140/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1110.6509 - val_loss: 1115.8745\n",
            "Epoch 141/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.4435 - val_loss: 1113.9766\n",
            "Epoch 142/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1109.6466 - val_loss: 1111.0582\n",
            "Epoch 143/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1108.4048 - val_loss: 1117.0634\n",
            "Epoch 144/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1107.9163 - val_loss: 1110.2866\n",
            "Epoch 145/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1109.3578 - val_loss: 1116.7000\n",
            "Epoch 146/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1108.2778 - val_loss: 1114.6244\n",
            "Epoch 147/2000\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1108.1722 - val_loss: 1114.0356\n",
            "Epoch 148/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1109.5672 - val_loss: 1118.6138\n",
            "Epoch 149/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1107.4845 - val_loss: 1112.6289\n",
            "Epoch 150/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1109.3371 - val_loss: 1112.7966\n",
            "Epoch 151/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1108.4227 - val_loss: 1109.5563\n",
            "Epoch 152/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1108.2119 - val_loss: 1108.8544\n",
            "Epoch 153/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1109.2258 - val_loss: 1110.9836\n",
            "Epoch 154/2000\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1109.8730 - val_loss: 1109.3062\n",
            "Epoch 155/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1107.1263 - val_loss: 1114.5042\n",
            "Epoch 156/2000\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1107.9291 - val_loss: 1108.9935\n",
            "Epoch 157/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1108.7530 - val_loss: 1113.7600\n",
            "Epoch 158/2000\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1107.9757 - val_loss: 1109.2399\n",
            "Epoch 159/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1108.0584 - val_loss: 1103.4794\n",
            "Epoch 160/2000\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1107.5084 - val_loss: 1109.8735\n",
            "Epoch 161/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1108.3683 - val_loss: 1111.4154\n",
            "Epoch 162/2000\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1106.5654 - val_loss: 1106.1981\n",
            "Epoch 163/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1107.4880 - val_loss: 1111.2096\n",
            "Epoch 164/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1107.4993 - val_loss: 1109.9165\n",
            "Epoch 165/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1108.0182 - val_loss: 1113.4923\n",
            "Epoch 166/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1107.6713 - val_loss: 1108.3434\n",
            "Epoch 167/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1107.1069 - val_loss: 1105.9487\n",
            "Epoch 168/2000\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1108.1694 - val_loss: 1112.2292\n",
            "Epoch 169/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.2251 - val_loss: 1105.2861\n",
            "Epoch 170/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1106.8673 - val_loss: 1108.1862\n",
            "Epoch 171/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.0861 - val_loss: 1102.8600\n",
            "Epoch 172/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1108.2837 - val_loss: 1108.7905\n",
            "Epoch 173/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1108.0298 - val_loss: 1102.7078\n",
            "Epoch 174/2000\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1107.1181 - val_loss: 1108.3325\n",
            "Epoch 175/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1108.2805 - val_loss: 1108.9778\n",
            "Epoch 176/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1106.6987 - val_loss: 1109.3383\n",
            "Epoch 177/2000\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1106.7434 - val_loss: 1111.1523\n",
            "Epoch 178/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1108.4400 - val_loss: 1106.0649\n",
            "Epoch 179/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1108.0678 - val_loss: 1110.2551\n",
            "Epoch 180/2000\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1108.4982 - val_loss: 1108.3997\n",
            "Epoch 181/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1106.5597 - val_loss: 1106.7346\n",
            "Epoch 182/2000\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1106.5905 - val_loss: 1108.7847\n",
            "Epoch 183/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1107.4097 - val_loss: 1105.3674\n",
            "Epoch 184/2000\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1108.1092 - val_loss: 1107.3480\n",
            "Epoch 185/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1106.5330 - val_loss: 1115.8345\n",
            "Epoch 186/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1109.1838 - val_loss: 1107.6274\n",
            "Epoch 187/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1107.3438 - val_loss: 1107.4078\n",
            "Epoch 188/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1107.3225 - val_loss: 1105.4021\n",
            "Epoch 189/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1107.5581 - val_loss: 1110.1862\n",
            "Epoch 190/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1107.3107 - val_loss: 1105.8380\n",
            "Epoch 191/2000\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1108.6192 - val_loss: 1110.2748\n",
            "Epoch 192/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1108.0167 - val_loss: 1108.1091\n",
            "Epoch 193/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1108.0219 - val_loss: 1112.6322\n",
            "Epoch 194/2000\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1107.2059 - val_loss: 1114.0835\n",
            "Epoch 195/2000\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1105.4811 - val_loss: 1108.9116\n",
            "Epoch 196/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1107.1410 - val_loss: 1112.3718\n",
            "Epoch 197/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1104.5293 - val_loss: 1107.7657\n",
            "Epoch 198/2000\n",
            "157/157 [==============================] - 0s 174us/step - loss: 1106.1517 - val_loss: 1114.0016\n",
            "Epoch 199/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1107.3443 - val_loss: 1114.4116\n",
            "Epoch 200/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1107.0184 - val_loss: 1110.9575\n",
            "Epoch 201/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1104.9790 - val_loss: 1116.7327\n",
            "Epoch 202/2000\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1105.6001 - val_loss: 1108.4413\n",
            "Epoch 203/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1106.1617 - val_loss: 1106.6715\n",
            "Epoch 204/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1105.8924 - val_loss: 1110.0167\n",
            "Epoch 205/2000\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1105.7163 - val_loss: 1112.4343\n",
            "Epoch 206/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1106.3216 - val_loss: 1111.1187\n",
            "Epoch 207/2000\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1108.2433 - val_loss: 1107.1235\n",
            "Epoch 208/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1107.5738 - val_loss: 1108.4896\n",
            "Epoch 209/2000\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1105.1841 - val_loss: 1110.9020\n",
            "Epoch 210/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1105.6028 - val_loss: 1107.4838\n",
            "Epoch 211/2000\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1107.0025 - val_loss: 1111.0225\n",
            "Epoch 212/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1107.0650 - val_loss: 1108.6572\n",
            "Epoch 213/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1106.3972 - val_loss: 1111.8853\n",
            "Epoch 214/2000\n",
            "157/157 [==============================] - 0s 171us/step - loss: 1106.9788 - val_loss: 1108.9448\n",
            "Epoch 215/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1105.4668 - val_loss: 1108.2495\n",
            "Epoch 216/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1105.4779 - val_loss: 1109.6095\n",
            "Epoch 217/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1109.1089 - val_loss: 1108.9879\n",
            "Epoch 218/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1106.7646 - val_loss: 1110.0084\n",
            "Epoch 219/2000\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1105.1191 - val_loss: 1109.4073\n",
            "Epoch 220/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1106.3880 - val_loss: 1108.6406\n",
            "Epoch 221/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1106.6021 - val_loss: 1109.0551\n",
            "Epoch 222/2000\n",
            "157/157 [==============================] - 0s 174us/step - loss: 1105.6358 - val_loss: 1109.1318\n",
            "Epoch 223/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1106.3953 - val_loss: 1111.8380\n",
            "Epoch 224/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1105.0772 - val_loss: 1108.8623\n",
            "Epoch 225/2000\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1106.4044 - val_loss: 1107.3815\n",
            "Epoch 226/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1106.3670 - val_loss: 1106.5992\n",
            "Epoch 227/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1105.1825 - val_loss: 1108.4106\n",
            "Epoch 228/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1104.7238 - val_loss: 1107.6808\n",
            "Epoch 229/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1105.1675 - val_loss: 1112.6586\n",
            "Epoch 230/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1106.1478 - val_loss: 1109.8262\n",
            "Epoch 231/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1104.5643 - val_loss: 1107.9065\n",
            "Epoch 232/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1106.2467 - val_loss: 1108.5601\n",
            "Epoch 233/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1106.7962 - val_loss: 1108.8062\n",
            "Epoch 234/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1106.2402 - val_loss: 1108.8784\n",
            "Epoch 235/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.5214 - val_loss: 1106.4829\n",
            "Epoch 236/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1104.5652 - val_loss: 1106.1069\n",
            "Epoch 237/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1106.6261 - val_loss: 1105.7673\n",
            "Epoch 238/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1106.1833 - val_loss: 1106.3674\n",
            "Epoch 239/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1105.2402 - val_loss: 1106.8539\n",
            "Epoch 240/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1106.3442 - val_loss: 1109.4081\n",
            "Epoch 241/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1105.1996 - val_loss: 1104.5060\n",
            "Epoch 242/2000\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1105.9406 - val_loss: 1105.1766\n",
            "Epoch 243/2000\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1102.8331 - val_loss: 1106.6637\n",
            "Epoch 244/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1105.9888 - val_loss: 1107.6119\n",
            "Epoch 245/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1105.1443 - val_loss: 1108.6055\n",
            "Epoch 246/2000\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1105.2318 - val_loss: 1107.0223\n",
            "Epoch 247/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1105.6957 - val_loss: 1107.4771\n",
            "Epoch 248/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1108.9515 - val_loss: 1111.3113\n",
            "Epoch 249/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1105.9943 - val_loss: 1107.1620\n",
            "Epoch 250/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1105.4478 - val_loss: 1109.8389\n",
            "Epoch 251/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1106.1686 - val_loss: 1110.0583\n",
            "Epoch 252/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.6154 - val_loss: 1108.4448\n",
            "Epoch 253/2000\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1106.0567 - val_loss: 1108.3672\n",
            "Epoch 254/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1104.5173 - val_loss: 1109.6583\n",
            "Epoch 255/2000\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1105.4148 - val_loss: 1107.2091\n",
            "Epoch 256/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1106.7668 - val_loss: 1107.2913\n",
            "Epoch 257/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.9993 - val_loss: 1108.6973\n",
            "Epoch 258/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1106.5169 - val_loss: 1107.7620\n",
            "Epoch 259/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1105.8474 - val_loss: 1106.2047\n",
            "Epoch 260/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1107.0961 - val_loss: 1104.4048\n",
            "Epoch 261/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1104.4869 - val_loss: 1107.6022\n",
            "Epoch 262/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1105.2028 - val_loss: 1104.8134\n",
            "Epoch 263/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1105.5143 - val_loss: 1104.0873\n",
            "Epoch 264/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1104.8182 - val_loss: 1105.7992\n",
            "Epoch 265/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1104.4640 - val_loss: 1104.7118\n",
            "Epoch 266/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1105.1019 - val_loss: 1107.1338\n",
            "Epoch 267/2000\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1104.1422 - val_loss: 1105.7626\n",
            "Epoch 268/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1105.3002 - val_loss: 1109.0034\n",
            "Epoch 269/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1104.0155 - val_loss: 1108.1167\n",
            "Epoch 270/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1104.1634 - val_loss: 1107.1459\n",
            "Epoch 271/2000\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1105.0447 - val_loss: 1107.2573\n",
            "Epoch 272/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1103.9387 - val_loss: 1109.6217\n",
            "Epoch 273/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1105.0490 - val_loss: 1107.6663\n",
            "Epoch 274/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1104.5943 - val_loss: 1106.1382\n",
            "Epoch 275/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1106.5173 - val_loss: 1109.4073\n",
            "Epoch 276/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1104.1316 - val_loss: 1111.3901\n",
            "Epoch 277/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1104.3595 - val_loss: 1110.1229\n",
            "Epoch 278/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1104.1111 - val_loss: 1111.9874\n",
            "Epoch 279/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1106.1816 - val_loss: 1114.8728\n",
            "Epoch 280/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1105.3971 - val_loss: 1105.6816\n",
            "Epoch 281/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.8630 - val_loss: 1108.7001\n",
            "Epoch 282/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1104.2024 - val_loss: 1109.3882\n",
            "Epoch 283/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1104.4292 - val_loss: 1107.0245\n",
            "Epoch 284/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1105.0893 - val_loss: 1106.4810\n",
            "Epoch 285/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1104.1799 - val_loss: 1106.0465\n",
            "Epoch 286/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1104.7073 - val_loss: 1107.5820\n",
            "Epoch 287/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.7179 - val_loss: 1107.0601\n",
            "Epoch 288/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1103.9693 - val_loss: 1107.3557\n",
            "Epoch 289/2000\n",
            "157/157 [==============================] - 0s 172us/step - loss: 1103.7384 - val_loss: 1108.9449\n",
            "Epoch 290/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1105.2712 - val_loss: 1111.5148\n",
            "Epoch 291/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1105.0632 - val_loss: 1112.7629\n",
            "Epoch 292/2000\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1105.5523 - val_loss: 1108.9888\n",
            "Epoch 293/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1109.1065 - val_loss: 1111.7297\n",
            "Epoch 294/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1106.4316 - val_loss: 1110.4705\n",
            "Epoch 295/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1103.7366 - val_loss: 1112.2573\n",
            "Epoch 296/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1105.4908 - val_loss: 1105.4530\n",
            "Epoch 297/2000\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1103.8147 - val_loss: 1106.6366\n",
            "Epoch 298/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1104.1016 - val_loss: 1107.6332\n",
            "Epoch 299/2000\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1104.1390 - val_loss: 1107.7393\n",
            "Epoch 300/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1104.1162 - val_loss: 1107.8755\n",
            "Epoch 301/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1105.3304 - val_loss: 1109.6035\n",
            "Epoch 302/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.6316 - val_loss: 1107.3425\n",
            "Epoch 303/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1104.7260 - val_loss: 1104.0083\n",
            "Epoch 304/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1104.1781 - val_loss: 1105.6040\n",
            "Epoch 305/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1104.6371 - val_loss: 1107.3229\n",
            "Epoch 306/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1103.2167 - val_loss: 1104.8728\n",
            "Epoch 307/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1104.1985 - val_loss: 1107.6385\n",
            "Epoch 308/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1103.4560 - val_loss: 1105.3049\n",
            "Epoch 309/2000\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1104.1816 - val_loss: 1107.6693\n",
            "Epoch 310/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1104.8585 - val_loss: 1104.7584\n",
            "Epoch 311/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1105.1207 - val_loss: 1105.2026\n",
            "Epoch 312/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1105.3102 - val_loss: 1105.7180\n",
            "Epoch 313/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1103.4026 - val_loss: 1105.8630\n",
            "Epoch 314/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.3131 - val_loss: 1107.0115\n",
            "Epoch 315/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1103.8317 - val_loss: 1109.1919\n",
            "Epoch 316/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1104.2199 - val_loss: 1107.2479\n",
            "Epoch 317/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1104.4346 - val_loss: 1104.3937\n",
            "Epoch 318/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1105.1509 - val_loss: 1106.3309\n",
            "Epoch 319/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1104.0225 - val_loss: 1105.8102\n",
            "Epoch 320/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1103.6023 - val_loss: 1106.1388\n",
            "Epoch 321/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1104.1012 - val_loss: 1107.1893\n",
            "Epoch 322/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1103.5740 - val_loss: 1107.1167\n",
            "Epoch 323/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1104.8232 - val_loss: 1104.4219\n",
            "Epoch 324/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1102.7842 - val_loss: 1106.5892\n",
            "Epoch 325/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1104.1279 - val_loss: 1105.3258\n",
            "Epoch 326/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1103.1856 - val_loss: 1107.8220\n",
            "Epoch 327/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1105.3229 - val_loss: 1106.9502\n",
            "Epoch 328/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1104.3634 - val_loss: 1107.9338\n",
            "Epoch 329/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1103.9451 - val_loss: 1106.2413\n",
            "Epoch 330/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.1947 - val_loss: 1108.3070\n",
            "Epoch 331/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1102.5192 - val_loss: 1108.5760\n",
            "Epoch 332/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1104.6823 - val_loss: 1105.0988\n",
            "Epoch 333/2000\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1102.8761 - val_loss: 1107.8402\n",
            "Epoch 334/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1103.1134 - val_loss: 1104.4008\n",
            "Epoch 335/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1103.1824 - val_loss: 1107.5027\n",
            "Epoch 336/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1103.7864 - val_loss: 1105.4669\n",
            "Epoch 337/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1102.9148 - val_loss: 1104.8403\n",
            "Epoch 338/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1103.2320 - val_loss: 1105.9507\n",
            "Epoch 339/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1106.3523 - val_loss: 1107.2523\n",
            "Epoch 340/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1103.6446 - val_loss: 1107.5092\n",
            "Epoch 341/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1103.8176 - val_loss: 1106.0370\n",
            "Epoch 342/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1104.5325 - val_loss: 1107.1975\n",
            "Epoch 343/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1103.9958 - val_loss: 1105.9182\n",
            "Epoch 344/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1103.3372 - val_loss: 1105.1375\n",
            "Epoch 345/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.9803 - val_loss: 1105.9896\n",
            "Epoch 346/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1104.4786 - val_loss: 1105.6599\n",
            "Epoch 347/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1104.1293 - val_loss: 1105.3199\n",
            "Epoch 348/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1102.9899 - val_loss: 1105.1681\n",
            "Epoch 349/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1103.6865 - val_loss: 1105.7089\n",
            "Epoch 350/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1102.6010 - val_loss: 1104.2413\n",
            "Epoch 351/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1105.1053 - val_loss: 1109.2747\n",
            "Epoch 352/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1103.3690 - val_loss: 1105.2622\n",
            "Epoch 353/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1103.1953 - val_loss: 1103.1849\n",
            "Epoch 354/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.5796 - val_loss: 1108.1693\n",
            "Epoch 355/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1103.1689 - val_loss: 1107.6343\n",
            "Epoch 356/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.5970 - val_loss: 1103.1992\n",
            "Epoch 357/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1103.4988 - val_loss: 1105.0469\n",
            "Epoch 358/2000\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1102.7986 - val_loss: 1105.1884\n",
            "Epoch 359/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1103.4685 - val_loss: 1105.0322\n",
            "Epoch 360/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1103.6252 - val_loss: 1103.3735\n",
            "Epoch 361/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1103.9734 - val_loss: 1103.9928\n",
            "Epoch 362/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1104.2147 - val_loss: 1103.0841\n",
            "Epoch 363/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1103.3516 - val_loss: 1106.0029\n",
            "Epoch 364/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.3389 - val_loss: 1105.9327\n",
            "Epoch 365/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1103.0779 - val_loss: 1108.4371\n",
            "Epoch 366/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1104.2416 - val_loss: 1109.5022\n",
            "Epoch 367/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1104.4329 - val_loss: 1107.1676\n",
            "Epoch 368/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.1479 - val_loss: 1109.9026\n",
            "Epoch 369/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1105.4111 - val_loss: 1108.1364\n",
            "Epoch 370/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1104.4637 - val_loss: 1108.7380\n",
            "Epoch 371/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.0772 - val_loss: 1108.9135\n",
            "Epoch 372/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1104.2319 - val_loss: 1104.8354\n",
            "Epoch 373/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1103.0607 - val_loss: 1103.9768\n",
            "Epoch 374/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1103.0884 - val_loss: 1103.9308\n",
            "Epoch 375/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1103.6546 - val_loss: 1102.5459\n",
            "Epoch 376/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1103.5673 - val_loss: 1102.9657\n",
            "Epoch 377/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1103.7135 - val_loss: 1104.3258\n",
            "Epoch 378/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1102.6832 - val_loss: 1102.4829\n",
            "Epoch 379/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1102.2066 - val_loss: 1105.0394\n",
            "Epoch 380/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1102.3375 - val_loss: 1105.1680\n",
            "Epoch 381/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1103.6307 - val_loss: 1108.3259\n",
            "Epoch 382/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1103.1467 - val_loss: 1106.0435\n",
            "Epoch 383/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1102.9426 - val_loss: 1107.2417\n",
            "Epoch 384/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1102.2196 - val_loss: 1102.6508\n",
            "Epoch 385/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.3048 - val_loss: 1108.4323\n",
            "Epoch 386/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.4275 - val_loss: 1104.7651\n",
            "Epoch 387/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1103.4379 - val_loss: 1104.3912\n",
            "Epoch 388/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.9568 - val_loss: 1104.5441\n",
            "Epoch 389/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1103.3449 - val_loss: 1103.3676\n",
            "Epoch 390/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1102.3641 - val_loss: 1106.2778\n",
            "Epoch 391/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1103.4887 - val_loss: 1105.7856\n",
            "Epoch 392/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1102.6341 - val_loss: 1108.9585\n",
            "Epoch 393/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1103.4724 - val_loss: 1105.8829\n",
            "Epoch 394/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.9943 - val_loss: 1106.6595\n",
            "Epoch 395/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1102.6619 - val_loss: 1104.2596\n",
            "Epoch 396/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1103.0562 - val_loss: 1103.6581\n",
            "Epoch 397/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1104.3265 - val_loss: 1105.3363\n",
            "Epoch 398/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1103.9983 - val_loss: 1106.7612\n",
            "Epoch 399/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.1929 - val_loss: 1106.4845\n",
            "Epoch 400/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1103.5495 - val_loss: 1106.3284\n",
            "Epoch 401/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1104.4791 - val_loss: 1106.3208\n",
            "Epoch 402/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1104.0915 - val_loss: 1107.1635\n",
            "Epoch 403/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1105.1079 - val_loss: 1108.3394\n",
            "Epoch 404/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1104.0016 - val_loss: 1109.4811\n",
            "Epoch 405/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1103.1832 - val_loss: 1108.4276\n",
            "Epoch 406/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1102.0500 - val_loss: 1108.0909\n",
            "Epoch 407/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1103.6916 - val_loss: 1105.9216\n",
            "Epoch 408/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.5890 - val_loss: 1105.1733\n",
            "Epoch 409/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1103.8378 - val_loss: 1105.9443\n",
            "Epoch 410/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1103.0380 - val_loss: 1107.0682\n",
            "Epoch 411/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.7430 - val_loss: 1106.0144\n",
            "Epoch 412/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1103.9226 - val_loss: 1104.6368\n",
            "Epoch 413/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1103.0109 - val_loss: 1105.1908\n",
            "Epoch 414/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1104.5838 - val_loss: 1104.1407\n",
            "Epoch 415/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1102.2002 - val_loss: 1101.2822\n",
            "Epoch 416/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1102.5637 - val_loss: 1104.2104\n",
            "Epoch 417/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1102.8075 - val_loss: 1103.9150\n",
            "Epoch 418/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.3375 - val_loss: 1104.8085\n",
            "Epoch 419/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1103.2417 - val_loss: 1105.6202\n",
            "Epoch 420/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.8783 - val_loss: 1104.1211\n",
            "Epoch 421/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1103.9322 - val_loss: 1106.0612\n",
            "Epoch 422/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1103.2855 - val_loss: 1104.5803\n",
            "Epoch 423/2000\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1102.6044 - val_loss: 1103.0217\n",
            "Epoch 424/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1102.8739 - val_loss: 1103.3882\n",
            "Epoch 425/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1102.6054 - val_loss: 1104.3062\n",
            "Epoch 426/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1102.3650 - val_loss: 1103.3662\n",
            "Epoch 427/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1102.5513 - val_loss: 1104.3961\n",
            "Epoch 428/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1103.1066 - val_loss: 1105.6724\n",
            "Epoch 429/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1102.7843 - val_loss: 1103.5446\n",
            "Epoch 430/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.1225 - val_loss: 1104.8268\n",
            "Epoch 431/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1103.2510 - val_loss: 1104.1636\n",
            "Epoch 432/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1102.9974 - val_loss: 1106.0120\n",
            "Epoch 433/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.2815 - val_loss: 1104.5753\n",
            "Epoch 434/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1103.2149 - val_loss: 1103.5405\n",
            "Epoch 435/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.9867 - val_loss: 1104.1017\n",
            "Epoch 436/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.0394 - val_loss: 1107.4557\n",
            "Epoch 437/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.4352 - val_loss: 1103.0095\n",
            "Epoch 438/2000\n",
            "157/157 [==============================] - 0s 349us/step - loss: 1103.6322 - val_loss: 1106.3293\n",
            "Epoch 439/2000\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1102.9593 - val_loss: 1105.7374\n",
            "Epoch 440/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1103.5206 - val_loss: 1105.1257\n",
            "Epoch 441/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1102.6080 - val_loss: 1104.8516\n",
            "Epoch 442/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1102.8116 - val_loss: 1105.4354\n",
            "Epoch 443/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1102.1264 - val_loss: 1102.8322\n",
            "Epoch 444/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1103.4521 - val_loss: 1102.9949\n",
            "Epoch 445/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.6620 - val_loss: 1102.7971\n",
            "Epoch 446/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1103.5039 - val_loss: 1101.7305\n",
            "Epoch 447/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1100.6732 - val_loss: 1103.5391\n",
            "Epoch 448/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1102.6200 - val_loss: 1101.4039\n",
            "Epoch 449/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1102.5857 - val_loss: 1103.9203\n",
            "Epoch 450/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.8773 - val_loss: 1103.4839\n",
            "Epoch 451/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.6484 - val_loss: 1103.7031\n",
            "Epoch 452/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1101.7214 - val_loss: 1105.8423\n",
            "Epoch 453/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1101.3883 - val_loss: 1104.7947\n",
            "Epoch 454/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1102.5149 - val_loss: 1103.9087\n",
            "Epoch 455/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1102.2133 - val_loss: 1104.5715\n",
            "Epoch 456/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1103.6891 - val_loss: 1107.1281\n",
            "Epoch 457/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1102.8467 - val_loss: 1107.2804\n",
            "Epoch 458/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.3807 - val_loss: 1104.8075\n",
            "Epoch 459/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.3404 - val_loss: 1103.1938\n",
            "Epoch 460/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.9569 - val_loss: 1102.6262\n",
            "Epoch 461/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1102.6633 - val_loss: 1100.5371\n",
            "Epoch 462/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1102.5921 - val_loss: 1106.8434\n",
            "Epoch 463/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1102.2359 - val_loss: 1106.9960\n",
            "Epoch 464/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.9268 - val_loss: 1105.0573\n",
            "Epoch 465/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1102.0041 - val_loss: 1107.6456\n",
            "Epoch 466/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1103.5468 - val_loss: 1106.9982\n",
            "Epoch 467/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1103.8639 - val_loss: 1107.1138\n",
            "Epoch 468/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1103.7803 - val_loss: 1106.1694\n",
            "Epoch 469/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1104.1042 - val_loss: 1106.5747\n",
            "Epoch 470/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.5878 - val_loss: 1103.5272\n",
            "Epoch 471/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.4345 - val_loss: 1106.3944\n",
            "Epoch 472/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1102.4570 - val_loss: 1105.0074\n",
            "Epoch 473/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1101.5470 - val_loss: 1105.4686\n",
            "Epoch 474/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1102.4315 - val_loss: 1105.8293\n",
            "Epoch 475/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1102.4242 - val_loss: 1103.1034\n",
            "Epoch 476/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1102.2797 - val_loss: 1103.1398\n",
            "Epoch 477/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1102.4807 - val_loss: 1104.0813\n",
            "Epoch 478/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1101.7555 - val_loss: 1104.4320\n",
            "Epoch 479/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1101.7147 - val_loss: 1104.1694\n",
            "Epoch 480/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1102.5903 - val_loss: 1106.7628\n",
            "Epoch 481/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1103.0654 - val_loss: 1103.3195\n",
            "Epoch 482/2000\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1101.8381 - val_loss: 1103.3448\n",
            "Epoch 483/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1104.4172 - val_loss: 1104.4735\n",
            "Epoch 484/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1102.2209 - val_loss: 1104.4215\n",
            "Epoch 485/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1102.7368 - val_loss: 1104.5598\n",
            "Epoch 486/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1101.6800 - val_loss: 1106.0923\n",
            "Epoch 487/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1103.4947 - val_loss: 1106.1115\n",
            "Epoch 488/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1101.5771 - val_loss: 1104.3179\n",
            "Epoch 489/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1102.1397 - val_loss: 1105.6722\n",
            "Epoch 490/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1101.1568 - val_loss: 1105.1239\n",
            "Epoch 491/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1103.5294 - val_loss: 1104.4565\n",
            "Epoch 492/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.0960 - val_loss: 1106.2871\n",
            "Epoch 493/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1101.4310 - val_loss: 1107.0481\n",
            "Epoch 494/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1101.7550 - val_loss: 1108.7446\n",
            "Epoch 495/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.8486 - val_loss: 1105.8273\n",
            "Epoch 496/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1104.4090 - val_loss: 1103.4729\n",
            "Epoch 497/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.2486 - val_loss: 1103.4735\n",
            "Epoch 498/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1102.4400 - val_loss: 1102.8961\n",
            "Epoch 499/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1102.1997 - val_loss: 1105.1260\n",
            "Epoch 500/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.7911 - val_loss: 1102.5118\n",
            "Epoch 501/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1104.1184 - val_loss: 1101.3795\n",
            "Epoch 502/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1101.6472 - val_loss: 1103.8567\n",
            "Epoch 503/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1101.3452 - val_loss: 1101.7802\n",
            "Epoch 504/2000\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1102.7504 - val_loss: 1101.6049\n",
            "Epoch 505/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.9321 - val_loss: 1101.9213\n",
            "Epoch 506/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.8602 - val_loss: 1105.6063\n",
            "Epoch 507/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1101.4541 - val_loss: 1102.2764\n",
            "Epoch 508/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1102.2320 - val_loss: 1105.7076\n",
            "Epoch 509/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1102.8381 - val_loss: 1106.4249\n",
            "Epoch 510/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1103.1015 - val_loss: 1105.5168\n",
            "Epoch 511/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1101.6387 - val_loss: 1105.3251\n",
            "Epoch 512/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1101.9917 - val_loss: 1105.6041\n",
            "Epoch 513/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1102.8151 - val_loss: 1102.5521\n",
            "Epoch 514/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.1731 - val_loss: 1103.7853\n",
            "Epoch 515/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1102.6948 - val_loss: 1103.7722\n",
            "Epoch 516/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.3615 - val_loss: 1103.1190\n",
            "Epoch 517/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1101.8197 - val_loss: 1104.5640\n",
            "Epoch 518/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1103.1326 - val_loss: 1102.2656\n",
            "Epoch 519/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.9837 - val_loss: 1101.6306\n",
            "Epoch 520/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1101.9677 - val_loss: 1101.3737\n",
            "Epoch 521/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.4255 - val_loss: 1101.7026\n",
            "Epoch 522/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1101.4355 - val_loss: 1101.7506\n",
            "Epoch 523/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1102.0588 - val_loss: 1103.0907\n",
            "Epoch 524/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1101.8773 - val_loss: 1102.5830\n",
            "Epoch 525/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1102.7097 - val_loss: 1103.3307\n",
            "Epoch 526/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1101.5372 - val_loss: 1103.9735\n",
            "Epoch 527/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.1555 - val_loss: 1103.4156\n",
            "Epoch 528/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.8467 - val_loss: 1103.5112\n",
            "Epoch 529/2000\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1101.3480 - val_loss: 1104.0372\n",
            "Epoch 530/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1102.3371 - val_loss: 1104.3046\n",
            "Epoch 531/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1103.2056 - val_loss: 1103.8611\n",
            "Epoch 532/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1101.5534 - val_loss: 1102.9827\n",
            "Epoch 533/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1101.8689 - val_loss: 1103.1732\n",
            "Epoch 534/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1102.6111 - val_loss: 1100.8826\n",
            "Epoch 535/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1102.3357 - val_loss: 1102.3115\n",
            "Epoch 536/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1102.1083 - val_loss: 1101.6674\n",
            "Epoch 537/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1101.7081 - val_loss: 1101.0549\n",
            "Epoch 538/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1100.4909 - val_loss: 1103.1260\n",
            "Epoch 539/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1101.6172 - val_loss: 1102.5430\n",
            "Epoch 540/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.8085 - val_loss: 1102.1586\n",
            "Epoch 541/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1102.0113 - val_loss: 1101.3307\n",
            "Epoch 542/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.4175 - val_loss: 1102.5726\n",
            "Epoch 543/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1100.7529 - val_loss: 1102.9056\n",
            "Epoch 544/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1103.1624 - val_loss: 1103.1060\n",
            "Epoch 545/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1101.0265 - val_loss: 1102.0483\n",
            "Epoch 546/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.7059 - val_loss: 1102.2537\n",
            "Epoch 547/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1102.7785 - val_loss: 1102.9717\n",
            "Epoch 548/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1100.7536 - val_loss: 1101.7959\n",
            "Epoch 549/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1101.8935 - val_loss: 1102.5750\n",
            "Epoch 550/2000\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1101.6340 - val_loss: 1102.4478\n",
            "Epoch 551/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1101.1790 - val_loss: 1105.6416\n",
            "Epoch 552/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1103.3802 - val_loss: 1104.9822\n",
            "Epoch 553/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1102.1960 - val_loss: 1101.8594\n",
            "Epoch 554/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1101.2186 - val_loss: 1102.5288\n",
            "Epoch 555/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1102.5302 - val_loss: 1103.0750\n",
            "Epoch 556/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.9973 - val_loss: 1103.2710\n",
            "Epoch 557/2000\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1103.1479 - val_loss: 1102.3586\n",
            "Epoch 558/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1103.7511 - val_loss: 1102.8059\n",
            "Epoch 559/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1100.6091 - val_loss: 1101.3892\n",
            "Epoch 560/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.5570 - val_loss: 1102.8494\n",
            "Epoch 561/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1101.2273 - val_loss: 1103.1732\n",
            "Epoch 562/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.0386 - val_loss: 1102.5416\n",
            "Epoch 563/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1102.2887 - val_loss: 1102.4104\n",
            "Epoch 564/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1101.2412 - val_loss: 1105.8815\n",
            "Epoch 565/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1101.5317 - val_loss: 1106.1174\n",
            "Epoch 566/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1104.6329 - val_loss: 1104.5007\n",
            "Epoch 567/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1102.4484 - val_loss: 1104.6968\n",
            "Epoch 568/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.6805 - val_loss: 1105.4381\n",
            "Epoch 569/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.8397 - val_loss: 1108.1710\n",
            "Epoch 570/2000\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1100.7130 - val_loss: 1105.9047\n",
            "Epoch 571/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1101.8237 - val_loss: 1101.8925\n",
            "Epoch 572/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.5518 - val_loss: 1104.6528\n",
            "Epoch 573/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1101.1581 - val_loss: 1105.3140\n",
            "Epoch 574/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1100.9412 - val_loss: 1106.7329\n",
            "Epoch 575/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1102.0418 - val_loss: 1105.6356\n",
            "Epoch 576/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.9028 - val_loss: 1104.7228\n",
            "Epoch 577/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1102.2655 - val_loss: 1105.5973\n",
            "Epoch 578/2000\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1101.5255 - val_loss: 1106.6357\n",
            "Epoch 579/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1101.3908 - val_loss: 1104.0642\n",
            "Epoch 580/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.9218 - val_loss: 1104.3542\n",
            "Epoch 581/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1101.3650 - val_loss: 1102.9974\n",
            "Epoch 582/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1101.8635 - val_loss: 1103.6395\n",
            "Epoch 583/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1101.7770 - val_loss: 1104.6888\n",
            "Epoch 584/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1101.2336 - val_loss: 1106.6620\n",
            "Epoch 585/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1100.3832 - val_loss: 1106.1401\n",
            "Epoch 586/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1103.5647 - val_loss: 1106.6936\n",
            "Epoch 587/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.0406 - val_loss: 1104.6967\n",
            "Epoch 588/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1102.2411 - val_loss: 1104.0215\n",
            "Epoch 589/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1101.7157 - val_loss: 1104.5033\n",
            "Epoch 590/2000\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1101.6641 - val_loss: 1104.5571\n",
            "Epoch 591/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.8437 - val_loss: 1103.6307\n",
            "Epoch 592/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1101.7457 - val_loss: 1104.6423\n",
            "Epoch 593/2000\n",
            "157/157 [==============================] - 0s 342us/step - loss: 1101.2841 - val_loss: 1105.4487\n",
            "Epoch 594/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.8833 - val_loss: 1105.1814\n",
            "Epoch 595/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1101.0253 - val_loss: 1103.0034\n",
            "Epoch 596/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1101.9976 - val_loss: 1102.3971\n",
            "Epoch 597/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1102.0870 - val_loss: 1104.3126\n",
            "Epoch 598/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.4293 - val_loss: 1102.6763\n",
            "Epoch 599/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.8399 - val_loss: 1104.2955\n",
            "Epoch 600/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.2799 - val_loss: 1105.8799\n",
            "Epoch 601/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.4005 - val_loss: 1104.1429\n",
            "Epoch 602/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1100.7843 - val_loss: 1103.6003\n",
            "Epoch 603/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.8151 - val_loss: 1105.0020\n",
            "Epoch 604/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1102.0841 - val_loss: 1103.6698\n",
            "Epoch 605/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1100.5158 - val_loss: 1105.1714\n",
            "Epoch 606/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1101.9433 - val_loss: 1105.2914\n",
            "Epoch 607/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.5659 - val_loss: 1102.8073\n",
            "Epoch 608/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1101.2362 - val_loss: 1106.0229\n",
            "Epoch 609/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.7182 - val_loss: 1105.7596\n",
            "Epoch 610/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1102.1983 - val_loss: 1104.2109\n",
            "Epoch 611/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.3492 - val_loss: 1104.0193\n",
            "Epoch 612/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1101.4589 - val_loss: 1103.1326\n",
            "Epoch 613/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.1877 - val_loss: 1103.6869\n",
            "Epoch 614/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1101.4837 - val_loss: 1103.0522\n",
            "Epoch 615/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1100.4330 - val_loss: 1100.8872\n",
            "Epoch 616/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1100.3163 - val_loss: 1102.8846\n",
            "Epoch 617/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1101.4102 - val_loss: 1103.3408\n",
            "Epoch 618/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.0838 - val_loss: 1103.8179\n",
            "Epoch 619/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.7449 - val_loss: 1104.1323\n",
            "Epoch 620/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1101.5854 - val_loss: 1103.2710\n",
            "Epoch 621/2000\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1102.2255 - val_loss: 1105.1412\n",
            "Epoch 622/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.1634 - val_loss: 1102.9397\n",
            "Epoch 623/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1101.2380 - val_loss: 1105.2599\n",
            "Epoch 624/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1101.6310 - val_loss: 1103.2894\n",
            "Epoch 625/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.7298 - val_loss: 1103.3240\n",
            "Epoch 626/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1101.0089 - val_loss: 1108.9818\n",
            "Epoch 627/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1100.8448 - val_loss: 1105.4435\n",
            "Epoch 628/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1101.0761 - val_loss: 1102.5056\n",
            "Epoch 629/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1101.5467 - val_loss: 1103.0922\n",
            "Epoch 630/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.6618 - val_loss: 1103.2052\n",
            "Epoch 631/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.4164 - val_loss: 1103.7118\n",
            "Epoch 632/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1101.7139 - val_loss: 1104.4830\n",
            "Epoch 633/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.6391 - val_loss: 1102.8588\n",
            "Epoch 634/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.4350 - val_loss: 1102.1735\n",
            "Epoch 635/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1102.1072 - val_loss: 1102.9902\n",
            "Epoch 636/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1102.9536 - val_loss: 1102.3596\n",
            "Epoch 637/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.6997 - val_loss: 1101.8574\n",
            "Epoch 638/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.5713 - val_loss: 1103.4113\n",
            "Epoch 639/2000\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.0079 - val_loss: 1102.3677\n",
            "Epoch 640/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1101.7601 - val_loss: 1103.8635\n",
            "Epoch 641/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1102.7097 - val_loss: 1103.7729\n",
            "Epoch 642/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1100.4005 - val_loss: 1102.5115\n",
            "Epoch 643/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.1676 - val_loss: 1102.3014\n",
            "Epoch 644/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1101.7989 - val_loss: 1103.4373\n",
            "Epoch 645/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1101.4013 - val_loss: 1101.0675\n",
            "Epoch 646/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1100.6344 - val_loss: 1102.9048\n",
            "Epoch 647/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.2241 - val_loss: 1102.1554\n",
            "Epoch 648/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1101.9584 - val_loss: 1104.0103\n",
            "Epoch 649/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1100.6292 - val_loss: 1103.8198\n",
            "Epoch 650/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1100.7284 - val_loss: 1103.5577\n",
            "Epoch 651/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1101.1291 - val_loss: 1103.7161\n",
            "Epoch 652/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1100.0549 - val_loss: 1105.5251\n",
            "Epoch 653/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1101.1177 - val_loss: 1104.3385\n",
            "Epoch 654/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1104.5981 - val_loss: 1102.9073\n",
            "Epoch 655/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1101.9601 - val_loss: 1103.5540\n",
            "Epoch 656/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.8343 - val_loss: 1102.3198\n",
            "Epoch 657/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.7524 - val_loss: 1103.2872\n",
            "Epoch 658/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1101.1589 - val_loss: 1103.1647\n",
            "Epoch 659/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1101.3169 - val_loss: 1101.2340\n",
            "Epoch 660/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1101.7641 - val_loss: 1101.7897\n",
            "Epoch 661/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1102.0622 - val_loss: 1101.9644\n",
            "Epoch 662/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1100.3505 - val_loss: 1102.7257\n",
            "Epoch 663/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1101.6484 - val_loss: 1102.8149\n",
            "Epoch 664/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1101.1337 - val_loss: 1102.2853\n",
            "Epoch 665/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.6097 - val_loss: 1104.1564\n",
            "Epoch 666/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.3086 - val_loss: 1105.1019\n",
            "Epoch 667/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1103.0164 - val_loss: 1102.8290\n",
            "Epoch 668/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1101.1901 - val_loss: 1104.2781\n",
            "Epoch 669/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1101.1700 - val_loss: 1102.9659\n",
            "Epoch 670/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.8322 - val_loss: 1103.7576\n",
            "Epoch 671/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1101.7561 - val_loss: 1103.7612\n",
            "Epoch 672/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1102.3456 - val_loss: 1106.8055\n",
            "Epoch 673/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.5548 - val_loss: 1104.3684\n",
            "Epoch 674/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.3820 - val_loss: 1103.4670\n",
            "Epoch 675/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1101.4252 - val_loss: 1103.0298\n",
            "Epoch 676/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1100.2132 - val_loss: 1103.5970\n",
            "Epoch 677/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1103.8175 - val_loss: 1101.5894\n",
            "Epoch 678/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.2135 - val_loss: 1103.3030\n",
            "Epoch 679/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1101.1718 - val_loss: 1104.7917\n",
            "Epoch 680/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1100.7217 - val_loss: 1102.9437\n",
            "Epoch 681/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1101.4290 - val_loss: 1105.4744\n",
            "Epoch 682/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1101.2753 - val_loss: 1102.7877\n",
            "Epoch 683/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1102.5327 - val_loss: 1103.6079\n",
            "Epoch 684/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1102.2744 - val_loss: 1105.4666\n",
            "Epoch 685/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1100.7294 - val_loss: 1103.8367\n",
            "Epoch 686/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1102.7065 - val_loss: 1101.2528\n",
            "Epoch 687/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1100.5295 - val_loss: 1103.1428\n",
            "Epoch 688/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1101.7890 - val_loss: 1100.9590\n",
            "Epoch 689/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.3305 - val_loss: 1102.2415\n",
            "Epoch 690/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.0762 - val_loss: 1102.5488\n",
            "Epoch 691/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1102.6187 - val_loss: 1102.4844\n",
            "Epoch 692/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1101.5948 - val_loss: 1101.3715\n",
            "Epoch 693/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.1250 - val_loss: 1102.0575\n",
            "Epoch 694/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.2636 - val_loss: 1101.0349\n",
            "Epoch 695/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1100.8923 - val_loss: 1102.7651\n",
            "Epoch 696/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1101.2392 - val_loss: 1100.8846\n",
            "Epoch 697/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.6362 - val_loss: 1103.4600\n",
            "Epoch 698/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1100.7111 - val_loss: 1104.3640\n",
            "Epoch 699/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.4770 - val_loss: 1103.0438\n",
            "Epoch 700/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1100.4656 - val_loss: 1103.8937\n",
            "Epoch 701/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1101.5229 - val_loss: 1105.3159\n",
            "Epoch 702/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1101.8606 - val_loss: 1104.7466\n",
            "Epoch 703/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1099.8853 - val_loss: 1104.1638\n",
            "Epoch 704/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1102.8361 - val_loss: 1102.5868\n",
            "Epoch 705/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1100.5452 - val_loss: 1102.8223\n",
            "Epoch 706/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.1002 - val_loss: 1103.4731\n",
            "Epoch 707/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1100.6863 - val_loss: 1105.0209\n",
            "Epoch 708/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.2476 - val_loss: 1104.6267\n",
            "Epoch 709/2000\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1099.8320 - val_loss: 1106.1805\n",
            "Epoch 710/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.5872 - val_loss: 1103.1506\n",
            "Epoch 711/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1101.2113 - val_loss: 1103.4504\n",
            "Epoch 712/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1101.3356 - val_loss: 1103.7465\n",
            "Epoch 713/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1101.2231 - val_loss: 1104.4087\n",
            "Epoch 714/2000\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1100.0816 - val_loss: 1104.1284\n",
            "Epoch 715/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1102.0713 - val_loss: 1103.7758\n",
            "Epoch 716/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.2761 - val_loss: 1103.7688\n",
            "Epoch 717/2000\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1100.3753 - val_loss: 1103.7921\n",
            "Epoch 718/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.8580 - val_loss: 1102.7134\n",
            "Epoch 719/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.8990 - val_loss: 1102.2280\n",
            "Epoch 720/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1099.8744 - val_loss: 1105.9023\n",
            "Epoch 721/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1101.1337 - val_loss: 1106.0886\n",
            "Epoch 722/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1101.9149 - val_loss: 1105.9292\n",
            "Epoch 723/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1101.0688 - val_loss: 1105.8663\n",
            "Epoch 724/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1102.0257 - val_loss: 1104.1938\n",
            "Epoch 725/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1102.0471 - val_loss: 1105.6613\n",
            "Epoch 726/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1100.8738 - val_loss: 1104.6727\n",
            "Epoch 727/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.3135 - val_loss: 1104.8501\n",
            "Epoch 728/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.8856 - val_loss: 1102.6942\n",
            "Epoch 729/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1100.8904 - val_loss: 1102.1273\n",
            "Epoch 730/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1100.1101 - val_loss: 1102.6384\n",
            "Epoch 731/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.5016 - val_loss: 1104.3152\n",
            "Epoch 732/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.8103 - val_loss: 1105.0566\n",
            "Epoch 733/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1101.5434 - val_loss: 1103.6368\n",
            "Epoch 734/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1101.4002 - val_loss: 1105.7219\n",
            "Epoch 735/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1100.9042 - val_loss: 1103.2145\n",
            "Epoch 736/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1102.0960 - val_loss: 1104.6511\n",
            "Epoch 737/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1103.0678 - val_loss: 1102.2090\n",
            "Epoch 738/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1103.6153 - val_loss: 1102.7388\n",
            "Epoch 739/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1101.4852 - val_loss: 1102.7029\n",
            "Epoch 740/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1101.6173 - val_loss: 1103.8721\n",
            "Epoch 741/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.9916 - val_loss: 1105.2388\n",
            "Epoch 742/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1101.8267 - val_loss: 1102.0426\n",
            "Epoch 743/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1101.6950 - val_loss: 1103.1389\n",
            "Epoch 744/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1100.3800 - val_loss: 1104.7036\n",
            "Epoch 745/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1101.2066 - val_loss: 1106.8767\n",
            "Epoch 746/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1102.3620 - val_loss: 1103.4908\n",
            "Epoch 747/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1100.4655 - val_loss: 1103.8625\n",
            "Epoch 748/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1101.0658 - val_loss: 1102.9524\n",
            "Epoch 749/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1101.2828 - val_loss: 1101.5814\n",
            "Epoch 750/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1101.8811 - val_loss: 1102.7982\n",
            "Epoch 751/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1100.7247 - val_loss: 1101.1549\n",
            "Epoch 752/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.7155 - val_loss: 1104.7258\n",
            "Epoch 753/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1100.5424 - val_loss: 1106.9738\n",
            "Epoch 754/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1101.1781 - val_loss: 1105.6840\n",
            "Epoch 755/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1101.7120 - val_loss: 1105.7023\n",
            "Epoch 756/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.5593 - val_loss: 1105.6600\n",
            "Epoch 757/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1101.8600 - val_loss: 1102.7672\n",
            "Epoch 758/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.1217 - val_loss: 1102.4331\n",
            "Epoch 759/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1101.0436 - val_loss: 1105.8651\n",
            "Epoch 760/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.7886 - val_loss: 1104.3486\n",
            "Epoch 761/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1100.2750 - val_loss: 1106.8794\n",
            "Epoch 762/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1102.9482 - val_loss: 1105.5605\n",
            "Epoch 763/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1101.8391 - val_loss: 1106.3356\n",
            "Epoch 764/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1103.3590 - val_loss: 1104.5240\n",
            "Epoch 765/2000\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1101.5118 - val_loss: 1105.1897\n",
            "Epoch 766/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1101.5761 - val_loss: 1104.0682\n",
            "Epoch 767/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1101.6891 - val_loss: 1104.8870\n",
            "Epoch 768/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.0842 - val_loss: 1105.3511\n",
            "Epoch 769/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1101.3795 - val_loss: 1105.6122\n",
            "Epoch 770/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.2217 - val_loss: 1105.1700\n",
            "Epoch 771/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1100.7722 - val_loss: 1105.0734\n",
            "Epoch 772/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1102.2451 - val_loss: 1105.6705\n",
            "Epoch 773/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1100.1708 - val_loss: 1104.1663\n",
            "Epoch 774/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1100.8404 - val_loss: 1104.6510\n",
            "Epoch 775/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.7164 - val_loss: 1102.2260\n",
            "Epoch 776/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.8352 - val_loss: 1102.3063\n",
            "Epoch 777/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1102.2579 - val_loss: 1102.4067\n",
            "Epoch 778/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.6542 - val_loss: 1104.8726\n",
            "Epoch 779/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1102.0201 - val_loss: 1103.6495\n",
            "Epoch 780/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1101.5113 - val_loss: 1106.5405\n",
            "Epoch 781/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1101.1818 - val_loss: 1103.2209\n",
            "Epoch 782/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1101.9434 - val_loss: 1101.8517\n",
            "Epoch 783/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.4654 - val_loss: 1103.6816\n",
            "Epoch 784/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1101.3938 - val_loss: 1104.8652\n",
            "Epoch 785/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1100.4319 - val_loss: 1101.9911\n",
            "Epoch 786/2000\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1100.2387 - val_loss: 1105.0366\n",
            "Epoch 787/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1100.2347 - val_loss: 1105.1560\n",
            "Epoch 788/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1100.4461 - val_loss: 1102.9497\n",
            "Epoch 789/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1101.3241 - val_loss: 1103.9243\n",
            "Epoch 790/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.2072 - val_loss: 1102.5428\n",
            "Epoch 791/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.0810 - val_loss: 1103.5007\n",
            "Epoch 792/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1101.3919 - val_loss: 1103.5488\n",
            "Epoch 793/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1102.5651 - val_loss: 1102.7163\n",
            "Epoch 794/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.6747 - val_loss: 1101.4761\n",
            "Epoch 795/2000\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1101.6023 - val_loss: 1102.4355\n",
            "Epoch 796/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1101.6179 - val_loss: 1101.8397\n",
            "Epoch 797/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1100.1797 - val_loss: 1102.5089\n",
            "Epoch 798/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1101.3543 - val_loss: 1104.2524\n",
            "Epoch 799/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1100.7302 - val_loss: 1104.5768\n",
            "Epoch 800/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.6432 - val_loss: 1101.1938\n",
            "Epoch 801/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1101.2248 - val_loss: 1103.6451\n",
            "Epoch 802/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.7755 - val_loss: 1102.5022\n",
            "Epoch 803/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.6100 - val_loss: 1102.7909\n",
            "Epoch 804/2000\n",
            "157/157 [==============================] - 0s 347us/step - loss: 1101.3935 - val_loss: 1104.3999\n",
            "Epoch 805/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1101.8137 - val_loss: 1105.0035\n",
            "Epoch 806/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1101.5923 - val_loss: 1101.2854\n",
            "Epoch 807/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.3302 - val_loss: 1099.5070\n",
            "Epoch 808/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.6880 - val_loss: 1100.6827\n",
            "Epoch 809/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.7907 - val_loss: 1103.2345\n",
            "Epoch 810/2000\n",
            "157/157 [==============================] - 0s 326us/step - loss: 1101.4107 - val_loss: 1101.1865\n",
            "Epoch 811/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1101.9199 - val_loss: 1102.6545\n",
            "Epoch 812/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1100.5137 - val_loss: 1100.6932\n",
            "Epoch 813/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1101.7109 - val_loss: 1100.9427\n",
            "Epoch 814/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.6005 - val_loss: 1102.8898\n",
            "Epoch 815/2000\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.2540 - val_loss: 1102.7500\n",
            "Epoch 816/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1100.9216 - val_loss: 1103.6873\n",
            "Epoch 817/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1101.2957 - val_loss: 1102.2793\n",
            "Epoch 818/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.6710 - val_loss: 1103.7372\n",
            "Epoch 819/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1100.6896 - val_loss: 1104.7567\n",
            "Epoch 820/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1101.2598 - val_loss: 1102.2025\n",
            "Epoch 821/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.6602 - val_loss: 1102.1013\n",
            "Epoch 822/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.6075 - val_loss: 1103.4537\n",
            "Epoch 823/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1100.8021 - val_loss: 1101.0610\n",
            "Epoch 824/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.4648 - val_loss: 1101.4326\n",
            "Epoch 825/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.9088 - val_loss: 1102.6666\n",
            "Epoch 826/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.0128 - val_loss: 1102.5134\n",
            "Epoch 827/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.4287 - val_loss: 1101.7676\n",
            "Epoch 828/2000\n",
            "157/157 [==============================] - 0s 356us/step - loss: 1100.5390 - val_loss: 1102.4709\n",
            "Epoch 829/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1102.5703 - val_loss: 1103.5945\n",
            "Epoch 830/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1101.0287 - val_loss: 1104.3447\n",
            "Epoch 831/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.2570 - val_loss: 1103.9640\n",
            "Epoch 832/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1100.4788 - val_loss: 1103.5620\n",
            "Epoch 833/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.6228 - val_loss: 1102.0487\n",
            "Epoch 834/2000\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1100.2936 - val_loss: 1101.9548\n",
            "Epoch 835/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1100.5755 - val_loss: 1102.5023\n",
            "Epoch 836/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1101.2454 - val_loss: 1102.6216\n",
            "Epoch 837/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1102.6024 - val_loss: 1101.1339\n",
            "Epoch 838/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1100.2936 - val_loss: 1103.3859\n",
            "Epoch 839/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1102.4230 - val_loss: 1102.0165\n",
            "Epoch 840/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1101.8704 - val_loss: 1102.3209\n",
            "Epoch 841/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.2444 - val_loss: 1103.3011\n",
            "Epoch 842/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1101.2089 - val_loss: 1104.2437\n",
            "Epoch 843/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.1306 - val_loss: 1103.7405\n",
            "Epoch 844/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.5147 - val_loss: 1104.2103\n",
            "Epoch 845/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1099.6903 - val_loss: 1104.3783\n",
            "Epoch 846/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.5188 - val_loss: 1103.8320\n",
            "Epoch 847/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.5588 - val_loss: 1102.6188\n",
            "Epoch 848/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.2758 - val_loss: 1103.2861\n",
            "Epoch 849/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1102.5134 - val_loss: 1101.9720\n",
            "Epoch 850/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1099.8660 - val_loss: 1103.0012\n",
            "Epoch 851/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.9211 - val_loss: 1101.2361\n",
            "Epoch 852/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.7645 - val_loss: 1102.6099\n",
            "Epoch 853/2000\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1100.1744 - val_loss: 1103.5461\n",
            "Epoch 854/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1101.4845 - val_loss: 1102.7163\n",
            "Epoch 855/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1101.3203 - val_loss: 1102.5549\n",
            "Epoch 856/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1102.6992 - val_loss: 1104.2593\n",
            "Epoch 857/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1101.8990 - val_loss: 1104.0337\n",
            "Epoch 858/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1103.0139 - val_loss: 1104.6217\n",
            "Epoch 859/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1102.8525 - val_loss: 1102.9266\n",
            "Epoch 860/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1101.4825 - val_loss: 1102.6274\n",
            "Epoch 861/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1101.3855 - val_loss: 1103.7950\n",
            "Epoch 862/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.6408 - val_loss: 1104.3782\n",
            "Epoch 863/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.6116 - val_loss: 1103.3075\n",
            "Epoch 864/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1100.6572 - val_loss: 1104.1830\n",
            "Epoch 865/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1100.6980 - val_loss: 1103.5781\n",
            "Epoch 866/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1101.3503 - val_loss: 1103.7129\n",
            "Epoch 867/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1102.0923 - val_loss: 1103.6980\n",
            "Epoch 868/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.1799 - val_loss: 1101.5603\n",
            "Epoch 869/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.3972 - val_loss: 1103.2463\n",
            "Epoch 870/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1099.8024 - val_loss: 1101.4301\n",
            "Epoch 871/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1101.5552 - val_loss: 1101.3452\n",
            "Epoch 872/2000\n",
            "157/157 [==============================] - 0s 328us/step - loss: 1100.8676 - val_loss: 1103.0433\n",
            "Epoch 873/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1102.5523 - val_loss: 1099.9952\n",
            "Epoch 874/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1100.8380 - val_loss: 1100.1427\n",
            "Epoch 875/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.5497 - val_loss: 1101.5404\n",
            "Epoch 876/2000\n",
            "157/157 [==============================] - 0s 329us/step - loss: 1099.4481 - val_loss: 1100.4841\n",
            "Epoch 877/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1101.5404 - val_loss: 1102.0670\n",
            "Epoch 878/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1101.6381 - val_loss: 1101.8687\n",
            "Epoch 879/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1100.1589 - val_loss: 1101.5818\n",
            "Epoch 880/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.9401 - val_loss: 1101.5513\n",
            "Epoch 881/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.5468 - val_loss: 1102.4377\n",
            "Epoch 882/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1101.0402 - val_loss: 1101.2363\n",
            "Epoch 883/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1101.6402 - val_loss: 1101.0775\n",
            "Epoch 884/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.9362 - val_loss: 1100.5962\n",
            "Epoch 885/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.5343 - val_loss: 1101.1874\n",
            "Epoch 886/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1102.1186 - val_loss: 1103.4791\n",
            "Epoch 887/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1100.3036 - val_loss: 1101.0262\n",
            "Epoch 888/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1101.2605 - val_loss: 1100.7070\n",
            "Epoch 889/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.0863 - val_loss: 1101.3844\n",
            "Epoch 890/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1099.8958 - val_loss: 1101.2593\n",
            "Epoch 891/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1099.8121 - val_loss: 1102.1083\n",
            "Epoch 892/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1101.1119 - val_loss: 1102.0269\n",
            "Epoch 893/2000\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1101.1988 - val_loss: 1101.8336\n",
            "Epoch 894/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.6906 - val_loss: 1104.3287\n",
            "Epoch 895/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1100.3404 - val_loss: 1102.8452\n",
            "Epoch 896/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1099.6376 - val_loss: 1101.1323\n",
            "Epoch 897/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1099.8077 - val_loss: 1101.9792\n",
            "Epoch 898/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.3463 - val_loss: 1102.2260\n",
            "Epoch 899/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.3774 - val_loss: 1101.4893\n",
            "Epoch 900/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1100.3094 - val_loss: 1100.1782\n",
            "Epoch 901/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1100.2941 - val_loss: 1100.8246\n",
            "Epoch 902/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.9723 - val_loss: 1102.2413\n",
            "Epoch 903/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1102.6347 - val_loss: 1103.7017\n",
            "Epoch 904/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.4803 - val_loss: 1101.4147\n",
            "Epoch 905/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.6320 - val_loss: 1102.1263\n",
            "Epoch 906/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1100.2767 - val_loss: 1102.7131\n",
            "Epoch 907/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1100.4063 - val_loss: 1104.0046\n",
            "Epoch 908/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1099.9888 - val_loss: 1104.4189\n",
            "Epoch 909/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1100.2703 - val_loss: 1107.5813\n",
            "Epoch 910/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1101.4475 - val_loss: 1107.9009\n",
            "Epoch 911/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.8305 - val_loss: 1106.5994\n",
            "Epoch 912/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.6343 - val_loss: 1103.9771\n",
            "Epoch 913/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.2864 - val_loss: 1102.4634\n",
            "Epoch 914/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1100.6557 - val_loss: 1102.2487\n",
            "Epoch 915/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1100.8911 - val_loss: 1103.2684\n",
            "Epoch 916/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1099.3617 - val_loss: 1101.6812\n",
            "Epoch 917/2000\n",
            "157/157 [==============================] - 0s 384us/step - loss: 1100.2198 - val_loss: 1104.1327\n",
            "Epoch 918/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.4340 - val_loss: 1103.6051\n",
            "Epoch 919/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1101.2931 - val_loss: 1102.0193\n",
            "Epoch 920/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1101.1660 - val_loss: 1103.4122\n",
            "Epoch 921/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1101.2241 - val_loss: 1100.5653\n",
            "Epoch 922/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1100.0934 - val_loss: 1103.7338\n",
            "Epoch 923/2000\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1101.1096 - val_loss: 1103.3083\n",
            "Epoch 924/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1100.8957 - val_loss: 1102.0759\n",
            "Epoch 925/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1101.1695 - val_loss: 1101.4458\n",
            "Epoch 926/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1099.7641 - val_loss: 1102.9922\n",
            "Epoch 927/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1101.1607 - val_loss: 1102.1161\n",
            "Epoch 928/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.7464 - val_loss: 1102.7576\n",
            "Epoch 929/2000\n",
            "157/157 [==============================] - 0s 363us/step - loss: 1103.3932 - val_loss: 1103.8252\n",
            "Epoch 930/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.3222 - val_loss: 1104.8405\n",
            "Epoch 931/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1100.1693 - val_loss: 1104.0382\n",
            "Epoch 932/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1099.9335 - val_loss: 1103.3461\n",
            "Epoch 933/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.8080 - val_loss: 1104.3553\n",
            "Epoch 934/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.0395 - val_loss: 1103.7476\n",
            "Epoch 935/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.2649 - val_loss: 1105.2462\n",
            "Epoch 936/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1101.5507 - val_loss: 1106.7587\n",
            "Epoch 937/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1101.3701 - val_loss: 1105.1569\n",
            "Epoch 938/2000\n",
            "157/157 [==============================] - 0s 336us/step - loss: 1100.7037 - val_loss: 1103.8252\n",
            "Epoch 939/2000\n",
            "157/157 [==============================] - 0s 336us/step - loss: 1102.0262 - val_loss: 1103.6200\n",
            "Epoch 940/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1099.3682 - val_loss: 1103.6200\n",
            "Epoch 941/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1103.3283 - val_loss: 1103.3744\n",
            "Epoch 942/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1099.7727 - val_loss: 1105.3999\n",
            "Epoch 943/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.6992 - val_loss: 1102.7809\n",
            "Epoch 944/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1100.5743 - val_loss: 1103.2054\n",
            "Epoch 945/2000\n",
            "157/157 [==============================] - 0s 350us/step - loss: 1100.1820 - val_loss: 1100.6132\n",
            "Epoch 946/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.0954 - val_loss: 1101.6226\n",
            "Epoch 947/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.3621 - val_loss: 1101.0020\n",
            "Epoch 948/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.4627 - val_loss: 1102.3826\n",
            "Epoch 949/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1101.0563 - val_loss: 1101.1205\n",
            "Epoch 950/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1100.7280 - val_loss: 1102.3347\n",
            "Epoch 951/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1100.4511 - val_loss: 1101.7368\n",
            "Epoch 952/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1102.4441 - val_loss: 1101.2059\n",
            "Epoch 953/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1101.8454 - val_loss: 1102.3892\n",
            "Epoch 954/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1101.2737 - val_loss: 1103.9672\n",
            "Epoch 955/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1101.1976 - val_loss: 1103.5901\n",
            "Epoch 956/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.8067 - val_loss: 1102.3920\n",
            "Epoch 957/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1099.6812 - val_loss: 1099.6970\n",
            "Epoch 958/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.9021 - val_loss: 1100.7043\n",
            "Epoch 959/2000\n",
            "157/157 [==============================] - 0s 333us/step - loss: 1100.2827 - val_loss: 1100.2562\n",
            "Epoch 960/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1101.0178 - val_loss: 1099.3757\n",
            "Epoch 961/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.9848 - val_loss: 1101.4807\n",
            "Epoch 962/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.1316 - val_loss: 1103.0573\n",
            "Epoch 963/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.2624 - val_loss: 1103.5524\n",
            "Epoch 964/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1100.6746 - val_loss: 1103.6152\n",
            "Epoch 965/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.6719 - val_loss: 1100.7095\n",
            "Epoch 966/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1102.0332 - val_loss: 1103.2682\n",
            "Epoch 967/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.7113 - val_loss: 1101.5782\n",
            "Epoch 968/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1100.0614 - val_loss: 1103.2437\n",
            "Epoch 969/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.9069 - val_loss: 1102.0825\n",
            "Epoch 970/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.2366 - val_loss: 1102.3574\n",
            "Epoch 971/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.8139 - val_loss: 1102.5718\n",
            "Epoch 972/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1101.0913 - val_loss: 1102.7230\n",
            "Epoch 973/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1099.1018 - val_loss: 1101.9885\n",
            "Epoch 974/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1102.4521 - val_loss: 1101.4989\n",
            "Epoch 975/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1100.0227 - val_loss: 1102.2142\n",
            "Epoch 976/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1102.1259 - val_loss: 1102.6440\n",
            "Epoch 977/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.8317 - val_loss: 1103.5834\n",
            "Epoch 978/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1101.3404 - val_loss: 1100.1814\n",
            "Epoch 979/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.3318 - val_loss: 1100.6124\n",
            "Epoch 980/2000\n",
            "157/157 [==============================] - 0s 354us/step - loss: 1101.8646 - val_loss: 1102.4600\n",
            "Epoch 981/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1101.1194 - val_loss: 1101.3195\n",
            "Epoch 982/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1101.3737 - val_loss: 1102.8010\n",
            "Epoch 983/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.0693 - val_loss: 1101.6707\n",
            "Epoch 984/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1100.2936 - val_loss: 1103.3268\n",
            "Epoch 985/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.2747 - val_loss: 1102.5571\n",
            "Epoch 986/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1100.4328 - val_loss: 1100.5624\n",
            "Epoch 987/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.8535 - val_loss: 1103.6165\n",
            "Epoch 988/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.9038 - val_loss: 1101.6093\n",
            "Epoch 989/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.1205 - val_loss: 1102.1392\n",
            "Epoch 990/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1100.0712 - val_loss: 1101.6581\n",
            "Epoch 991/2000\n",
            "157/157 [==============================] - 0s 348us/step - loss: 1100.3654 - val_loss: 1102.0657\n",
            "Epoch 992/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1100.9037 - val_loss: 1100.9037\n",
            "Epoch 993/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1101.1576 - val_loss: 1101.9081\n",
            "Epoch 994/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1101.8360 - val_loss: 1102.8246\n",
            "Epoch 995/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.2853 - val_loss: 1103.3326\n",
            "Epoch 996/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1100.1791 - val_loss: 1103.1149\n",
            "Epoch 997/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.8882 - val_loss: 1103.9193\n",
            "Epoch 998/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.6628 - val_loss: 1102.1919\n",
            "Epoch 999/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1099.9548 - val_loss: 1102.3378\n",
            "Epoch 1000/2000\n",
            "157/157 [==============================] - 0s 341us/step - loss: 1100.9560 - val_loss: 1103.1450\n",
            "Epoch 1001/2000\n",
            "157/157 [==============================] - 0s 345us/step - loss: 1100.7861 - val_loss: 1101.7944\n",
            "Epoch 1002/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1100.8489 - val_loss: 1101.2452\n",
            "Epoch 1003/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1099.5097 - val_loss: 1103.9351\n",
            "Epoch 1004/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.7231 - val_loss: 1101.7296\n",
            "Epoch 1005/2000\n",
            "157/157 [==============================] - 0s 360us/step - loss: 1100.4910 - val_loss: 1102.4712\n",
            "Epoch 1006/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1102.1358 - val_loss: 1102.2834\n",
            "Epoch 1007/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.1988 - val_loss: 1102.5636\n",
            "Epoch 1008/2000\n",
            "157/157 [==============================] - 0s 334us/step - loss: 1100.9092 - val_loss: 1103.6823\n",
            "Epoch 1009/2000\n",
            "157/157 [==============================] - 0s 345us/step - loss: 1101.6723 - val_loss: 1101.9124\n",
            "Epoch 1010/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1101.7030 - val_loss: 1101.2471\n",
            "Epoch 1011/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1102.1405 - val_loss: 1099.7299\n",
            "Epoch 1012/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.8967 - val_loss: 1100.4486\n",
            "Epoch 1013/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1099.8764 - val_loss: 1099.9927\n",
            "Epoch 1014/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.9295 - val_loss: 1103.0923\n",
            "Epoch 1015/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1100.4691 - val_loss: 1102.0007\n",
            "Epoch 1016/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1101.5711 - val_loss: 1103.2339\n",
            "Epoch 1017/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.3457 - val_loss: 1102.7936\n",
            "Epoch 1018/2000\n",
            "157/157 [==============================] - 0s 334us/step - loss: 1100.8042 - val_loss: 1103.2936\n",
            "Epoch 1019/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1099.8119 - val_loss: 1100.9844\n",
            "Epoch 1020/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1100.8633 - val_loss: 1101.1968\n",
            "Epoch 1021/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.4353 - val_loss: 1104.4243\n",
            "Epoch 1022/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1100.6574 - val_loss: 1103.6899\n",
            "Epoch 1023/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.8400 - val_loss: 1101.7062\n",
            "Epoch 1024/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1100.3261 - val_loss: 1102.2169\n",
            "Epoch 1025/2000\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1100.5075 - val_loss: 1103.7836\n",
            "Epoch 1026/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1100.0472 - val_loss: 1103.8544\n",
            "Epoch 1027/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1100.6662 - val_loss: 1102.3417\n",
            "Epoch 1028/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1100.9967 - val_loss: 1102.2007\n",
            "Epoch 1029/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1099.7651 - val_loss: 1103.0658\n",
            "Epoch 1030/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1100.0680 - val_loss: 1102.9778\n",
            "Epoch 1031/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.1810 - val_loss: 1101.5262\n",
            "Epoch 1032/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.3317 - val_loss: 1104.2572\n",
            "Epoch 1033/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.7124 - val_loss: 1103.9360\n",
            "Epoch 1034/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.3020 - val_loss: 1102.6826\n",
            "Epoch 1035/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.6953 - val_loss: 1102.5326\n",
            "Epoch 1036/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1101.1478 - val_loss: 1102.4248\n",
            "Epoch 1037/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.1289 - val_loss: 1101.1294\n",
            "Epoch 1038/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1100.2850 - val_loss: 1102.0450\n",
            "Epoch 1039/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1099.8795 - val_loss: 1100.1841\n",
            "Epoch 1040/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.6919 - val_loss: 1102.1481\n",
            "Epoch 1041/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.5535 - val_loss: 1101.4412\n",
            "Epoch 1042/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.2454 - val_loss: 1102.3586\n",
            "Epoch 1043/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1098.8707 - val_loss: 1102.3352\n",
            "Epoch 1044/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.4115 - val_loss: 1102.5886\n",
            "Epoch 1045/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.5520 - val_loss: 1103.5474\n",
            "Epoch 1046/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1100.0725 - val_loss: 1102.2073\n",
            "Epoch 1047/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.2973 - val_loss: 1103.2242\n",
            "Epoch 1048/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.8883 - val_loss: 1104.2285\n",
            "Epoch 1049/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.0333 - val_loss: 1105.0498\n",
            "Epoch 1050/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1100.5342 - val_loss: 1103.4095\n",
            "Epoch 1051/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.6050 - val_loss: 1103.6147\n",
            "Epoch 1052/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.9202 - val_loss: 1104.5690\n",
            "Epoch 1053/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1100.7787 - val_loss: 1103.3655\n",
            "Epoch 1054/2000\n",
            "157/157 [==============================] - 0s 362us/step - loss: 1100.8958 - val_loss: 1103.4025\n",
            "Epoch 1055/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1100.7516 - val_loss: 1104.2112\n",
            "Epoch 1056/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1101.0161 - val_loss: 1103.1294\n",
            "Epoch 1057/2000\n",
            "157/157 [==============================] - 0s 317us/step - loss: 1101.2967 - val_loss: 1102.8270\n",
            "Epoch 1058/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1100.2781 - val_loss: 1101.9073\n",
            "Epoch 1059/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1100.1398 - val_loss: 1102.0139\n",
            "Epoch 1060/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1100.5829 - val_loss: 1101.7180\n",
            "Epoch 1061/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1100.6818 - val_loss: 1102.3237\n",
            "Epoch 1062/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1100.6951 - val_loss: 1101.4658\n",
            "Epoch 1063/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1100.2199 - val_loss: 1101.5330\n",
            "Epoch 1064/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.9876 - val_loss: 1101.7361\n",
            "Epoch 1065/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.1478 - val_loss: 1101.9868\n",
            "Epoch 1066/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.7820 - val_loss: 1102.7986\n",
            "Epoch 1067/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1101.8331 - val_loss: 1101.8640\n",
            "Epoch 1068/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1099.7488 - val_loss: 1103.8124\n",
            "Epoch 1069/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1101.0507 - val_loss: 1104.7545\n",
            "Epoch 1070/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1100.0528 - val_loss: 1102.5454\n",
            "Epoch 1071/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.9281 - val_loss: 1102.7189\n",
            "Epoch 1072/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1101.2804 - val_loss: 1102.5104\n",
            "Epoch 1073/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.6225 - val_loss: 1101.1018\n",
            "Epoch 1074/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.2676 - val_loss: 1100.8308\n",
            "Epoch 1075/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1100.1446 - val_loss: 1101.1392\n",
            "Epoch 1076/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.8983 - val_loss: 1101.3325\n",
            "Epoch 1077/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.4582 - val_loss: 1102.8860\n",
            "Epoch 1078/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.0158 - val_loss: 1103.1838\n",
            "Epoch 1079/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1101.1082 - val_loss: 1101.0057\n",
            "Epoch 1080/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.6597 - val_loss: 1103.5798\n",
            "Epoch 1081/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1100.4900 - val_loss: 1103.5035\n",
            "Epoch 1082/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1101.0364 - val_loss: 1103.7666\n",
            "Epoch 1083/2000\n",
            "157/157 [==============================] - 0s 358us/step - loss: 1100.4337 - val_loss: 1103.2784\n",
            "Epoch 1084/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.6311 - val_loss: 1101.7621\n",
            "Epoch 1085/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1099.8538 - val_loss: 1102.3998\n",
            "Epoch 1086/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1100.1153 - val_loss: 1101.8905\n",
            "Epoch 1087/2000\n",
            "157/157 [==============================] - 0s 353us/step - loss: 1099.7692 - val_loss: 1100.7866\n",
            "Epoch 1088/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1101.0982 - val_loss: 1101.3403\n",
            "Epoch 1089/2000\n",
            "157/157 [==============================] - 0s 358us/step - loss: 1100.2012 - val_loss: 1100.5543\n",
            "Epoch 1090/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1099.1041 - val_loss: 1102.1051\n",
            "Epoch 1091/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1099.7427 - val_loss: 1102.7476\n",
            "Epoch 1092/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.7168 - val_loss: 1101.3794\n",
            "Epoch 1093/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1100.9421 - val_loss: 1101.0278\n",
            "Epoch 1094/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1099.0328 - val_loss: 1103.6814\n",
            "Epoch 1095/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1100.4720 - val_loss: 1103.0858\n",
            "Epoch 1096/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.5449 - val_loss: 1101.3970\n",
            "Epoch 1097/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.8467 - val_loss: 1101.6836\n",
            "Epoch 1098/2000\n",
            "157/157 [==============================] - 0s 329us/step - loss: 1099.5669 - val_loss: 1103.1252\n",
            "Epoch 1099/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1101.5312 - val_loss: 1101.8257\n",
            "Epoch 1100/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.7397 - val_loss: 1102.0708\n",
            "Epoch 1101/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.6696 - val_loss: 1101.3226\n",
            "Epoch 1102/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1099.8444 - val_loss: 1102.5961\n",
            "Epoch 1103/2000\n",
            "157/157 [==============================] - 0s 323us/step - loss: 1100.6028 - val_loss: 1102.9451\n",
            "Epoch 1104/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1099.7131 - val_loss: 1103.7214\n",
            "Epoch 1105/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1099.5853 - val_loss: 1101.8389\n",
            "Epoch 1106/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1100.0631 - val_loss: 1101.4939\n",
            "Epoch 1107/2000\n",
            "157/157 [==============================] - 0s 347us/step - loss: 1100.6343 - val_loss: 1101.4863\n",
            "Epoch 1108/2000\n",
            "157/157 [==============================] - 0s 308us/step - loss: 1100.3034 - val_loss: 1102.2944\n",
            "Epoch 1109/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.5975 - val_loss: 1102.2877\n",
            "Epoch 1110/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.8443 - val_loss: 1102.3237\n",
            "Epoch 1111/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1101.3110 - val_loss: 1103.1449\n",
            "Epoch 1112/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.0661 - val_loss: 1102.8665\n",
            "Epoch 1113/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1102.2303 - val_loss: 1103.0405\n",
            "Epoch 1114/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1101.1740 - val_loss: 1103.6570\n",
            "Epoch 1115/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.7387 - val_loss: 1103.3917\n",
            "Epoch 1116/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.9175 - val_loss: 1105.3049\n",
            "Epoch 1117/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1100.2255 - val_loss: 1104.6439\n",
            "Epoch 1118/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.2133 - val_loss: 1102.3560\n",
            "Epoch 1119/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1100.6070 - val_loss: 1103.3344\n",
            "Epoch 1120/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1100.0869 - val_loss: 1101.8833\n",
            "Epoch 1121/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1099.6971 - val_loss: 1102.1382\n",
            "Epoch 1122/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1101.3881 - val_loss: 1102.4138\n",
            "Epoch 1123/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.6653 - val_loss: 1103.3253\n",
            "Epoch 1124/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.2277 - val_loss: 1102.9523\n",
            "Epoch 1125/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.3333 - val_loss: 1101.8839\n",
            "Epoch 1126/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.9234 - val_loss: 1101.6003\n",
            "Epoch 1127/2000\n",
            "157/157 [==============================] - 0s 346us/step - loss: 1100.1946 - val_loss: 1102.2272\n",
            "Epoch 1128/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1100.0939 - val_loss: 1100.9974\n",
            "Epoch 1129/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1098.6978 - val_loss: 1102.6177\n",
            "Epoch 1130/2000\n",
            "157/157 [==============================] - 0s 338us/step - loss: 1100.3524 - val_loss: 1103.6072\n",
            "Epoch 1131/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.7664 - val_loss: 1102.9977\n",
            "Epoch 1132/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.1605 - val_loss: 1103.5560\n",
            "Epoch 1133/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1101.0777 - val_loss: 1102.1787\n",
            "Epoch 1134/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1100.9408 - val_loss: 1104.0012\n",
            "Epoch 1135/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.4668 - val_loss: 1102.9824\n",
            "Epoch 1136/2000\n",
            "157/157 [==============================] - 0s 335us/step - loss: 1099.2747 - val_loss: 1101.9993\n",
            "Epoch 1137/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1101.8895 - val_loss: 1101.4214\n",
            "Epoch 1138/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.3127 - val_loss: 1101.1863\n",
            "Epoch 1139/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.2633 - val_loss: 1101.1736\n",
            "Epoch 1140/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.7247 - val_loss: 1101.6415\n",
            "Epoch 1141/2000\n",
            "157/157 [==============================] - 0s 323us/step - loss: 1100.0883 - val_loss: 1099.2747\n",
            "Epoch 1142/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.3525 - val_loss: 1101.8265\n",
            "Epoch 1143/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.8188 - val_loss: 1100.7894\n",
            "Epoch 1144/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1101.2607 - val_loss: 1100.4729\n",
            "Epoch 1145/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1100.0709 - val_loss: 1099.9856\n",
            "Epoch 1146/2000\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1099.8214 - val_loss: 1099.9314\n",
            "Epoch 1147/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1101.2909 - val_loss: 1099.7537\n",
            "Epoch 1148/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1100.9040 - val_loss: 1101.0374\n",
            "Epoch 1149/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1100.8978 - val_loss: 1100.3052\n",
            "Epoch 1150/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.9235 - val_loss: 1101.4963\n",
            "Epoch 1151/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.3066 - val_loss: 1103.7820\n",
            "Epoch 1152/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.0336 - val_loss: 1101.6597\n",
            "Epoch 1153/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.4195 - val_loss: 1100.5210\n",
            "Epoch 1154/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.9136 - val_loss: 1100.6968\n",
            "Epoch 1155/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1098.9578 - val_loss: 1102.0646\n",
            "Epoch 1156/2000\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1100.7434 - val_loss: 1101.6967\n",
            "Epoch 1157/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1100.1276 - val_loss: 1101.7726\n",
            "Epoch 1158/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.2251 - val_loss: 1102.0183\n",
            "Epoch 1159/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1100.1188 - val_loss: 1102.3329\n",
            "Epoch 1160/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.2265 - val_loss: 1101.1289\n",
            "Epoch 1161/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1100.4963 - val_loss: 1101.6864\n",
            "Epoch 1162/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1099.7936 - val_loss: 1101.1227\n",
            "Epoch 1163/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.8482 - val_loss: 1100.7883\n",
            "Epoch 1164/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.0276 - val_loss: 1102.0327\n",
            "Epoch 1165/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.3628 - val_loss: 1103.3691\n",
            "Epoch 1166/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.9158 - val_loss: 1101.5287\n",
            "Epoch 1167/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1099.3000 - val_loss: 1102.6118\n",
            "Epoch 1168/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.0999 - val_loss: 1104.2054\n",
            "Epoch 1169/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1099.5667 - val_loss: 1103.7249\n",
            "Epoch 1170/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.8471 - val_loss: 1102.9122\n",
            "Epoch 1171/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.4549 - val_loss: 1103.2888\n",
            "Epoch 1172/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.0950 - val_loss: 1102.1096\n",
            "Epoch 1173/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1100.4924 - val_loss: 1102.2170\n",
            "Epoch 1174/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.2553 - val_loss: 1100.8787\n",
            "Epoch 1175/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1100.7813 - val_loss: 1100.8284\n",
            "Epoch 1176/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1100.6705 - val_loss: 1103.0737\n",
            "Epoch 1177/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1099.3667 - val_loss: 1100.6836\n",
            "Epoch 1178/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.5283 - val_loss: 1102.1155\n",
            "Epoch 1179/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1099.7284 - val_loss: 1101.7153\n",
            "Epoch 1180/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.2943 - val_loss: 1103.4069\n",
            "Epoch 1181/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.6151 - val_loss: 1103.0640\n",
            "Epoch 1182/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1100.4429 - val_loss: 1102.2070\n",
            "Epoch 1183/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.0170 - val_loss: 1103.1671\n",
            "Epoch 1184/2000\n",
            "157/157 [==============================] - 0s 343us/step - loss: 1099.1186 - val_loss: 1102.2153\n",
            "Epoch 1185/2000\n",
            "157/157 [==============================] - 0s 337us/step - loss: 1101.0630 - val_loss: 1101.8046\n",
            "Epoch 1186/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1099.5152 - val_loss: 1102.4132\n",
            "Epoch 1187/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1101.6878 - val_loss: 1104.3390\n",
            "Epoch 1188/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.7791 - val_loss: 1104.5278\n",
            "Epoch 1189/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1100.6252 - val_loss: 1105.2518\n",
            "Epoch 1190/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.9010 - val_loss: 1105.9114\n",
            "Epoch 1191/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.2777 - val_loss: 1105.3671\n",
            "Epoch 1192/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.2102 - val_loss: 1104.6561\n",
            "Epoch 1193/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.8292 - val_loss: 1106.1202\n",
            "Epoch 1194/2000\n",
            "157/157 [==============================] - 0s 328us/step - loss: 1098.6721 - val_loss: 1105.1324\n",
            "Epoch 1195/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1101.4302 - val_loss: 1103.5829\n",
            "Epoch 1196/2000\n",
            "157/157 [==============================] - 0s 337us/step - loss: 1100.2661 - val_loss: 1103.7039\n",
            "Epoch 1197/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1099.8038 - val_loss: 1103.4569\n",
            "Epoch 1198/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1101.0103 - val_loss: 1104.4220\n",
            "Epoch 1199/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1099.8719 - val_loss: 1103.8345\n",
            "Epoch 1200/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1101.3149 - val_loss: 1102.5050\n",
            "Epoch 1201/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1100.7811 - val_loss: 1104.3253\n",
            "Epoch 1202/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1099.8992 - val_loss: 1103.3320\n",
            "Epoch 1203/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.8048 - val_loss: 1102.6243\n",
            "Epoch 1204/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1100.8370 - val_loss: 1102.1305\n",
            "Epoch 1205/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1099.6096 - val_loss: 1101.5786\n",
            "Epoch 1206/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1100.1379 - val_loss: 1100.0631\n",
            "Epoch 1207/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1100.0110 - val_loss: 1099.9771\n",
            "Epoch 1208/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.4227 - val_loss: 1100.6997\n",
            "Epoch 1209/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1100.1519 - val_loss: 1102.0270\n",
            "Epoch 1210/2000\n",
            "157/157 [==============================] - 0s 328us/step - loss: 1102.0265 - val_loss: 1102.2919\n",
            "Epoch 1211/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1102.2631 - val_loss: 1102.3086\n",
            "Epoch 1212/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1100.0284 - val_loss: 1100.6029\n",
            "Epoch 1213/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.3748 - val_loss: 1099.8579\n",
            "Epoch 1214/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1098.9712 - val_loss: 1100.5436\n",
            "Epoch 1215/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1098.9826 - val_loss: 1101.5886\n",
            "Epoch 1216/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1099.1417 - val_loss: 1102.2701\n",
            "Epoch 1217/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.6631 - val_loss: 1103.1050\n",
            "Epoch 1218/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1103.2740 - val_loss: 1102.7703\n",
            "Epoch 1219/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1099.7236 - val_loss: 1100.9459\n",
            "Epoch 1220/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.7656 - val_loss: 1103.6406\n",
            "Epoch 1221/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1099.6961 - val_loss: 1104.2421\n",
            "Epoch 1222/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1099.1324 - val_loss: 1100.9644\n",
            "Epoch 1223/2000\n",
            "157/157 [==============================] - 0s 316us/step - loss: 1099.4068 - val_loss: 1102.7905\n",
            "Epoch 1224/2000\n",
            "157/157 [==============================] - 0s 328us/step - loss: 1101.0867 - val_loss: 1103.4000\n",
            "Epoch 1225/2000\n",
            "157/157 [==============================] - 0s 385us/step - loss: 1099.5583 - val_loss: 1102.2456\n",
            "Epoch 1226/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.6736 - val_loss: 1100.5676\n",
            "Epoch 1227/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1100.9964 - val_loss: 1100.3564\n",
            "Epoch 1228/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1100.3614 - val_loss: 1102.9902\n",
            "Epoch 1229/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1100.2061 - val_loss: 1101.3566\n",
            "Epoch 1230/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.6889 - val_loss: 1099.8264\n",
            "Epoch 1231/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.0939 - val_loss: 1100.0765\n",
            "Epoch 1232/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1100.1283 - val_loss: 1100.1061\n",
            "Epoch 1233/2000\n",
            "157/157 [==============================] - 0s 326us/step - loss: 1099.8081 - val_loss: 1101.2686\n",
            "Epoch 1234/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.2633 - val_loss: 1101.8844\n",
            "Epoch 1235/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1099.7333 - val_loss: 1101.3986\n",
            "Epoch 1236/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.8690 - val_loss: 1102.1587\n",
            "Epoch 1237/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.0672 - val_loss: 1101.7019\n",
            "Epoch 1238/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1100.9170 - val_loss: 1101.4911\n",
            "Epoch 1239/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1100.7493 - val_loss: 1102.1006\n",
            "Epoch 1240/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.8646 - val_loss: 1100.7565\n",
            "Epoch 1241/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1100.2359 - val_loss: 1100.1821\n",
            "Epoch 1242/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.3619 - val_loss: 1100.3459\n",
            "Epoch 1243/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.6821 - val_loss: 1100.1470\n",
            "Epoch 1244/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1099.7613 - val_loss: 1100.9353\n",
            "Epoch 1245/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.9356 - val_loss: 1102.4800\n",
            "Epoch 1246/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.3910 - val_loss: 1102.6127\n",
            "Epoch 1247/2000\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1099.8029 - val_loss: 1100.5790\n",
            "Epoch 1248/2000\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1100.3384 - val_loss: 1100.6125\n",
            "Epoch 1249/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1099.6472 - val_loss: 1100.1683\n",
            "Epoch 1250/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.9580 - val_loss: 1102.4282\n",
            "Epoch 1251/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.4113 - val_loss: 1101.7859\n",
            "Epoch 1252/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.7592 - val_loss: 1101.6837\n",
            "Epoch 1253/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.8574 - val_loss: 1100.4076\n",
            "Epoch 1254/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.7430 - val_loss: 1101.8336\n",
            "Epoch 1255/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.8705 - val_loss: 1102.6587\n",
            "Epoch 1256/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.7703 - val_loss: 1102.6793\n",
            "Epoch 1257/2000\n",
            "157/157 [==============================] - 0s 329us/step - loss: 1099.4404 - val_loss: 1104.0868\n",
            "Epoch 1258/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1098.9592 - val_loss: 1101.7737\n",
            "Epoch 1259/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.0545 - val_loss: 1102.5853\n",
            "Epoch 1260/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1100.3052 - val_loss: 1103.5603\n",
            "Epoch 1261/2000\n",
            "157/157 [==============================] - 0s 352us/step - loss: 1099.8859 - val_loss: 1100.0437\n",
            "Epoch 1262/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.1460 - val_loss: 1101.3964\n",
            "Epoch 1263/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1100.5233 - val_loss: 1101.1857\n",
            "Epoch 1264/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.1950 - val_loss: 1102.2056\n",
            "Epoch 1265/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.0879 - val_loss: 1102.1510\n",
            "Epoch 1266/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1098.9936 - val_loss: 1103.2819\n",
            "Epoch 1267/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1100.7164 - val_loss: 1101.5250\n",
            "Epoch 1268/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.9230 - val_loss: 1103.7372\n",
            "Epoch 1269/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.3708 - val_loss: 1101.6932\n",
            "Epoch 1270/2000\n",
            "157/157 [==============================] - 0s 406us/step - loss: 1099.5594 - val_loss: 1101.7089\n",
            "Epoch 1271/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.3018 - val_loss: 1102.3854\n",
            "Epoch 1272/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.5060 - val_loss: 1101.6431\n",
            "Epoch 1273/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1098.9954 - val_loss: 1104.2704\n",
            "Epoch 1274/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1100.5185 - val_loss: 1104.6036\n",
            "Epoch 1275/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.1430 - val_loss: 1103.4830\n",
            "Epoch 1276/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.5346 - val_loss: 1101.1998\n",
            "Epoch 1277/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.1253 - val_loss: 1101.6404\n",
            "Epoch 1278/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.9566 - val_loss: 1102.4797\n",
            "Epoch 1279/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.8165 - val_loss: 1101.0408\n",
            "Epoch 1280/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.7487 - val_loss: 1101.9031\n",
            "Epoch 1281/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.8093 - val_loss: 1101.8636\n",
            "Epoch 1282/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1099.3617 - val_loss: 1103.5707\n",
            "Epoch 1283/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.2688 - val_loss: 1103.1700\n",
            "Epoch 1284/2000\n",
            "157/157 [==============================] - 0s 359us/step - loss: 1099.9458 - val_loss: 1103.5093\n",
            "Epoch 1285/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.1480 - val_loss: 1103.1848\n",
            "Epoch 1286/2000\n",
            "157/157 [==============================] - 0s 390us/step - loss: 1100.8213 - val_loss: 1103.4165\n",
            "Epoch 1287/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.4952 - val_loss: 1102.3812\n",
            "Epoch 1288/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1099.6277 - val_loss: 1102.0697\n",
            "Epoch 1289/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1099.8785 - val_loss: 1102.0497\n",
            "Epoch 1290/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.4075 - val_loss: 1100.9390\n",
            "Epoch 1291/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.7320 - val_loss: 1101.8171\n",
            "Epoch 1292/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.7177 - val_loss: 1101.7328\n",
            "Epoch 1293/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1099.3999 - val_loss: 1101.8883\n",
            "Epoch 1294/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.8276 - val_loss: 1101.9551\n",
            "Epoch 1295/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.9078 - val_loss: 1100.1300\n",
            "Epoch 1296/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1099.9652 - val_loss: 1100.0691\n",
            "Epoch 1297/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.7052 - val_loss: 1100.9402\n",
            "Epoch 1298/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1100.2226 - val_loss: 1101.1755\n",
            "Epoch 1299/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1099.4222 - val_loss: 1100.6632\n",
            "Epoch 1300/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.1072 - val_loss: 1101.3074\n",
            "Epoch 1301/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.1671 - val_loss: 1101.1252\n",
            "Epoch 1302/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1100.4659 - val_loss: 1104.1733\n",
            "Epoch 1303/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.8688 - val_loss: 1102.5353\n",
            "Epoch 1304/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1099.9901 - val_loss: 1102.5977\n",
            "Epoch 1305/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.7512 - val_loss: 1103.5067\n",
            "Epoch 1306/2000\n",
            "157/157 [==============================] - 0s 441us/step - loss: 1098.8703 - val_loss: 1101.3875\n",
            "Epoch 1307/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1099.9214 - val_loss: 1102.6187\n",
            "Epoch 1308/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.7990 - val_loss: 1102.5771\n",
            "Epoch 1309/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.6853 - val_loss: 1102.4468\n",
            "Epoch 1310/2000\n",
            "157/157 [==============================] - 0s 351us/step - loss: 1105.4820 - val_loss: 1104.0471\n",
            "Epoch 1311/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1099.5068 - val_loss: 1102.7333\n",
            "Epoch 1312/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.4614 - val_loss: 1101.7957\n",
            "Epoch 1313/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1100.4391 - val_loss: 1102.8892\n",
            "Epoch 1314/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1099.3374 - val_loss: 1104.5688\n",
            "Epoch 1315/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.3194 - val_loss: 1103.4401\n",
            "Epoch 1316/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.8852 - val_loss: 1102.8328\n",
            "Epoch 1317/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1098.7901 - val_loss: 1103.8414\n",
            "Epoch 1318/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.1891 - val_loss: 1102.3436\n",
            "Epoch 1319/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.1497 - val_loss: 1102.2512\n",
            "Epoch 1320/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.2183 - val_loss: 1100.3116\n",
            "Epoch 1321/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1100.8883 - val_loss: 1100.0416\n",
            "Epoch 1322/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1100.7314 - val_loss: 1100.3652\n",
            "Epoch 1323/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1098.6551 - val_loss: 1100.8142\n",
            "Epoch 1324/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.7590 - val_loss: 1100.1892\n",
            "Epoch 1325/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1101.6547 - val_loss: 1102.1183\n",
            "Epoch 1326/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1101.6476 - val_loss: 1101.3932\n",
            "Epoch 1327/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1098.9058 - val_loss: 1102.1733\n",
            "Epoch 1328/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.7765 - val_loss: 1100.3234\n",
            "Epoch 1329/2000\n",
            "157/157 [==============================] - 0s 344us/step - loss: 1099.7593 - val_loss: 1100.1255\n",
            "Epoch 1330/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.4365 - val_loss: 1100.6866\n",
            "Epoch 1331/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1101.0472 - val_loss: 1101.5236\n",
            "Epoch 1332/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1101.0464 - val_loss: 1100.8301\n",
            "Epoch 1333/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.9852 - val_loss: 1101.4836\n",
            "Epoch 1334/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1099.6865 - val_loss: 1102.3097\n",
            "Epoch 1335/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.9124 - val_loss: 1104.5941\n",
            "Epoch 1336/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1100.3840 - val_loss: 1102.3265\n",
            "Epoch 1337/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.2493 - val_loss: 1103.4747\n",
            "Epoch 1338/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1100.4975 - val_loss: 1102.1910\n",
            "Epoch 1339/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1099.9662 - val_loss: 1101.5220\n",
            "Epoch 1340/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1100.1575 - val_loss: 1103.3140\n",
            "Epoch 1341/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.5562 - val_loss: 1103.0033\n",
            "Epoch 1342/2000\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1099.6455 - val_loss: 1103.5682\n",
            "Epoch 1343/2000\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1100.6916 - val_loss: 1102.1412\n",
            "Epoch 1344/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.7460 - val_loss: 1103.1428\n",
            "Epoch 1345/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.4287 - val_loss: 1103.9896\n",
            "Epoch 1346/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1100.0613 - val_loss: 1103.2542\n",
            "Epoch 1347/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.1643 - val_loss: 1103.3290\n",
            "Epoch 1348/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.9131 - val_loss: 1101.7561\n",
            "Epoch 1349/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1100.7058 - val_loss: 1104.1831\n",
            "Epoch 1350/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1101.7212 - val_loss: 1102.1938\n",
            "Epoch 1351/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.1135 - val_loss: 1100.8412\n",
            "Epoch 1352/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.6622 - val_loss: 1102.5293\n",
            "Epoch 1353/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.4392 - val_loss: 1100.1830\n",
            "Epoch 1354/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1098.8597 - val_loss: 1101.4736\n",
            "Epoch 1355/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.2232 - val_loss: 1101.5363\n",
            "Epoch 1356/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1099.8676 - val_loss: 1101.5941\n",
            "Epoch 1357/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.0206 - val_loss: 1101.0892\n",
            "Epoch 1358/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1098.6478 - val_loss: 1103.3292\n",
            "Epoch 1359/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1098.9396 - val_loss: 1102.5161\n",
            "Epoch 1360/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1099.5286 - val_loss: 1102.1432\n",
            "Epoch 1361/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.1225 - val_loss: 1103.5840\n",
            "Epoch 1362/2000\n",
            "157/157 [==============================] - 0s 348us/step - loss: 1100.4390 - val_loss: 1103.1608\n",
            "Epoch 1363/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1100.3472 - val_loss: 1102.8325\n",
            "Epoch 1364/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1101.0716 - val_loss: 1102.7642\n",
            "Epoch 1365/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1101.3873 - val_loss: 1104.7126\n",
            "Epoch 1366/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1100.8356 - val_loss: 1102.5902\n",
            "Epoch 1367/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1102.4704 - val_loss: 1102.0958\n",
            "Epoch 1368/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1100.6405 - val_loss: 1101.9398\n",
            "Epoch 1369/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1100.0429 - val_loss: 1103.1903\n",
            "Epoch 1370/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1100.7068 - val_loss: 1102.0085\n",
            "Epoch 1371/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.0047 - val_loss: 1101.5396\n",
            "Epoch 1372/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.6848 - val_loss: 1100.1892\n",
            "Epoch 1373/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.3338 - val_loss: 1102.0636\n",
            "Epoch 1374/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1100.3112 - val_loss: 1101.8622\n",
            "Epoch 1375/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1099.7296 - val_loss: 1102.6063\n",
            "Epoch 1376/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1098.5681 - val_loss: 1100.2969\n",
            "Epoch 1377/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1098.9449 - val_loss: 1102.0065\n",
            "Epoch 1378/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.5470 - val_loss: 1100.9924\n",
            "Epoch 1379/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1099.7649 - val_loss: 1102.1237\n",
            "Epoch 1380/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.3043 - val_loss: 1099.6904\n",
            "Epoch 1381/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.3019 - val_loss: 1100.6094\n",
            "Epoch 1382/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1100.4384 - val_loss: 1101.0938\n",
            "Epoch 1383/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1098.9882 - val_loss: 1101.0022\n",
            "Epoch 1384/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.1098 - val_loss: 1101.9404\n",
            "Epoch 1385/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.3337 - val_loss: 1102.2069\n",
            "Epoch 1386/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1098.9840 - val_loss: 1104.2527\n",
            "Epoch 1387/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.5327 - val_loss: 1102.4036\n",
            "Epoch 1388/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.0867 - val_loss: 1103.4419\n",
            "Epoch 1389/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.3953 - val_loss: 1102.5399\n",
            "Epoch 1390/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.0765 - val_loss: 1103.3162\n",
            "Epoch 1391/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.6861 - val_loss: 1102.8285\n",
            "Epoch 1392/2000\n",
            "157/157 [==============================] - 0s 360us/step - loss: 1099.8013 - val_loss: 1102.7017\n",
            "Epoch 1393/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.5724 - val_loss: 1103.6731\n",
            "Epoch 1394/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1099.3809 - val_loss: 1101.7572\n",
            "Epoch 1395/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1098.9327 - val_loss: 1100.7363\n",
            "Epoch 1396/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1099.4298 - val_loss: 1101.6843\n",
            "Epoch 1397/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1100.0407 - val_loss: 1101.0948\n",
            "Epoch 1398/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1098.4067 - val_loss: 1103.6244\n",
            "Epoch 1399/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.0621 - val_loss: 1102.5881\n",
            "Epoch 1400/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.0396 - val_loss: 1103.6831\n",
            "Epoch 1401/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1099.6285 - val_loss: 1102.9316\n",
            "Epoch 1402/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1101.3374 - val_loss: 1103.0947\n",
            "Epoch 1403/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1099.0930 - val_loss: 1100.7051\n",
            "Epoch 1404/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1101.1609 - val_loss: 1099.6631\n",
            "Epoch 1405/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.8511 - val_loss: 1100.7668\n",
            "Epoch 1406/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1098.9868 - val_loss: 1100.2765\n",
            "Epoch 1407/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.1109 - val_loss: 1101.4412\n",
            "Epoch 1408/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.7522 - val_loss: 1101.8737\n",
            "Epoch 1409/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.3680 - val_loss: 1102.9111\n",
            "Epoch 1410/2000\n",
            "157/157 [==============================] - 0s 338us/step - loss: 1099.3670 - val_loss: 1104.1479\n",
            "Epoch 1411/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1098.7782 - val_loss: 1103.5375\n",
            "Epoch 1412/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.8165 - val_loss: 1104.1434\n",
            "Epoch 1413/2000\n",
            "157/157 [==============================] - 0s 332us/step - loss: 1099.4893 - val_loss: 1102.7498\n",
            "Epoch 1414/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1100.5538 - val_loss: 1103.7643\n",
            "Epoch 1415/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1100.4645 - val_loss: 1101.2578\n",
            "Epoch 1416/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1099.9079 - val_loss: 1100.6237\n",
            "Epoch 1417/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.7629 - val_loss: 1101.3652\n",
            "Epoch 1418/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1099.8639 - val_loss: 1100.8870\n",
            "Epoch 1419/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1099.3651 - val_loss: 1101.4185\n",
            "Epoch 1420/2000\n",
            "157/157 [==============================] - 0s 388us/step - loss: 1099.1167 - val_loss: 1101.3448\n",
            "Epoch 1421/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.3901 - val_loss: 1100.8788\n",
            "Epoch 1422/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.1035 - val_loss: 1100.6965\n",
            "Epoch 1423/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1099.9796 - val_loss: 1100.1987\n",
            "Epoch 1424/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.4749 - val_loss: 1101.7491\n",
            "Epoch 1425/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1098.5366 - val_loss: 1102.8188\n",
            "Epoch 1426/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1099.7108 - val_loss: 1101.8671\n",
            "Epoch 1427/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1101.3092 - val_loss: 1102.6115\n",
            "Epoch 1428/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.6191 - val_loss: 1102.3740\n",
            "Epoch 1429/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1099.0338 - val_loss: 1104.0780\n",
            "Epoch 1430/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1099.5628 - val_loss: 1103.7520\n",
            "Epoch 1431/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1099.7386 - val_loss: 1103.9321\n",
            "Epoch 1432/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1101.1689 - val_loss: 1104.9712\n",
            "Epoch 1433/2000\n",
            "157/157 [==============================] - 0s 342us/step - loss: 1101.3705 - val_loss: 1103.2567\n",
            "Epoch 1434/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.3230 - val_loss: 1104.5554\n",
            "Epoch 1435/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.9000 - val_loss: 1103.1743\n",
            "Epoch 1436/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.8207 - val_loss: 1102.8657\n",
            "Epoch 1437/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1101.1932 - val_loss: 1103.5391\n",
            "Epoch 1438/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1100.2465 - val_loss: 1100.4200\n",
            "Epoch 1439/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.1147 - val_loss: 1102.4692\n",
            "Epoch 1440/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.0537 - val_loss: 1099.8716\n",
            "Epoch 1441/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.1838 - val_loss: 1101.4760\n",
            "Epoch 1442/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.4106 - val_loss: 1100.8977\n",
            "Epoch 1443/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.6423 - val_loss: 1102.8641\n",
            "Epoch 1444/2000\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1099.8362 - val_loss: 1102.3998\n",
            "Epoch 1445/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.5803 - val_loss: 1100.9943\n",
            "Epoch 1446/2000\n",
            "157/157 [==============================] - 0s 306us/step - loss: 1099.9984 - val_loss: 1100.6978\n",
            "Epoch 1447/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1098.9278 - val_loss: 1100.3447\n",
            "Epoch 1448/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.7385 - val_loss: 1102.5259\n",
            "Epoch 1449/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.3954 - val_loss: 1102.5386\n",
            "Epoch 1450/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1099.3896 - val_loss: 1101.6053\n",
            "Epoch 1451/2000\n",
            "157/157 [==============================] - 0s 350us/step - loss: 1099.4895 - val_loss: 1103.6465\n",
            "Epoch 1452/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.0183 - val_loss: 1102.8876\n",
            "Epoch 1453/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.8575 - val_loss: 1103.4695\n",
            "Epoch 1454/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1100.0255 - val_loss: 1104.0537\n",
            "Epoch 1455/2000\n",
            "157/157 [==============================] - 0s 334us/step - loss: 1100.0413 - val_loss: 1103.1139\n",
            "Epoch 1456/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1099.3778 - val_loss: 1103.6836\n",
            "Epoch 1457/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.5951 - val_loss: 1101.6615\n",
            "Epoch 1458/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.2058 - val_loss: 1103.4490\n",
            "Epoch 1459/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.4596 - val_loss: 1102.0189\n",
            "Epoch 1460/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.8099 - val_loss: 1102.3641\n",
            "Epoch 1461/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1100.5729 - val_loss: 1102.3027\n",
            "Epoch 1462/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1100.1075 - val_loss: 1101.7153\n",
            "Epoch 1463/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1100.5203 - val_loss: 1100.7489\n",
            "Epoch 1464/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1100.5331 - val_loss: 1102.5723\n",
            "Epoch 1465/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1102.1717 - val_loss: 1105.3979\n",
            "Epoch 1466/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1101.7819 - val_loss: 1104.5470\n",
            "Epoch 1467/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1100.0032 - val_loss: 1104.9390\n",
            "Epoch 1468/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.2157 - val_loss: 1103.1390\n",
            "Epoch 1469/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.4104 - val_loss: 1103.0455\n",
            "Epoch 1470/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1098.8905 - val_loss: 1103.2849\n",
            "Epoch 1471/2000\n",
            "157/157 [==============================] - 0s 388us/step - loss: 1099.1976 - val_loss: 1101.7847\n",
            "Epoch 1472/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.9420 - val_loss: 1101.2540\n",
            "Epoch 1473/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1099.1034 - val_loss: 1101.8431\n",
            "Epoch 1474/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1101.1812 - val_loss: 1101.3977\n",
            "Epoch 1475/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.1633 - val_loss: 1103.3059\n",
            "Epoch 1476/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1100.1094 - val_loss: 1102.9028\n",
            "Epoch 1477/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1100.0368 - val_loss: 1102.6938\n",
            "Epoch 1478/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1101.1789 - val_loss: 1104.0216\n",
            "Epoch 1479/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1098.8276 - val_loss: 1103.0251\n",
            "Epoch 1480/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1099.0323 - val_loss: 1103.2632\n",
            "Epoch 1481/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1099.4435 - val_loss: 1104.2073\n",
            "Epoch 1482/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1100.1523 - val_loss: 1103.6903\n",
            "Epoch 1483/2000\n",
            "157/157 [==============================] - 0s 412us/step - loss: 1099.2857 - val_loss: 1101.5990\n",
            "Epoch 1484/2000\n",
            "157/157 [==============================] - 0s 352us/step - loss: 1099.2212 - val_loss: 1106.9457\n",
            "Epoch 1485/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.9635 - val_loss: 1101.2697\n",
            "Epoch 1486/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1100.0076 - val_loss: 1103.2020\n",
            "Epoch 1487/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.7397 - val_loss: 1102.0980\n",
            "Epoch 1488/2000\n",
            "157/157 [==============================] - 0s 314us/step - loss: 1099.1947 - val_loss: 1101.7893\n",
            "Epoch 1489/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.2718 - val_loss: 1103.1577\n",
            "Epoch 1490/2000\n",
            "157/157 [==============================] - 0s 341us/step - loss: 1099.7877 - val_loss: 1102.3124\n",
            "Epoch 1491/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.2774 - val_loss: 1103.7578\n",
            "Epoch 1492/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1101.3159 - val_loss: 1103.6074\n",
            "Epoch 1493/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.8399 - val_loss: 1103.4601\n",
            "Epoch 1494/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1100.2647 - val_loss: 1102.5000\n",
            "Epoch 1495/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.5234 - val_loss: 1103.4634\n",
            "Epoch 1496/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1101.0000 - val_loss: 1103.3112\n",
            "Epoch 1497/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.7184 - val_loss: 1104.6902\n",
            "Epoch 1498/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1100.1030 - val_loss: 1102.8630\n",
            "Epoch 1499/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.1279 - val_loss: 1102.1447\n",
            "Epoch 1500/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1100.8555 - val_loss: 1101.6852\n",
            "Epoch 1501/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1098.8656 - val_loss: 1102.4144\n",
            "Epoch 1502/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1101.0813 - val_loss: 1102.7000\n",
            "Epoch 1503/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.7492 - val_loss: 1102.8632\n",
            "Epoch 1504/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1098.9580 - val_loss: 1102.2428\n",
            "Epoch 1505/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.1423 - val_loss: 1100.9023\n",
            "Epoch 1506/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.4898 - val_loss: 1101.4342\n",
            "Epoch 1507/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1100.6660 - val_loss: 1101.9969\n",
            "Epoch 1508/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.5824 - val_loss: 1102.6608\n",
            "Epoch 1509/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1099.4363 - val_loss: 1101.5925\n",
            "Epoch 1510/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1099.3740 - val_loss: 1102.4080\n",
            "Epoch 1511/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.2766 - val_loss: 1103.2410\n",
            "Epoch 1512/2000\n",
            "157/157 [==============================] - 0s 351us/step - loss: 1100.2258 - val_loss: 1101.9507\n",
            "Epoch 1513/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.4846 - val_loss: 1102.3036\n",
            "Epoch 1514/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.7543 - val_loss: 1102.7999\n",
            "Epoch 1515/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1100.1409 - val_loss: 1101.2013\n",
            "Epoch 1516/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1100.5984 - val_loss: 1100.2428\n",
            "Epoch 1517/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1100.4520 - val_loss: 1100.4087\n",
            "Epoch 1518/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1099.7879 - val_loss: 1100.8893\n",
            "Epoch 1519/2000\n",
            "157/157 [==============================] - 0s 329us/step - loss: 1099.7710 - val_loss: 1102.7745\n",
            "Epoch 1520/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1101.8369 - val_loss: 1101.7733\n",
            "Epoch 1521/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1100.1580 - val_loss: 1103.6873\n",
            "Epoch 1522/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1100.9245 - val_loss: 1102.9944\n",
            "Epoch 1523/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1101.5475 - val_loss: 1102.2272\n",
            "Epoch 1524/2000\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1100.9301 - val_loss: 1101.1245\n",
            "Epoch 1525/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.1370 - val_loss: 1101.1404\n",
            "Epoch 1526/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.6935 - val_loss: 1100.9076\n",
            "Epoch 1527/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.5864 - val_loss: 1102.4403\n",
            "Epoch 1528/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1100.2481 - val_loss: 1103.9489\n",
            "Epoch 1529/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.6080 - val_loss: 1102.4232\n",
            "Epoch 1530/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1099.9914 - val_loss: 1103.3223\n",
            "Epoch 1531/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.5427 - val_loss: 1101.6558\n",
            "Epoch 1532/2000\n",
            "157/157 [==============================] - 0s 341us/step - loss: 1100.0949 - val_loss: 1101.5242\n",
            "Epoch 1533/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.3401 - val_loss: 1101.3389\n",
            "Epoch 1534/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1101.8908 - val_loss: 1100.6841\n",
            "Epoch 1535/2000\n",
            "157/157 [==============================] - 0s 312us/step - loss: 1100.7366 - val_loss: 1100.9382\n",
            "Epoch 1536/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.2485 - val_loss: 1100.2988\n",
            "Epoch 1537/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1100.6861 - val_loss: 1101.2014\n",
            "Epoch 1538/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.6424 - val_loss: 1100.4781\n",
            "Epoch 1539/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.4105 - val_loss: 1101.3301\n",
            "Epoch 1540/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1098.9030 - val_loss: 1100.2085\n",
            "Epoch 1541/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.9324 - val_loss: 1101.4474\n",
            "Epoch 1542/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.1199 - val_loss: 1102.2947\n",
            "Epoch 1543/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.7341 - val_loss: 1103.0010\n",
            "Epoch 1544/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.5918 - val_loss: 1101.6157\n",
            "Epoch 1545/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.3547 - val_loss: 1102.4298\n",
            "Epoch 1546/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1099.2218 - val_loss: 1102.0599\n",
            "Epoch 1547/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1101.7077 - val_loss: 1102.9714\n",
            "Epoch 1548/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.6117 - val_loss: 1102.8557\n",
            "Epoch 1549/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.6497 - val_loss: 1102.5726\n",
            "Epoch 1550/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1100.7241 - val_loss: 1103.0128\n",
            "Epoch 1551/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1099.2492 - val_loss: 1103.0559\n",
            "Epoch 1552/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.2348 - val_loss: 1102.9136\n",
            "Epoch 1553/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1100.2512 - val_loss: 1104.0033\n",
            "Epoch 1554/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1099.5568 - val_loss: 1102.6144\n",
            "Epoch 1555/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.9435 - val_loss: 1102.3030\n",
            "Epoch 1556/2000\n",
            "157/157 [==============================] - 0s 352us/step - loss: 1099.1201 - val_loss: 1102.4880\n",
            "Epoch 1557/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1099.1219 - val_loss: 1103.0289\n",
            "Epoch 1558/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1101.1234 - val_loss: 1103.0470\n",
            "Epoch 1559/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.1090 - val_loss: 1102.3751\n",
            "Epoch 1560/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.2686 - val_loss: 1101.4948\n",
            "Epoch 1561/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1099.7829 - val_loss: 1101.3488\n",
            "Epoch 1562/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.9005 - val_loss: 1100.2056\n",
            "Epoch 1563/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.0526 - val_loss: 1101.2316\n",
            "Epoch 1564/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1098.9587 - val_loss: 1103.7516\n",
            "Epoch 1565/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1098.6626 - val_loss: 1103.0272\n",
            "Epoch 1566/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.8772 - val_loss: 1101.3876\n",
            "Epoch 1567/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.5693 - val_loss: 1102.2720\n",
            "Epoch 1568/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1099.4659 - val_loss: 1100.8080\n",
            "Epoch 1569/2000\n",
            "157/157 [==============================] - 0s 323us/step - loss: 1100.3321 - val_loss: 1101.8772\n",
            "Epoch 1570/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.6812 - val_loss: 1101.1511\n",
            "Epoch 1571/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1101.2258 - val_loss: 1101.9504\n",
            "Epoch 1572/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.5247 - val_loss: 1100.5020\n",
            "Epoch 1573/2000\n",
            "157/157 [==============================] - 0s 335us/step - loss: 1099.9814 - val_loss: 1101.5099\n",
            "Epoch 1574/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.6068 - val_loss: 1100.1378\n",
            "Epoch 1575/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.4899 - val_loss: 1101.4009\n",
            "Epoch 1576/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.9485 - val_loss: 1101.0576\n",
            "Epoch 1577/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.0929 - val_loss: 1102.8895\n",
            "Epoch 1578/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1098.9244 - val_loss: 1102.4426\n",
            "Epoch 1579/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1098.8376 - val_loss: 1102.4409\n",
            "Epoch 1580/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1098.9955 - val_loss: 1102.8417\n",
            "Epoch 1581/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.6913 - val_loss: 1102.1163\n",
            "Epoch 1582/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1099.3122 - val_loss: 1101.7643\n",
            "Epoch 1583/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1100.2923 - val_loss: 1102.0601\n",
            "Epoch 1584/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.5240 - val_loss: 1101.0876\n",
            "Epoch 1585/2000\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1100.9637 - val_loss: 1099.7003\n",
            "Epoch 1586/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1098.7154 - val_loss: 1100.1691\n",
            "Epoch 1587/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.7387 - val_loss: 1101.1626\n",
            "Epoch 1588/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1098.7461 - val_loss: 1100.7151\n",
            "Epoch 1589/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.7232 - val_loss: 1101.4349\n",
            "Epoch 1590/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.8743 - val_loss: 1101.8149\n",
            "Epoch 1591/2000\n",
            "157/157 [==============================] - 0s 296us/step - loss: 1099.0995 - val_loss: 1100.1172\n",
            "Epoch 1592/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1098.9416 - val_loss: 1099.9066\n",
            "Epoch 1593/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1099.0539 - val_loss: 1102.4478\n",
            "Epoch 1594/2000\n",
            "157/157 [==============================] - 0s 381us/step - loss: 1099.4348 - val_loss: 1100.4763\n",
            "Epoch 1595/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.2480 - val_loss: 1100.5819\n",
            "Epoch 1596/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1098.6019 - val_loss: 1101.0808\n",
            "Epoch 1597/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.1921 - val_loss: 1102.8651\n",
            "Epoch 1598/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.7352 - val_loss: 1103.0497\n",
            "Epoch 1599/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1099.7513 - val_loss: 1102.3440\n",
            "Epoch 1600/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.6516 - val_loss: 1103.1415\n",
            "Epoch 1601/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.9569 - val_loss: 1104.1660\n",
            "Epoch 1602/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.4459 - val_loss: 1103.3490\n",
            "Epoch 1603/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1100.0427 - val_loss: 1102.2695\n",
            "Epoch 1604/2000\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1100.7865 - val_loss: 1103.2163\n",
            "Epoch 1605/2000\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1099.5136 - val_loss: 1101.0143\n",
            "Epoch 1606/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1099.2759 - val_loss: 1101.8032\n",
            "Epoch 1607/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.6018 - val_loss: 1100.5942\n",
            "Epoch 1608/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1098.9238 - val_loss: 1101.0974\n",
            "Epoch 1609/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.9479 - val_loss: 1100.5522\n",
            "Epoch 1610/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.1494 - val_loss: 1104.2385\n",
            "Epoch 1611/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1098.8362 - val_loss: 1103.5728\n",
            "Epoch 1612/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.5071 - val_loss: 1103.2932\n",
            "Epoch 1613/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.8616 - val_loss: 1102.4152\n",
            "Epoch 1614/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1101.6705 - val_loss: 1103.4137\n",
            "Epoch 1615/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1100.3257 - val_loss: 1102.8176\n",
            "Epoch 1616/2000\n",
            "157/157 [==============================] - 0s 308us/step - loss: 1099.6111 - val_loss: 1103.7971\n",
            "Epoch 1617/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.4858 - val_loss: 1101.9592\n",
            "Epoch 1618/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.2060 - val_loss: 1103.2235\n",
            "Epoch 1619/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1098.6260 - val_loss: 1105.4412\n",
            "Epoch 1620/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1099.3423 - val_loss: 1104.2693\n",
            "Epoch 1621/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1100.0169 - val_loss: 1103.8954\n",
            "Epoch 1622/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1099.1844 - val_loss: 1103.3475\n",
            "Epoch 1623/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.9682 - val_loss: 1104.5944\n",
            "Epoch 1624/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1099.8245 - val_loss: 1106.3470\n",
            "Epoch 1625/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1101.4000 - val_loss: 1103.9258\n",
            "Epoch 1626/2000\n",
            "157/157 [==============================] - 0s 327us/step - loss: 1099.9833 - val_loss: 1104.5396\n",
            "Epoch 1627/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.5208 - val_loss: 1104.0995\n",
            "Epoch 1628/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.5528 - val_loss: 1103.7157\n",
            "Epoch 1629/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1099.4162 - val_loss: 1103.1082\n",
            "Epoch 1630/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1099.9208 - val_loss: 1104.9312\n",
            "Epoch 1631/2000\n",
            "157/157 [==============================] - 0s 316us/step - loss: 1099.5777 - val_loss: 1103.7581\n",
            "Epoch 1632/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.3280 - val_loss: 1102.8955\n",
            "Epoch 1633/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.3213 - val_loss: 1102.5144\n",
            "Epoch 1634/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1099.5125 - val_loss: 1103.5608\n",
            "Epoch 1635/2000\n",
            "157/157 [==============================] - 0s 357us/step - loss: 1099.5800 - val_loss: 1104.2028\n",
            "Epoch 1636/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.0666 - val_loss: 1102.4689\n",
            "Epoch 1637/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.8678 - val_loss: 1102.3145\n",
            "Epoch 1638/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1098.8618 - val_loss: 1102.1163\n",
            "Epoch 1639/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.9916 - val_loss: 1101.4861\n",
            "Epoch 1640/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.4211 - val_loss: 1102.8641\n",
            "Epoch 1641/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.9461 - val_loss: 1102.3518\n",
            "Epoch 1642/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.2514 - val_loss: 1102.2274\n",
            "Epoch 1643/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.6922 - val_loss: 1101.2593\n",
            "Epoch 1644/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.1514 - val_loss: 1100.8770\n",
            "Epoch 1645/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1100.0053 - val_loss: 1100.7880\n",
            "Epoch 1646/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.9633 - val_loss: 1102.2013\n",
            "Epoch 1647/2000\n",
            "157/157 [==============================] - 0s 317us/step - loss: 1098.6610 - val_loss: 1101.9885\n",
            "Epoch 1648/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.0955 - val_loss: 1101.5488\n",
            "Epoch 1649/2000\n",
            "157/157 [==============================] - 0s 332us/step - loss: 1099.4931 - val_loss: 1101.2905\n",
            "Epoch 1650/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1098.6547 - val_loss: 1101.2139\n",
            "Epoch 1651/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.4460 - val_loss: 1101.9258\n",
            "Epoch 1652/2000\n",
            "157/157 [==============================] - 0s 340us/step - loss: 1099.4912 - val_loss: 1100.5555\n",
            "Epoch 1653/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1100.9682 - val_loss: 1101.6486\n",
            "Epoch 1654/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.6044 - val_loss: 1102.3134\n",
            "Epoch 1655/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1098.9490 - val_loss: 1100.2363\n",
            "Epoch 1656/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.2198 - val_loss: 1101.4088\n",
            "Epoch 1657/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1098.6986 - val_loss: 1101.8173\n",
            "Epoch 1658/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1098.0620 - val_loss: 1101.2415\n",
            "Epoch 1659/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.7661 - val_loss: 1101.2224\n",
            "Epoch 1660/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1098.7606 - val_loss: 1100.5115\n",
            "Epoch 1661/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1099.2754 - val_loss: 1102.4368\n",
            "Epoch 1662/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1098.8824 - val_loss: 1102.8092\n",
            "Epoch 1663/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.1596 - val_loss: 1102.4597\n",
            "Epoch 1664/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1099.0283 - val_loss: 1102.6216\n",
            "Epoch 1665/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1099.0327 - val_loss: 1103.0436\n",
            "Epoch 1666/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1101.0468 - val_loss: 1103.0135\n",
            "Epoch 1667/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.9774 - val_loss: 1103.0042\n",
            "Epoch 1668/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1098.7143 - val_loss: 1100.8438\n",
            "Epoch 1669/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1100.6279 - val_loss: 1100.5660\n",
            "Epoch 1670/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1098.6456 - val_loss: 1102.6339\n",
            "Epoch 1671/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1100.2191 - val_loss: 1103.4519\n",
            "Epoch 1672/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1099.0476 - val_loss: 1103.8751\n",
            "Epoch 1673/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1101.1755 - val_loss: 1103.1625\n",
            "Epoch 1674/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1098.8744 - val_loss: 1102.8544\n",
            "Epoch 1675/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1100.0512 - val_loss: 1103.2657\n",
            "Epoch 1676/2000\n",
            "157/157 [==============================] - 0s 383us/step - loss: 1100.3470 - val_loss: 1100.6049\n",
            "Epoch 1677/2000\n",
            "157/157 [==============================] - 0s 310us/step - loss: 1099.7947 - val_loss: 1100.3678\n",
            "Epoch 1678/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.8983 - val_loss: 1099.1456\n",
            "Epoch 1679/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.9440 - val_loss: 1100.0945\n",
            "Epoch 1680/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1100.9147 - val_loss: 1101.5942\n",
            "Epoch 1681/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1099.8061 - val_loss: 1102.3301\n",
            "Epoch 1682/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.5984 - val_loss: 1102.6196\n",
            "Epoch 1683/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.9269 - val_loss: 1103.0366\n",
            "Epoch 1684/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1099.4006 - val_loss: 1103.8225\n",
            "Epoch 1685/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.1635 - val_loss: 1100.8959\n",
            "Epoch 1686/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1100.2409 - val_loss: 1102.1818\n",
            "Epoch 1687/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.1060 - val_loss: 1100.9034\n",
            "Epoch 1688/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1098.8929 - val_loss: 1099.7196\n",
            "Epoch 1689/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.8412 - val_loss: 1099.9885\n",
            "Epoch 1690/2000\n",
            "157/157 [==============================] - 0s 351us/step - loss: 1098.9544 - val_loss: 1101.3624\n",
            "Epoch 1691/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1100.1958 - val_loss: 1101.1083\n",
            "Epoch 1692/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.8699 - val_loss: 1101.5656\n",
            "Epoch 1693/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1098.9894 - val_loss: 1103.4935\n",
            "Epoch 1694/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1099.3515 - val_loss: 1103.3942\n",
            "Epoch 1695/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1100.4889 - val_loss: 1103.4573\n",
            "Epoch 1696/2000\n",
            "157/157 [==============================] - 0s 301us/step - loss: 1100.1936 - val_loss: 1105.5912\n",
            "Epoch 1697/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.8165 - val_loss: 1102.8499\n",
            "Epoch 1698/2000\n",
            "157/157 [==============================] - 0s 338us/step - loss: 1101.0504 - val_loss: 1102.4343\n",
            "Epoch 1699/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1099.2172 - val_loss: 1101.8579\n",
            "Epoch 1700/2000\n",
            "157/157 [==============================] - 0s 360us/step - loss: 1099.3931 - val_loss: 1101.2494\n",
            "Epoch 1701/2000\n",
            "157/157 [==============================] - 0s 350us/step - loss: 1100.2074 - val_loss: 1102.1782\n",
            "Epoch 1702/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1099.7657 - val_loss: 1101.0864\n",
            "Epoch 1703/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.0052 - val_loss: 1101.7881\n",
            "Epoch 1704/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.3020 - val_loss: 1102.9172\n",
            "Epoch 1705/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.4582 - val_loss: 1104.0356\n",
            "Epoch 1706/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.8395 - val_loss: 1104.7784\n",
            "Epoch 1707/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.2994 - val_loss: 1104.4080\n",
            "Epoch 1708/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.7205 - val_loss: 1102.2313\n",
            "Epoch 1709/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1098.8585 - val_loss: 1101.9753\n",
            "Epoch 1710/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1098.7600 - val_loss: 1100.3950\n",
            "Epoch 1711/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1098.6238 - val_loss: 1102.7738\n",
            "Epoch 1712/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.1272 - val_loss: 1102.1995\n",
            "Epoch 1713/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.1221 - val_loss: 1104.0492\n",
            "Epoch 1714/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.3471 - val_loss: 1103.6776\n",
            "Epoch 1715/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.5989 - val_loss: 1103.1675\n",
            "Epoch 1716/2000\n",
            "157/157 [==============================] - 0s 442us/step - loss: 1100.3466 - val_loss: 1101.7103\n",
            "Epoch 1717/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1100.0196 - val_loss: 1102.9749\n",
            "Epoch 1718/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1098.6541 - val_loss: 1102.6865\n",
            "Epoch 1719/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1100.3653 - val_loss: 1103.7427\n",
            "Epoch 1720/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.2399 - val_loss: 1102.3256\n",
            "Epoch 1721/2000\n",
            "157/157 [==============================] - 0s 295us/step - loss: 1100.7504 - val_loss: 1104.3004\n",
            "Epoch 1722/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.8595 - val_loss: 1102.1786\n",
            "Epoch 1723/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.4843 - val_loss: 1102.7610\n",
            "Epoch 1724/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.5834 - val_loss: 1102.9137\n",
            "Epoch 1725/2000\n",
            "157/157 [==============================] - 0s 351us/step - loss: 1098.9551 - val_loss: 1103.5664\n",
            "Epoch 1726/2000\n",
            "157/157 [==============================] - 0s 307us/step - loss: 1099.1070 - val_loss: 1102.7605\n",
            "Epoch 1727/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.0661 - val_loss: 1103.6323\n",
            "Epoch 1728/2000\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1099.3488 - val_loss: 1102.8229\n",
            "Epoch 1729/2000\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1099.5580 - val_loss: 1102.4430\n",
            "Epoch 1730/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1100.7666 - val_loss: 1102.8643\n",
            "Epoch 1731/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.1784 - val_loss: 1102.8540\n",
            "Epoch 1732/2000\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1100.0202 - val_loss: 1103.4243\n",
            "Epoch 1733/2000\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1100.2214 - val_loss: 1101.7446\n",
            "Epoch 1734/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1098.3543 - val_loss: 1102.1406\n",
            "Epoch 1735/2000\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1098.6772 - val_loss: 1101.2539\n",
            "Epoch 1736/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.1658 - val_loss: 1102.7496\n",
            "Epoch 1737/2000\n",
            "157/157 [==============================] - 0s 361us/step - loss: 1100.2400 - val_loss: 1102.8077\n",
            "Epoch 1738/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1098.4092 - val_loss: 1101.7793\n",
            "Epoch 1739/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1098.6986 - val_loss: 1102.5963\n",
            "Epoch 1740/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1100.2953 - val_loss: 1102.4313\n",
            "Epoch 1741/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.2561 - val_loss: 1101.9763\n",
            "Epoch 1742/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1100.1284 - val_loss: 1103.2118\n",
            "Epoch 1743/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.5285 - val_loss: 1102.4675\n",
            "Epoch 1744/2000\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1099.0170 - val_loss: 1100.5071\n",
            "Epoch 1745/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1100.4333 - val_loss: 1101.2885\n",
            "Epoch 1746/2000\n",
            "157/157 [==============================] - 0s 348us/step - loss: 1100.7298 - val_loss: 1101.1606\n",
            "Epoch 1747/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.3377 - val_loss: 1102.3143\n",
            "Epoch 1748/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1100.5434 - val_loss: 1101.1018\n",
            "Epoch 1749/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1100.6683 - val_loss: 1100.0986\n",
            "Epoch 1750/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1101.9605 - val_loss: 1100.1515\n",
            "Epoch 1751/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.9127 - val_loss: 1101.2271\n",
            "Epoch 1752/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1100.3998 - val_loss: 1101.7117\n",
            "Epoch 1753/2000\n",
            "157/157 [==============================] - 0s 317us/step - loss: 1099.4581 - val_loss: 1102.4120\n",
            "Epoch 1754/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.1055 - val_loss: 1101.7729\n",
            "Epoch 1755/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1100.1663 - val_loss: 1102.8727\n",
            "Epoch 1756/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1098.8620 - val_loss: 1101.0453\n",
            "Epoch 1757/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1098.7468 - val_loss: 1101.1951\n",
            "Epoch 1758/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1099.7872 - val_loss: 1101.2104\n",
            "Epoch 1759/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1098.8487 - val_loss: 1101.1259\n",
            "Epoch 1760/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1098.5996 - val_loss: 1101.0632\n",
            "Epoch 1761/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1098.9009 - val_loss: 1100.0107\n",
            "Epoch 1762/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.9789 - val_loss: 1100.7402\n",
            "Epoch 1763/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.4233 - val_loss: 1101.3094\n",
            "Epoch 1764/2000\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1099.9587 - val_loss: 1101.0657\n",
            "Epoch 1765/2000\n",
            "157/157 [==============================] - 0s 297us/step - loss: 1099.2876 - val_loss: 1101.8514\n",
            "Epoch 1766/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1099.4532 - val_loss: 1103.1945\n",
            "Epoch 1767/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.0903 - val_loss: 1103.3624\n",
            "Epoch 1768/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1099.1079 - val_loss: 1103.9657\n",
            "Epoch 1769/2000\n",
            "157/157 [==============================] - 0s 376us/step - loss: 1099.6571 - val_loss: 1103.7272\n",
            "Epoch 1770/2000\n",
            "157/157 [==============================] - 0s 302us/step - loss: 1099.1343 - val_loss: 1103.3992\n",
            "Epoch 1771/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.7908 - val_loss: 1102.5050\n",
            "Epoch 1772/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1099.5902 - val_loss: 1101.5647\n",
            "Epoch 1773/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1099.5969 - val_loss: 1101.8944\n",
            "Epoch 1774/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.6981 - val_loss: 1101.7260\n",
            "Epoch 1775/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.1530 - val_loss: 1101.7699\n",
            "Epoch 1776/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.9329 - val_loss: 1101.4252\n",
            "Epoch 1777/2000\n",
            "157/157 [==============================] - 0s 332us/step - loss: 1100.6342 - val_loss: 1101.2969\n",
            "Epoch 1778/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1099.0216 - val_loss: 1101.2490\n",
            "Epoch 1779/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.0221 - val_loss: 1101.9243\n",
            "Epoch 1780/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.4592 - val_loss: 1103.2867\n",
            "Epoch 1781/2000\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1099.4478 - val_loss: 1100.3414\n",
            "Epoch 1782/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.0106 - val_loss: 1101.0458\n",
            "Epoch 1783/2000\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1098.8518 - val_loss: 1101.8538\n",
            "Epoch 1784/2000\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1099.5640 - val_loss: 1100.2181\n",
            "Epoch 1785/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1099.3074 - val_loss: 1100.4292\n",
            "Epoch 1786/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1099.1602 - val_loss: 1101.4006\n",
            "Epoch 1787/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1098.5267 - val_loss: 1102.8029\n",
            "Epoch 1788/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1099.3947 - val_loss: 1103.4363\n",
            "Epoch 1789/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1098.7985 - val_loss: 1103.5829\n",
            "Epoch 1790/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.8170 - val_loss: 1103.4946\n",
            "Epoch 1791/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.6949 - val_loss: 1104.0790\n",
            "Epoch 1792/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.2157 - val_loss: 1101.4448\n",
            "Epoch 1793/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1099.3657 - val_loss: 1102.4637\n",
            "Epoch 1794/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.0029 - val_loss: 1101.5559\n",
            "Epoch 1795/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1098.6079 - val_loss: 1101.3160\n",
            "Epoch 1796/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.7640 - val_loss: 1101.1290\n",
            "Epoch 1797/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1098.3254 - val_loss: 1101.5431\n",
            "Epoch 1798/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1098.9589 - val_loss: 1101.0684\n",
            "Epoch 1799/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.1124 - val_loss: 1101.9344\n",
            "Epoch 1800/2000\n",
            "157/157 [==============================] - 0s 326us/step - loss: 1099.0050 - val_loss: 1102.1503\n",
            "Epoch 1801/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1099.8530 - val_loss: 1101.7711\n",
            "Epoch 1802/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.1064 - val_loss: 1101.0037\n",
            "Epoch 1803/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.3541 - val_loss: 1101.4076\n",
            "Epoch 1804/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.6728 - val_loss: 1100.6460\n",
            "Epoch 1805/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.3914 - val_loss: 1100.9907\n",
            "Epoch 1806/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.0111 - val_loss: 1100.3538\n",
            "Epoch 1807/2000\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1099.9016 - val_loss: 1101.3236\n",
            "Epoch 1808/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.9680 - val_loss: 1100.8416\n",
            "Epoch 1809/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.3904 - val_loss: 1101.0864\n",
            "Epoch 1810/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.4187 - val_loss: 1101.7638\n",
            "Epoch 1811/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.0619 - val_loss: 1101.5698\n",
            "Epoch 1812/2000\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1099.5822 - val_loss: 1099.9647\n",
            "Epoch 1813/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1101.2194 - val_loss: 1100.0454\n",
            "Epoch 1814/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1101.2367 - val_loss: 1100.7163\n",
            "Epoch 1815/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1098.7616 - val_loss: 1100.0546\n",
            "Epoch 1816/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.5444 - val_loss: 1102.1757\n",
            "Epoch 1817/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.9408 - val_loss: 1103.7181\n",
            "Epoch 1818/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1100.9634 - val_loss: 1103.3723\n",
            "Epoch 1819/2000\n",
            "157/157 [==============================] - 0s 299us/step - loss: 1100.0873 - val_loss: 1103.0503\n",
            "Epoch 1820/2000\n",
            "157/157 [==============================] - 0s 317us/step - loss: 1099.3299 - val_loss: 1102.4158\n",
            "Epoch 1821/2000\n",
            "157/157 [==============================] - 0s 408us/step - loss: 1100.5884 - val_loss: 1102.1284\n",
            "Epoch 1822/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.2146 - val_loss: 1101.5867\n",
            "Epoch 1823/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1100.7007 - val_loss: 1101.9888\n",
            "Epoch 1824/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.6684 - val_loss: 1100.4041\n",
            "Epoch 1825/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.1763 - val_loss: 1102.0571\n",
            "Epoch 1826/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1100.3753 - val_loss: 1100.8640\n",
            "Epoch 1827/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1100.9849 - val_loss: 1101.2791\n",
            "Epoch 1828/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.7509 - val_loss: 1101.4863\n",
            "Epoch 1829/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1098.9864 - val_loss: 1100.1028\n",
            "Epoch 1830/2000\n",
            "157/157 [==============================] - 0s 320us/step - loss: 1097.8884 - val_loss: 1101.1313\n",
            "Epoch 1831/2000\n",
            "157/157 [==============================] - 0s 317us/step - loss: 1099.2346 - val_loss: 1102.0002\n",
            "Epoch 1832/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1099.4178 - val_loss: 1102.1498\n",
            "Epoch 1833/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1100.2771 - val_loss: 1101.0442\n",
            "Epoch 1834/2000\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1098.9262 - val_loss: 1100.4695\n",
            "Epoch 1835/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1098.8808 - val_loss: 1101.3853\n",
            "Epoch 1836/2000\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1099.1316 - val_loss: 1102.5035\n",
            "Epoch 1837/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1100.7175 - val_loss: 1102.7148\n",
            "Epoch 1838/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1100.2001 - val_loss: 1102.4534\n",
            "Epoch 1839/2000\n",
            "157/157 [==============================] - 0s 336us/step - loss: 1100.7108 - val_loss: 1103.2793\n",
            "Epoch 1840/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1100.7658 - val_loss: 1101.9147\n",
            "Epoch 1841/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.4589 - val_loss: 1102.3579\n",
            "Epoch 1842/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.8737 - val_loss: 1102.1206\n",
            "Epoch 1843/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1098.7637 - val_loss: 1103.3607\n",
            "Epoch 1844/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.0373 - val_loss: 1103.7988\n",
            "Epoch 1845/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.5404 - val_loss: 1103.4877\n",
            "Epoch 1846/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1098.9670 - val_loss: 1102.2351\n",
            "Epoch 1847/2000\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1099.4185 - val_loss: 1102.3813\n",
            "Epoch 1848/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1098.7488 - val_loss: 1102.0638\n",
            "Epoch 1849/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1099.5757 - val_loss: 1103.9327\n",
            "Epoch 1850/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.2993 - val_loss: 1102.3662\n",
            "Epoch 1851/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.5396 - val_loss: 1100.7776\n",
            "Epoch 1852/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1098.3228 - val_loss: 1101.7804\n",
            "Epoch 1853/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.6530 - val_loss: 1102.5577\n",
            "Epoch 1854/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1100.6392 - val_loss: 1099.9955\n",
            "Epoch 1855/2000\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1099.3785 - val_loss: 1101.3385\n",
            "Epoch 1856/2000\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1099.6376 - val_loss: 1100.8290\n",
            "Epoch 1857/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.8724 - val_loss: 1100.6743\n",
            "Epoch 1858/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1100.0589 - val_loss: 1102.5629\n",
            "Epoch 1859/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1100.8217 - val_loss: 1102.3446\n",
            "Epoch 1860/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1099.1027 - val_loss: 1101.2152\n",
            "Epoch 1861/2000\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1099.1383 - val_loss: 1099.8260\n",
            "Epoch 1862/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.2176 - val_loss: 1102.4963\n",
            "Epoch 1863/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.4671 - val_loss: 1100.6630\n",
            "Epoch 1864/2000\n",
            "157/157 [==============================] - 0s 386us/step - loss: 1099.0948 - val_loss: 1101.7776\n",
            "Epoch 1865/2000\n",
            "157/157 [==============================] - 0s 285us/step - loss: 1099.2011 - val_loss: 1101.1975\n",
            "Epoch 1866/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.1658 - val_loss: 1104.3420\n",
            "Epoch 1867/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1098.7810 - val_loss: 1103.7698\n",
            "Epoch 1868/2000\n",
            "157/157 [==============================] - 0s 290us/step - loss: 1098.7178 - val_loss: 1101.5687\n",
            "Epoch 1869/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1098.7043 - val_loss: 1102.9036\n",
            "Epoch 1870/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.8374 - val_loss: 1101.1453\n",
            "Epoch 1871/2000\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1098.2006 - val_loss: 1100.6611\n",
            "Epoch 1872/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1100.0206 - val_loss: 1100.4459\n",
            "Epoch 1873/2000\n",
            "157/157 [==============================] - 0s 359us/step - loss: 1100.7439 - val_loss: 1100.1560\n",
            "Epoch 1874/2000\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1099.8194 - val_loss: 1101.7300\n",
            "Epoch 1875/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1098.7161 - val_loss: 1101.8595\n",
            "Epoch 1876/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.1605 - val_loss: 1101.4152\n",
            "Epoch 1877/2000\n",
            "157/157 [==============================] - 0s 311us/step - loss: 1098.7297 - val_loss: 1103.4390\n",
            "Epoch 1878/2000\n",
            "157/157 [==============================] - 0s 345us/step - loss: 1098.5878 - val_loss: 1102.1987\n",
            "Epoch 1879/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1098.8307 - val_loss: 1101.5643\n",
            "Epoch 1880/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.6996 - val_loss: 1101.4336\n",
            "Epoch 1881/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1099.0968 - val_loss: 1101.1090\n",
            "Epoch 1882/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1100.5439 - val_loss: 1101.3164\n",
            "Epoch 1883/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1100.2874 - val_loss: 1101.6794\n",
            "Epoch 1884/2000\n",
            "157/157 [==============================] - 0s 356us/step - loss: 1099.1257 - val_loss: 1102.0237\n",
            "Epoch 1885/2000\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1099.0452 - val_loss: 1102.4528\n",
            "Epoch 1886/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.3961 - val_loss: 1102.6980\n",
            "Epoch 1887/2000\n",
            "157/157 [==============================] - 0s 409us/step - loss: 1100.2858 - val_loss: 1102.8240\n",
            "Epoch 1888/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.6199 - val_loss: 1102.1914\n",
            "Epoch 1889/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1099.6223 - val_loss: 1102.7938\n",
            "Epoch 1890/2000\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1099.6940 - val_loss: 1101.6252\n",
            "Epoch 1891/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.6880 - val_loss: 1101.5586\n",
            "Epoch 1892/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.5504 - val_loss: 1100.1646\n",
            "Epoch 1893/2000\n",
            "157/157 [==============================] - 0s 316us/step - loss: 1099.3435 - val_loss: 1101.6450\n",
            "Epoch 1894/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.2479 - val_loss: 1099.9980\n",
            "Epoch 1895/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1098.9716 - val_loss: 1099.7979\n",
            "Epoch 1896/2000\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1099.0090 - val_loss: 1099.3333\n",
            "Epoch 1897/2000\n",
            "157/157 [==============================] - 0s 330us/step - loss: 1099.7027 - val_loss: 1101.1191\n",
            "Epoch 1898/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.8211 - val_loss: 1100.8444\n",
            "Epoch 1899/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.2276 - val_loss: 1100.4009\n",
            "Epoch 1900/2000\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1099.1848 - val_loss: 1100.9370\n",
            "Epoch 1901/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.5521 - val_loss: 1099.5873\n",
            "Epoch 1902/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1098.5939 - val_loss: 1100.4774\n",
            "Epoch 1903/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1098.9429 - val_loss: 1100.2129\n",
            "Epoch 1904/2000\n",
            "157/157 [==============================] - 0s 359us/step - loss: 1099.2129 - val_loss: 1100.2097\n",
            "Epoch 1905/2000\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1099.2788 - val_loss: 1101.5427\n",
            "Epoch 1906/2000\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1099.8180 - val_loss: 1102.0870\n",
            "Epoch 1907/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.1900 - val_loss: 1101.0205\n",
            "Epoch 1908/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1098.5969 - val_loss: 1102.2218\n",
            "Epoch 1909/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.4198 - val_loss: 1100.4877\n",
            "Epoch 1910/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1100.0879 - val_loss: 1102.7003\n",
            "Epoch 1911/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1100.5083 - val_loss: 1103.5122\n",
            "Epoch 1912/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.2874 - val_loss: 1102.4906\n",
            "Epoch 1913/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1100.0376 - val_loss: 1101.3585\n",
            "Epoch 1914/2000\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1099.8955 - val_loss: 1101.4171\n",
            "Epoch 1915/2000\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1099.4808 - val_loss: 1100.3325\n",
            "Epoch 1916/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1098.8898 - val_loss: 1101.1979\n",
            "Epoch 1917/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1099.0646 - val_loss: 1100.9380\n",
            "Epoch 1918/2000\n",
            "157/157 [==============================] - 0s 360us/step - loss: 1100.0229 - val_loss: 1100.0435\n",
            "Epoch 1919/2000\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1099.3381 - val_loss: 1100.4027\n",
            "Epoch 1920/2000\n",
            "157/157 [==============================] - 0s 309us/step - loss: 1099.3177 - val_loss: 1101.9139\n",
            "Epoch 1921/2000\n",
            "157/157 [==============================] - 0s 332us/step - loss: 1098.5310 - val_loss: 1100.6200\n",
            "Epoch 1922/2000\n",
            "157/157 [==============================] - 0s 334us/step - loss: 1099.8657 - val_loss: 1101.1151\n",
            "Epoch 1923/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1099.2940 - val_loss: 1101.9419\n",
            "Epoch 1924/2000\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1099.5742 - val_loss: 1102.3231\n",
            "Epoch 1925/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.8806 - val_loss: 1103.0787\n",
            "Epoch 1926/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.9201 - val_loss: 1103.4993\n",
            "Epoch 1927/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1098.9605 - val_loss: 1102.0126\n",
            "Epoch 1928/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.8482 - val_loss: 1102.0704\n",
            "Epoch 1929/2000\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1099.0401 - val_loss: 1100.9633\n",
            "Epoch 1930/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.7529 - val_loss: 1100.6604\n",
            "Epoch 1931/2000\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1099.4993 - val_loss: 1101.7444\n",
            "Epoch 1932/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.5271 - val_loss: 1102.3926\n",
            "Epoch 1933/2000\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1098.9942 - val_loss: 1100.4067\n",
            "Epoch 1934/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.1686 - val_loss: 1101.1799\n",
            "Epoch 1935/2000\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1101.0166 - val_loss: 1101.2313\n",
            "Epoch 1936/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.5658 - val_loss: 1101.3444\n",
            "Epoch 1937/2000\n",
            "157/157 [==============================] - 0s 315us/step - loss: 1098.9419 - val_loss: 1102.1173\n",
            "Epoch 1938/2000\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1099.3780 - val_loss: 1101.1733\n",
            "Epoch 1939/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1099.9802 - val_loss: 1102.9453\n",
            "Epoch 1940/2000\n",
            "157/157 [==============================] - 0s 337us/step - loss: 1099.6050 - val_loss: 1100.1251\n",
            "Epoch 1941/2000\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1099.2231 - val_loss: 1101.1493\n",
            "Epoch 1942/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1099.6665 - val_loss: 1101.4021\n",
            "Epoch 1943/2000\n",
            "157/157 [==============================] - 0s 292us/step - loss: 1099.1648 - val_loss: 1101.5498\n",
            "Epoch 1944/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.4162 - val_loss: 1100.8169\n",
            "Epoch 1945/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1100.8463 - val_loss: 1101.8585\n",
            "Epoch 1946/2000\n",
            "157/157 [==============================] - 0s 357us/step - loss: 1100.4873 - val_loss: 1100.0044\n",
            "Epoch 1947/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.5046 - val_loss: 1101.5469\n",
            "Epoch 1948/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.7094 - val_loss: 1101.2725\n",
            "Epoch 1949/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1098.4890 - val_loss: 1100.0846\n",
            "Epoch 1950/2000\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1098.6062 - val_loss: 1099.4624\n",
            "Epoch 1951/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.0467 - val_loss: 1099.3837\n",
            "Epoch 1952/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.1844 - val_loss: 1101.6522\n",
            "Epoch 1953/2000\n",
            "157/157 [==============================] - 0s 284us/step - loss: 1098.3223 - val_loss: 1101.2761\n",
            "Epoch 1954/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.0755 - val_loss: 1103.0159\n",
            "Epoch 1955/2000\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1099.4777 - val_loss: 1103.1200\n",
            "Epoch 1956/2000\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1100.2228 - val_loss: 1103.1641\n",
            "Epoch 1957/2000\n",
            "157/157 [==============================] - 0s 259us/step - loss: 1099.8316 - val_loss: 1102.2952\n",
            "Epoch 1958/2000\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1099.0705 - val_loss: 1100.6169\n",
            "Epoch 1959/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1098.9255 - val_loss: 1101.3788\n",
            "Epoch 1960/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.6099 - val_loss: 1100.7552\n",
            "Epoch 1961/2000\n",
            "157/157 [==============================] - 0s 274us/step - loss: 1099.2361 - val_loss: 1102.2213\n",
            "Epoch 1962/2000\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1099.6149 - val_loss: 1104.3967\n",
            "Epoch 1963/2000\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1098.9556 - val_loss: 1103.1016\n",
            "Epoch 1964/2000\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1098.4966 - val_loss: 1103.9491\n",
            "Epoch 1965/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1098.4833 - val_loss: 1104.3353\n",
            "Epoch 1966/2000\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1098.3920 - val_loss: 1102.6562\n",
            "Epoch 1967/2000\n",
            "157/157 [==============================] - 0s 325us/step - loss: 1099.3525 - val_loss: 1102.5347\n",
            "Epoch 1968/2000\n",
            "157/157 [==============================] - 0s 464us/step - loss: 1098.6155 - val_loss: 1102.0811\n",
            "Epoch 1969/2000\n",
            "157/157 [==============================] - 0s 350us/step - loss: 1099.1426 - val_loss: 1101.0132\n",
            "Epoch 1970/2000\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1098.9483 - val_loss: 1101.5371\n",
            "Epoch 1971/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1099.6135 - val_loss: 1100.9683\n",
            "Epoch 1972/2000\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1099.5609 - val_loss: 1100.2701\n",
            "Epoch 1973/2000\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1098.8278 - val_loss: 1102.3960\n",
            "Epoch 1974/2000\n",
            "157/157 [==============================] - 0s 305us/step - loss: 1100.2206 - val_loss: 1101.9454\n",
            "Epoch 1975/2000\n",
            "157/157 [==============================] - 0s 294us/step - loss: 1099.5706 - val_loss: 1102.9236\n",
            "Epoch 1976/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1098.9746 - val_loss: 1103.4786\n",
            "Epoch 1977/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.2441 - val_loss: 1104.0792\n",
            "Epoch 1978/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1099.3668 - val_loss: 1102.3823\n",
            "Epoch 1979/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1098.8173 - val_loss: 1102.0023\n",
            "Epoch 1980/2000\n",
            "157/157 [==============================] - 0s 279us/step - loss: 1100.0962 - val_loss: 1101.3372\n",
            "Epoch 1981/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1100.1269 - val_loss: 1102.3716\n",
            "Epoch 1982/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.7199 - val_loss: 1101.0427\n",
            "Epoch 1983/2000\n",
            "157/157 [==============================] - 0s 269us/step - loss: 1099.5913 - val_loss: 1101.8236\n",
            "Epoch 1984/2000\n",
            "157/157 [==============================] - 0s 288us/step - loss: 1099.4679 - val_loss: 1100.8511\n",
            "Epoch 1985/2000\n",
            "157/157 [==============================] - 0s 272us/step - loss: 1099.4132 - val_loss: 1100.8521\n",
            "Epoch 1986/2000\n",
            "157/157 [==============================] - 0s 303us/step - loss: 1099.8122 - val_loss: 1100.9850\n",
            "Epoch 1987/2000\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1099.9600 - val_loss: 1100.9891\n",
            "Epoch 1988/2000\n",
            "157/157 [==============================] - 0s 308us/step - loss: 1098.8800 - val_loss: 1101.1621\n",
            "Epoch 1989/2000\n",
            "157/157 [==============================] - 0s 319us/step - loss: 1099.8836 - val_loss: 1100.7014\n",
            "Epoch 1990/2000\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1099.5205 - val_loss: 1100.9185\n",
            "Epoch 1991/2000\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1099.6328 - val_loss: 1101.9059\n",
            "Epoch 1992/2000\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1099.1840 - val_loss: 1101.4307\n",
            "Epoch 1993/2000\n",
            "157/157 [==============================] - 0s 270us/step - loss: 1099.7518 - val_loss: 1101.2706\n",
            "Epoch 1994/2000\n",
            "157/157 [==============================] - 0s 281us/step - loss: 1099.4385 - val_loss: 1100.7296\n",
            "Epoch 1995/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1099.7684 - val_loss: 1100.9818\n",
            "Epoch 1996/2000\n",
            "157/157 [==============================] - 0s 278us/step - loss: 1099.0241 - val_loss: 1102.3462\n",
            "Epoch 1997/2000\n",
            "157/157 [==============================] - 0s 308us/step - loss: 1098.9775 - val_loss: 1100.6854\n",
            "Epoch 1998/2000\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1098.4121 - val_loss: 1100.9420\n",
            "Epoch 1999/2000\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1099.4298 - val_loss: 1099.8433\n",
            "Epoch 2000/2000\n",
            "157/157 [==============================] - 0s 335us/step - loss: 1099.1673 - val_loss: 1101.3529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZfkDSDXNfPff",
        "outputId": "d539484e-ccbb-4263-a807-0048b153768e"
      },
      "source": [
        "vae.history_df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>val_loss</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1155.657227</td>\n",
              "      <td>1157.192552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1157.885986</td>\n",
              "      <td>1155.101569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1155.747803</td>\n",
              "      <td>1153.134459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1159.093262</td>\n",
              "      <td>1151.287608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1200.149536</td>\n",
              "      <td>1148.969648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1102.346191</td>\n",
              "      <td>1099.024079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1100.685425</td>\n",
              "      <td>1098.977527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1100.942017</td>\n",
              "      <td>1098.412143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1099.843262</td>\n",
              "      <td>1099.429829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1101.352905</td>\n",
              "      <td>1099.167344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         val_loss         loss\n",
              "0     1155.657227  1157.192552\n",
              "1     1157.885986  1155.101569\n",
              "2     1155.747803  1153.134459\n",
              "3     1159.093262  1151.287608\n",
              "4     1200.149536  1148.969648\n",
              "...           ...          ...\n",
              "1995  1102.346191  1099.024079\n",
              "1996  1100.685425  1098.977527\n",
              "1997  1100.942017  1098.412143\n",
              "1998  1099.843262  1099.429829\n",
              "1999  1101.352905  1099.167344\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "SJwKs5ASdT2v",
        "outputId": "247cf40e-8d6c-444c-efa8-3d7409d653fb"
      },
      "source": [
        "plt.figure()\n",
        "ax = vae.history_df.plot()\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('VAE Loss')\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"hist_plot_file.png\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRcI5vqkfcM3",
        "outputId": "73702cee-024f-4067-aeba-7ed58af9a5f3"
      },
      "source": [
        "vae = Tybalt(original_dim, latent_dim, batch_size=50, epochs=500,\n",
        "                 learning_rate=0.005, kappa=0.5, epsilon_std=1.0,\n",
        "                 beta=K.variable(0), loss='binary_crossentropy',\n",
        "                 verbose=True)\n",
        "vae._build_encoder_layer()\n",
        "vae._build_decoder_layer()\n",
        "vae._compile_vae()\n",
        "vae._connect_layers()\n",
        "\n",
        "vae.train_vae(pcos_train_df, pcos_test_df, separate_loss=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_4:0' shape=() dtype=float32> beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training_utils.py:819: UserWarning: Output variational_layer_4 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to variational_layer_4.\n",
            "  'be expecting any data to be passed to {0}.'.format(name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 157 samples, validate on 18 samples\n",
            "Epoch 1/500\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 1156.9131 - val_loss: 1156.5286\n",
            "Epoch 2/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1155.2038 - val_loss: 1155.8087\n",
            "Epoch 3/500\n",
            "157/157 [==============================] - 0s 165us/step - loss: 1152.9024 - val_loss: 1151.2563\n",
            "Epoch 4/500\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1151.0285 - val_loss: 1151.6532\n",
            "Epoch 5/500\n",
            "157/157 [==============================] - 0s 166us/step - loss: 1149.2049 - val_loss: 1153.6174\n",
            "Epoch 6/500\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1148.1022 - val_loss: 1151.6736\n",
            "Epoch 7/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1146.1771 - val_loss: 1156.8120\n",
            "Epoch 8/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1144.5959 - val_loss: 1162.2717\n",
            "Epoch 9/500\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1142.5299 - val_loss: 1154.1437\n",
            "Epoch 10/500\n",
            "157/157 [==============================] - 0s 160us/step - loss: 1141.3479 - val_loss: 1161.1826\n",
            "Epoch 11/500\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1140.3816 - val_loss: 1158.4425\n",
            "Epoch 12/500\n",
            "157/157 [==============================] - 0s 164us/step - loss: 1138.4777 - val_loss: 1188.0034\n",
            "Epoch 13/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1137.2082 - val_loss: 1152.9150\n",
            "Epoch 14/500\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1134.4721 - val_loss: 1160.2119\n",
            "Epoch 15/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1134.3980 - val_loss: 1150.0835\n",
            "Epoch 16/500\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1133.4684 - val_loss: 1147.0922\n",
            "Epoch 17/500\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1133.9423 - val_loss: 1148.2687\n",
            "Epoch 18/500\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1132.7133 - val_loss: 1148.3103\n",
            "Epoch 19/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1133.5103 - val_loss: 1160.5161\n",
            "Epoch 20/500\n",
            "157/157 [==============================] - 0s 165us/step - loss: 1127.9203 - val_loss: 1149.4797\n",
            "Epoch 21/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1126.9373 - val_loss: 1146.2275\n",
            "Epoch 22/500\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1127.1404 - val_loss: 1137.0897\n",
            "Epoch 23/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1126.6193 - val_loss: 1137.7806\n",
            "Epoch 24/500\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1125.1154 - val_loss: 1133.2112\n",
            "Epoch 25/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1125.9737 - val_loss: 1132.1866\n",
            "Epoch 26/500\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1125.7634 - val_loss: 1126.4202\n",
            "Epoch 27/500\n",
            "157/157 [==============================] - 0s 161us/step - loss: 1124.6499 - val_loss: 1138.4841\n",
            "Epoch 28/500\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1123.1347 - val_loss: 1137.6074\n",
            "Epoch 29/500\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1122.5269 - val_loss: 1148.9601\n",
            "Epoch 30/500\n",
            "157/157 [==============================] - 0s 151us/step - loss: 1122.1477 - val_loss: 1136.7280\n",
            "Epoch 31/500\n",
            "157/157 [==============================] - 0s 168us/step - loss: 1121.9157 - val_loss: 1146.0388\n",
            "Epoch 32/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1121.4272 - val_loss: 1143.2881\n",
            "Epoch 33/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1120.2462 - val_loss: 1122.0938\n",
            "Epoch 34/500\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1122.6976 - val_loss: 1133.0376\n",
            "Epoch 35/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1121.1803 - val_loss: 1146.4572\n",
            "Epoch 36/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1120.6512 - val_loss: 1133.6639\n",
            "Epoch 37/500\n",
            "157/157 [==============================] - 0s 166us/step - loss: 1119.1120 - val_loss: 1136.4551\n",
            "Epoch 38/500\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1118.9406 - val_loss: 1131.9510\n",
            "Epoch 39/500\n",
            "157/157 [==============================] - 0s 174us/step - loss: 1117.6043 - val_loss: 1123.4360\n",
            "Epoch 40/500\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1117.8502 - val_loss: 1127.9177\n",
            "Epoch 41/500\n",
            "157/157 [==============================] - 0s 165us/step - loss: 1117.7412 - val_loss: 1129.2349\n",
            "Epoch 42/500\n",
            "157/157 [==============================] - 0s 163us/step - loss: 1118.6119 - val_loss: 1121.1530\n",
            "Epoch 43/500\n",
            "157/157 [==============================] - 0s 169us/step - loss: 1119.5831 - val_loss: 1125.2938\n",
            "Epoch 44/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1118.1988 - val_loss: 1130.1283\n",
            "Epoch 45/500\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1120.1356 - val_loss: 1120.6836\n",
            "Epoch 46/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1115.5678 - val_loss: 1122.8331\n",
            "Epoch 47/500\n",
            "157/157 [==============================] - 0s 172us/step - loss: 1117.3834 - val_loss: 1121.8806\n",
            "Epoch 48/500\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1118.6491 - val_loss: 1127.1370\n",
            "Epoch 49/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1115.2440 - val_loss: 1122.2756\n",
            "Epoch 50/500\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1116.4690 - val_loss: 1123.1029\n",
            "Epoch 51/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1116.2441 - val_loss: 1118.0945\n",
            "Epoch 52/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1117.4785 - val_loss: 1123.9067\n",
            "Epoch 53/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1115.4492 - val_loss: 1122.4155\n",
            "Epoch 54/500\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1115.8320 - val_loss: 1125.0637\n",
            "Epoch 55/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1116.9818 - val_loss: 1122.0486\n",
            "Epoch 56/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1116.8032 - val_loss: 1119.3557\n",
            "Epoch 57/500\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1115.7685 - val_loss: 1129.7867\n",
            "Epoch 58/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1114.4645 - val_loss: 1119.8180\n",
            "Epoch 59/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1114.1557 - val_loss: 1139.5631\n",
            "Epoch 60/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1115.3649 - val_loss: 1131.3315\n",
            "Epoch 61/500\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1114.1812 - val_loss: 1129.1479\n",
            "Epoch 62/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1116.4171 - val_loss: 1120.3446\n",
            "Epoch 63/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1116.5070 - val_loss: 1119.3732\n",
            "Epoch 64/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1114.8070 - val_loss: 1130.8120\n",
            "Epoch 65/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1112.5153 - val_loss: 1128.4583\n",
            "Epoch 66/500\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1114.1087 - val_loss: 1121.7230\n",
            "Epoch 67/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1115.2007 - val_loss: 1118.4950\n",
            "Epoch 68/500\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1114.8583 - val_loss: 1120.0178\n",
            "Epoch 69/500\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1115.2234 - val_loss: 1126.4197\n",
            "Epoch 70/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1114.8088 - val_loss: 1123.5100\n",
            "Epoch 71/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1114.6782 - val_loss: 1119.8400\n",
            "Epoch 72/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1113.3101 - val_loss: 1123.9321\n",
            "Epoch 73/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1113.6232 - val_loss: 1119.0186\n",
            "Epoch 74/500\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1114.5900 - val_loss: 1112.4829\n",
            "Epoch 75/500\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1115.6407 - val_loss: 1112.8512\n",
            "Epoch 76/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1111.2078 - val_loss: 1117.1626\n",
            "Epoch 77/500\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1112.9526 - val_loss: 1119.6704\n",
            "Epoch 78/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1112.3564 - val_loss: 1127.1144\n",
            "Epoch 79/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1112.5565 - val_loss: 1119.0360\n",
            "Epoch 80/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1113.7898 - val_loss: 1118.8752\n",
            "Epoch 81/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1113.4317 - val_loss: 1121.3237\n",
            "Epoch 82/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1112.3941 - val_loss: 1119.6538\n",
            "Epoch 83/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1110.7349 - val_loss: 1116.8560\n",
            "Epoch 84/500\n",
            "157/157 [==============================] - 0s 261us/step - loss: 1111.6363 - val_loss: 1114.6124\n",
            "Epoch 85/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1112.7585 - val_loss: 1114.3784\n",
            "Epoch 86/500\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1113.0340 - val_loss: 1114.3511\n",
            "Epoch 87/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1110.0304 - val_loss: 1120.9038\n",
            "Epoch 88/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1111.6826 - val_loss: 1122.0585\n",
            "Epoch 89/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1110.2492 - val_loss: 1115.7383\n",
            "Epoch 90/500\n",
            "157/157 [==============================] - 0s 170us/step - loss: 1111.2192 - val_loss: 1120.8951\n",
            "Epoch 91/500\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1111.1030 - val_loss: 1115.9659\n",
            "Epoch 92/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1110.1562 - val_loss: 1117.8088\n",
            "Epoch 93/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1111.9767 - val_loss: 1124.7563\n",
            "Epoch 94/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1110.6208 - val_loss: 1118.8059\n",
            "Epoch 95/500\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1109.1668 - val_loss: 1113.0513\n",
            "Epoch 96/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1111.0511 - val_loss: 1113.8605\n",
            "Epoch 97/500\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1112.6031 - val_loss: 1117.6053\n",
            "Epoch 98/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1110.4078 - val_loss: 1115.8978\n",
            "Epoch 99/500\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1108.9221 - val_loss: 1118.0443\n",
            "Epoch 100/500\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1109.8182 - val_loss: 1110.8175\n",
            "Epoch 101/500\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1110.5225 - val_loss: 1113.8589\n",
            "Epoch 102/500\n",
            "157/157 [==============================] - 0s 159us/step - loss: 1110.9421 - val_loss: 1111.4120\n",
            "Epoch 103/500\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1110.7307 - val_loss: 1118.6479\n",
            "Epoch 104/500\n",
            "157/157 [==============================] - 0s 286us/step - loss: 1110.7945 - val_loss: 1115.5521\n",
            "Epoch 105/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1109.3846 - val_loss: 1109.2847\n",
            "Epoch 106/500\n",
            "157/157 [==============================] - 0s 175us/step - loss: 1109.8596 - val_loss: 1117.2698\n",
            "Epoch 107/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1108.3943 - val_loss: 1120.4626\n",
            "Epoch 108/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1111.4013 - val_loss: 1114.2563\n",
            "Epoch 109/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1108.5853 - val_loss: 1120.0991\n",
            "Epoch 110/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1108.9048 - val_loss: 1117.4230\n",
            "Epoch 111/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1108.9669 - val_loss: 1120.0322\n",
            "Epoch 112/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1109.3177 - val_loss: 1115.8865\n",
            "Epoch 113/500\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1109.1989 - val_loss: 1115.3004\n",
            "Epoch 114/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1110.3993 - val_loss: 1113.3030\n",
            "Epoch 115/500\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.8448 - val_loss: 1110.7252\n",
            "Epoch 116/500\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1112.4861 - val_loss: 1117.2780\n",
            "Epoch 117/500\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.7752 - val_loss: 1112.8086\n",
            "Epoch 118/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1110.1710 - val_loss: 1115.4474\n",
            "Epoch 119/500\n",
            "157/157 [==============================] - 0s 175us/step - loss: 1107.6506 - val_loss: 1108.0728\n",
            "Epoch 120/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1108.4568 - val_loss: 1107.0942\n",
            "Epoch 121/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1108.9702 - val_loss: 1114.7377\n",
            "Epoch 122/500\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1109.9630 - val_loss: 1114.1078\n",
            "Epoch 123/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1108.4793 - val_loss: 1107.3119\n",
            "Epoch 124/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1109.6551 - val_loss: 1110.3527\n",
            "Epoch 125/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1109.0239 - val_loss: 1109.7231\n",
            "Epoch 126/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1107.7729 - val_loss: 1111.0167\n",
            "Epoch 127/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1109.4169 - val_loss: 1107.0643\n",
            "Epoch 128/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1107.7918 - val_loss: 1110.0771\n",
            "Epoch 129/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1109.6213 - val_loss: 1107.9270\n",
            "Epoch 130/500\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1109.6709 - val_loss: 1110.4370\n",
            "Epoch 131/500\n",
            "157/157 [==============================] - 0s 179us/step - loss: 1109.1371 - val_loss: 1110.3300\n",
            "Epoch 132/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1107.6954 - val_loss: 1105.4231\n",
            "Epoch 133/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1108.6440 - val_loss: 1104.6694\n",
            "Epoch 134/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1110.4640 - val_loss: 1110.3868\n",
            "Epoch 135/500\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1107.8168 - val_loss: 1110.7168\n",
            "Epoch 136/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1109.0787 - val_loss: 1112.4104\n",
            "Epoch 137/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1108.7040 - val_loss: 1116.2720\n",
            "Epoch 138/500\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1108.7230 - val_loss: 1118.2802\n",
            "Epoch 139/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1108.9089 - val_loss: 1113.2568\n",
            "Epoch 140/500\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1109.2147 - val_loss: 1116.3484\n",
            "Epoch 141/500\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1109.1811 - val_loss: 1117.1058\n",
            "Epoch 142/500\n",
            "157/157 [==============================] - 0s 170us/step - loss: 1108.9909 - val_loss: 1113.5481\n",
            "Epoch 143/500\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1106.7383 - val_loss: 1110.2921\n",
            "Epoch 144/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1106.8015 - val_loss: 1109.7059\n",
            "Epoch 145/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1106.4095 - val_loss: 1116.1548\n",
            "Epoch 146/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1106.5644 - val_loss: 1114.1851\n",
            "Epoch 147/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1105.1787 - val_loss: 1109.6509\n",
            "Epoch 148/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1107.3079 - val_loss: 1111.0118\n",
            "Epoch 149/500\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1107.5031 - val_loss: 1110.3833\n",
            "Epoch 150/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1108.2338 - val_loss: 1111.7935\n",
            "Epoch 151/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1108.8533 - val_loss: 1110.7654\n",
            "Epoch 152/500\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1106.6571 - val_loss: 1110.5140\n",
            "Epoch 153/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1106.9747 - val_loss: 1109.9534\n",
            "Epoch 154/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1105.9131 - val_loss: 1106.8960\n",
            "Epoch 155/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1107.3350 - val_loss: 1109.2458\n",
            "Epoch 156/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1107.1049 - val_loss: 1112.3994\n",
            "Epoch 157/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1107.8234 - val_loss: 1111.7679\n",
            "Epoch 158/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1107.1754 - val_loss: 1106.7626\n",
            "Epoch 159/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1110.1856 - val_loss: 1112.0824\n",
            "Epoch 160/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1109.5262 - val_loss: 1109.9006\n",
            "Epoch 161/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1105.3686 - val_loss: 1109.6772\n",
            "Epoch 162/500\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1109.0638 - val_loss: 1107.2820\n",
            "Epoch 163/500\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1106.4061 - val_loss: 1107.1942\n",
            "Epoch 164/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1107.0618 - val_loss: 1108.1498\n",
            "Epoch 165/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1107.5914 - val_loss: 1109.7173\n",
            "Epoch 166/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1107.0766 - val_loss: 1105.0839\n",
            "Epoch 167/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1107.4595 - val_loss: 1105.8345\n",
            "Epoch 168/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1106.7959 - val_loss: 1106.4097\n",
            "Epoch 169/500\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1106.3020 - val_loss: 1108.2496\n",
            "Epoch 170/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1106.6928 - val_loss: 1107.5306\n",
            "Epoch 171/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1105.2299 - val_loss: 1106.5200\n",
            "Epoch 172/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1108.8578 - val_loss: 1108.1406\n",
            "Epoch 173/500\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1106.9826 - val_loss: 1108.4575\n",
            "Epoch 174/500\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1106.7557 - val_loss: 1108.4324\n",
            "Epoch 175/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1106.8603 - val_loss: 1106.3064\n",
            "Epoch 176/500\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1108.2992 - val_loss: 1107.4790\n",
            "Epoch 177/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1106.0163 - val_loss: 1108.6172\n",
            "Epoch 178/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1107.9599 - val_loss: 1108.4178\n",
            "Epoch 179/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1109.0533 - val_loss: 1108.1489\n",
            "Epoch 180/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1106.9849 - val_loss: 1109.4817\n",
            "Epoch 181/500\n",
            "157/157 [==============================] - 0s 173us/step - loss: 1108.2687 - val_loss: 1108.0925\n",
            "Epoch 182/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1107.1684 - val_loss: 1109.3529\n",
            "Epoch 183/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1107.1965 - val_loss: 1106.4868\n",
            "Epoch 184/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1106.9496 - val_loss: 1114.6008\n",
            "Epoch 185/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1107.6843 - val_loss: 1109.2050\n",
            "Epoch 186/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1105.8467 - val_loss: 1107.0803\n",
            "Epoch 187/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1106.7090 - val_loss: 1106.9097\n",
            "Epoch 188/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1107.1017 - val_loss: 1109.1404\n",
            "Epoch 189/500\n",
            "157/157 [==============================] - 0s 313us/step - loss: 1105.7881 - val_loss: 1106.1283\n",
            "Epoch 190/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1106.9754 - val_loss: 1115.4216\n",
            "Epoch 191/500\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1108.1761 - val_loss: 1111.4081\n",
            "Epoch 192/500\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1106.3442 - val_loss: 1112.7301\n",
            "Epoch 193/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1106.7601 - val_loss: 1112.5383\n",
            "Epoch 194/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1108.4271 - val_loss: 1109.9523\n",
            "Epoch 195/500\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1105.2103 - val_loss: 1107.4434\n",
            "Epoch 196/500\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1106.1382 - val_loss: 1111.2802\n",
            "Epoch 197/500\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1105.9038 - val_loss: 1106.3634\n",
            "Epoch 198/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1105.8353 - val_loss: 1106.5359\n",
            "Epoch 199/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1105.8505 - val_loss: 1104.4741\n",
            "Epoch 200/500\n",
            "157/157 [==============================] - 0s 169us/step - loss: 1108.0466 - val_loss: 1106.3584\n",
            "Epoch 201/500\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1106.5052 - val_loss: 1107.5807\n",
            "Epoch 202/500\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1105.6121 - val_loss: 1109.8927\n",
            "Epoch 203/500\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1108.6303 - val_loss: 1110.1641\n",
            "Epoch 204/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1107.1327 - val_loss: 1107.2798\n",
            "Epoch 205/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1107.4549 - val_loss: 1107.9731\n",
            "Epoch 206/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1106.6169 - val_loss: 1107.2734\n",
            "Epoch 207/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1105.4434 - val_loss: 1108.6443\n",
            "Epoch 208/500\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1105.7394 - val_loss: 1106.9603\n",
            "Epoch 209/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1104.9652 - val_loss: 1106.4329\n",
            "Epoch 210/500\n",
            "157/157 [==============================] - 0s 178us/step - loss: 1106.0369 - val_loss: 1112.2727\n",
            "Epoch 211/500\n",
            "157/157 [==============================] - 0s 167us/step - loss: 1105.8913 - val_loss: 1107.2852\n",
            "Epoch 212/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1104.7829 - val_loss: 1108.9136\n",
            "Epoch 213/500\n",
            "157/157 [==============================] - 0s 196us/step - loss: 1105.4039 - val_loss: 1108.1920\n",
            "Epoch 214/500\n",
            "157/157 [==============================] - 0s 198us/step - loss: 1104.9136 - val_loss: 1107.7025\n",
            "Epoch 215/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1105.3016 - val_loss: 1107.8462\n",
            "Epoch 216/500\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1106.4432 - val_loss: 1105.2632\n",
            "Epoch 217/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1106.0920 - val_loss: 1104.8973\n",
            "Epoch 218/500\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1107.3357 - val_loss: 1105.8915\n",
            "Epoch 219/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1107.3075 - val_loss: 1103.7034\n",
            "Epoch 220/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1105.6393 - val_loss: 1105.2551\n",
            "Epoch 221/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1106.2900 - val_loss: 1103.7595\n",
            "Epoch 222/500\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1105.6756 - val_loss: 1105.6400\n",
            "Epoch 223/500\n",
            "157/157 [==============================] - 0s 318us/step - loss: 1105.7102 - val_loss: 1103.4451\n",
            "Epoch 224/500\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1106.0989 - val_loss: 1106.5033\n",
            "Epoch 225/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1105.1983 - val_loss: 1105.2681\n",
            "Epoch 226/500\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1104.7223 - val_loss: 1105.3392\n",
            "Epoch 227/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1106.4044 - val_loss: 1104.3903\n",
            "Epoch 228/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1105.3908 - val_loss: 1105.6595\n",
            "Epoch 229/500\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1105.9204 - val_loss: 1106.1201\n",
            "Epoch 230/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1105.8604 - val_loss: 1106.7904\n",
            "Epoch 231/500\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1104.5653 - val_loss: 1103.3032\n",
            "Epoch 232/500\n",
            "157/157 [==============================] - 0s 207us/step - loss: 1107.3874 - val_loss: 1104.7665\n",
            "Epoch 233/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1105.2324 - val_loss: 1110.0951\n",
            "Epoch 234/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1104.2681 - val_loss: 1106.5380\n",
            "Epoch 235/500\n",
            "157/157 [==============================] - 0s 176us/step - loss: 1105.9651 - val_loss: 1102.0941\n",
            "Epoch 236/500\n",
            "157/157 [==============================] - 0s 182us/step - loss: 1104.7894 - val_loss: 1104.8362\n",
            "Epoch 237/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1103.5001 - val_loss: 1106.5193\n",
            "Epoch 238/500\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1105.2110 - val_loss: 1105.7604\n",
            "Epoch 239/500\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1104.2339 - val_loss: 1103.1023\n",
            "Epoch 240/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1103.5571 - val_loss: 1107.3390\n",
            "Epoch 241/500\n",
            "157/157 [==============================] - 0s 186us/step - loss: 1104.6166 - val_loss: 1104.1169\n",
            "Epoch 242/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1105.1842 - val_loss: 1105.3698\n",
            "Epoch 243/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.6694 - val_loss: 1105.2251\n",
            "Epoch 244/500\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1106.3005 - val_loss: 1108.4554\n",
            "Epoch 245/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1105.3573 - val_loss: 1108.8092\n",
            "Epoch 246/500\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1104.9593 - val_loss: 1106.4974\n",
            "Epoch 247/500\n",
            "157/157 [==============================] - 0s 189us/step - loss: 1105.8095 - val_loss: 1105.9713\n",
            "Epoch 248/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1105.7368 - val_loss: 1107.7482\n",
            "Epoch 249/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1105.3618 - val_loss: 1106.7926\n",
            "Epoch 250/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1103.8237 - val_loss: 1108.5123\n",
            "Epoch 251/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1104.8147 - val_loss: 1107.6416\n",
            "Epoch 252/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1105.0418 - val_loss: 1106.4026\n",
            "Epoch 253/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1105.4145 - val_loss: 1107.5894\n",
            "Epoch 254/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1105.4875 - val_loss: 1104.6647\n",
            "Epoch 255/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1105.1060 - val_loss: 1105.7994\n",
            "Epoch 256/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1105.6721 - val_loss: 1104.3862\n",
            "Epoch 257/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1105.8633 - val_loss: 1102.1782\n",
            "Epoch 258/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1106.8071 - val_loss: 1105.6052\n",
            "Epoch 259/500\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1104.3603 - val_loss: 1106.3212\n",
            "Epoch 260/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1104.1928 - val_loss: 1104.7710\n",
            "Epoch 261/500\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1105.7266 - val_loss: 1107.2473\n",
            "Epoch 262/500\n",
            "157/157 [==============================] - 0s 220us/step - loss: 1103.1925 - val_loss: 1105.9885\n",
            "Epoch 263/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1105.3383 - val_loss: 1103.7452\n",
            "Epoch 264/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1102.7269 - val_loss: 1107.3259\n",
            "Epoch 265/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1103.6212 - val_loss: 1103.5171\n",
            "Epoch 266/500\n",
            "157/157 [==============================] - 0s 185us/step - loss: 1104.7334 - val_loss: 1105.3501\n",
            "Epoch 267/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1104.7007 - val_loss: 1105.9509\n",
            "Epoch 268/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1102.2696 - val_loss: 1106.3765\n",
            "Epoch 269/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1103.8982 - val_loss: 1105.6699\n",
            "Epoch 270/500\n",
            "157/157 [==============================] - 0s 164us/step - loss: 1104.6107 - val_loss: 1107.0204\n",
            "Epoch 271/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1104.7748 - val_loss: 1108.3164\n",
            "Epoch 272/500\n",
            "157/157 [==============================] - 0s 192us/step - loss: 1106.1061 - val_loss: 1108.7457\n",
            "Epoch 273/500\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1106.3297 - val_loss: 1110.6135\n",
            "Epoch 274/500\n",
            "157/157 [==============================] - 0s 202us/step - loss: 1104.0120 - val_loss: 1109.8491\n",
            "Epoch 275/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1104.6776 - val_loss: 1110.8082\n",
            "Epoch 276/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1104.4204 - val_loss: 1106.2374\n",
            "Epoch 277/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1104.1695 - val_loss: 1109.5106\n",
            "Epoch 278/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1103.7096 - val_loss: 1106.8040\n",
            "Epoch 279/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.4523 - val_loss: 1109.5768\n",
            "Epoch 280/500\n",
            "157/157 [==============================] - 0s 194us/step - loss: 1104.2432 - val_loss: 1110.3308\n",
            "Epoch 281/500\n",
            "157/157 [==============================] - 0s 180us/step - loss: 1104.0708 - val_loss: 1107.1421\n",
            "Epoch 282/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1104.5735 - val_loss: 1105.3159\n",
            "Epoch 283/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1104.5105 - val_loss: 1106.3901\n",
            "Epoch 284/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1104.1410 - val_loss: 1105.7104\n",
            "Epoch 285/500\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1103.8924 - val_loss: 1107.0293\n",
            "Epoch 286/500\n",
            "157/157 [==============================] - 0s 221us/step - loss: 1103.3133 - val_loss: 1103.6244\n",
            "Epoch 287/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1104.1687 - val_loss: 1105.0219\n",
            "Epoch 288/500\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1103.7904 - val_loss: 1103.1509\n",
            "Epoch 289/500\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1104.5841 - val_loss: 1106.6875\n",
            "Epoch 290/500\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1103.8588 - val_loss: 1107.7800\n",
            "Epoch 291/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1103.1205 - val_loss: 1107.5293\n",
            "Epoch 292/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1103.3021 - val_loss: 1102.7979\n",
            "Epoch 293/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1105.1156 - val_loss: 1105.3641\n",
            "Epoch 294/500\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1103.7331 - val_loss: 1104.8956\n",
            "Epoch 295/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1103.7638 - val_loss: 1106.3103\n",
            "Epoch 296/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1105.1291 - val_loss: 1105.6105\n",
            "Epoch 297/500\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1104.0611 - val_loss: 1106.0497\n",
            "Epoch 298/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.4816 - val_loss: 1106.1666\n",
            "Epoch 299/500\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1104.7091 - val_loss: 1108.5011\n",
            "Epoch 300/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1105.1954 - val_loss: 1105.1189\n",
            "Epoch 301/500\n",
            "157/157 [==============================] - 0s 217us/step - loss: 1103.9438 - val_loss: 1106.1042\n",
            "Epoch 302/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.7822 - val_loss: 1103.8149\n",
            "Epoch 303/500\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1104.0420 - val_loss: 1103.1525\n",
            "Epoch 304/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.5913 - val_loss: 1104.3374\n",
            "Epoch 305/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1103.4841 - val_loss: 1104.6844\n",
            "Epoch 306/500\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1105.0014 - val_loss: 1104.9636\n",
            "Epoch 307/500\n",
            "157/157 [==============================] - 0s 184us/step - loss: 1104.5588 - val_loss: 1103.1719\n",
            "Epoch 308/500\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1106.6592 - val_loss: 1102.5939\n",
            "Epoch 309/500\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1104.0018 - val_loss: 1103.1002\n",
            "Epoch 310/500\n",
            "157/157 [==============================] - 0s 187us/step - loss: 1102.5373 - val_loss: 1103.3386\n",
            "Epoch 311/500\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.5328 - val_loss: 1104.4147\n",
            "Epoch 312/500\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1103.9323 - val_loss: 1105.4701\n",
            "Epoch 313/500\n",
            "157/157 [==============================] - 0s 280us/step - loss: 1103.5748 - val_loss: 1102.2324\n",
            "Epoch 314/500\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1103.8392 - val_loss: 1106.1354\n",
            "Epoch 315/500\n",
            "157/157 [==============================] - 0s 177us/step - loss: 1104.7513 - val_loss: 1104.5590\n",
            "Epoch 316/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1102.1537 - val_loss: 1105.2275\n",
            "Epoch 317/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1103.4329 - val_loss: 1104.4939\n",
            "Epoch 318/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1103.1530 - val_loss: 1106.4370\n",
            "Epoch 319/500\n",
            "157/157 [==============================] - 0s 206us/step - loss: 1103.7421 - val_loss: 1104.1370\n",
            "Epoch 320/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1105.1320 - val_loss: 1106.0533\n",
            "Epoch 321/500\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1103.2480 - val_loss: 1104.7014\n",
            "Epoch 322/500\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1103.7544 - val_loss: 1106.3331\n",
            "Epoch 323/500\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1104.1246 - val_loss: 1105.5383\n",
            "Epoch 324/500\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1104.2420 - val_loss: 1106.8568\n",
            "Epoch 325/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1103.9370 - val_loss: 1107.6703\n",
            "Epoch 326/500\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1103.1018 - val_loss: 1108.0006\n",
            "Epoch 327/500\n",
            "157/157 [==============================] - 0s 227us/step - loss: 1103.3534 - val_loss: 1108.0490\n",
            "Epoch 328/500\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1102.5993 - val_loss: 1106.1448\n",
            "Epoch 329/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1104.3249 - val_loss: 1109.0145\n",
            "Epoch 330/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1105.9479 - val_loss: 1104.7108\n",
            "Epoch 331/500\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1104.5288 - val_loss: 1105.8118\n",
            "Epoch 332/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1103.1569 - val_loss: 1104.8389\n",
            "Epoch 333/500\n",
            "157/157 [==============================] - 0s 188us/step - loss: 1103.5868 - val_loss: 1106.0156\n",
            "Epoch 334/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1103.1975 - val_loss: 1107.0409\n",
            "Epoch 335/500\n",
            "157/157 [==============================] - 0s 219us/step - loss: 1103.3179 - val_loss: 1107.5050\n",
            "Epoch 336/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1104.7970 - val_loss: 1108.5736\n",
            "Epoch 337/500\n",
            "157/157 [==============================] - 0s 190us/step - loss: 1104.2503 - val_loss: 1107.6046\n",
            "Epoch 338/500\n",
            "157/157 [==============================] - 0s 184us/step - loss: 1102.5676 - val_loss: 1105.3062\n",
            "Epoch 339/500\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1103.4189 - val_loss: 1102.4048\n",
            "Epoch 340/500\n",
            "157/157 [==============================] - 0s 224us/step - loss: 1102.5860 - val_loss: 1105.2628\n",
            "Epoch 341/500\n",
            "157/157 [==============================] - 0s 195us/step - loss: 1102.7926 - val_loss: 1107.8269\n",
            "Epoch 342/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1105.1434 - val_loss: 1106.8910\n",
            "Epoch 343/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1103.5016 - val_loss: 1104.2007\n",
            "Epoch 344/500\n",
            "157/157 [==============================] - 0s 183us/step - loss: 1102.3069 - val_loss: 1105.1390\n",
            "Epoch 345/500\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1103.4742 - val_loss: 1104.3435\n",
            "Epoch 346/500\n",
            "157/157 [==============================] - 0s 181us/step - loss: 1103.6098 - val_loss: 1104.9966\n",
            "Epoch 347/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1101.9850 - val_loss: 1105.6848\n",
            "Epoch 348/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1104.3570 - val_loss: 1107.6052\n",
            "Epoch 349/500\n",
            "157/157 [==============================] - 0s 204us/step - loss: 1103.4433 - val_loss: 1106.2196\n",
            "Epoch 350/500\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1103.0541 - val_loss: 1107.2462\n",
            "Epoch 351/500\n",
            "157/157 [==============================] - 0s 304us/step - loss: 1101.3848 - val_loss: 1106.1283\n",
            "Epoch 352/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1102.7322 - val_loss: 1105.6610\n",
            "Epoch 353/500\n",
            "157/157 [==============================] - 0s 238us/step - loss: 1103.8063 - val_loss: 1103.6964\n",
            "Epoch 354/500\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.9878 - val_loss: 1105.2695\n",
            "Epoch 355/500\n",
            "157/157 [==============================] - 0s 212us/step - loss: 1104.5509 - val_loss: 1106.2611\n",
            "Epoch 356/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1103.0916 - val_loss: 1107.7994\n",
            "Epoch 357/500\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1103.8373 - val_loss: 1104.7616\n",
            "Epoch 358/500\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1103.2930 - val_loss: 1104.6823\n",
            "Epoch 359/500\n",
            "157/157 [==============================] - 0s 223us/step - loss: 1103.0066 - val_loss: 1108.0961\n",
            "Epoch 360/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1104.6781 - val_loss: 1105.3616\n",
            "Epoch 361/500\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1103.1201 - val_loss: 1104.4086\n",
            "Epoch 362/500\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1102.9051 - val_loss: 1105.8990\n",
            "Epoch 363/500\n",
            "157/157 [==============================] - 0s 293us/step - loss: 1102.5412 - val_loss: 1103.7845\n",
            "Epoch 364/500\n",
            "157/157 [==============================] - 0s 205us/step - loss: 1104.9504 - val_loss: 1104.2012\n",
            "Epoch 365/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1103.8478 - val_loss: 1103.9026\n",
            "Epoch 366/500\n",
            "157/157 [==============================] - 0s 257us/step - loss: 1104.2646 - val_loss: 1104.9980\n",
            "Epoch 367/500\n",
            "157/157 [==============================] - 0s 248us/step - loss: 1102.6769 - val_loss: 1104.7251\n",
            "Epoch 368/500\n",
            "157/157 [==============================] - 0s 291us/step - loss: 1102.9016 - val_loss: 1102.2037\n",
            "Epoch 369/500\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.1915 - val_loss: 1102.6261\n",
            "Epoch 370/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1103.3663 - val_loss: 1104.2134\n",
            "Epoch 371/500\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1102.7681 - val_loss: 1106.3079\n",
            "Epoch 372/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1105.7213 - val_loss: 1109.5103\n",
            "Epoch 373/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.9530 - val_loss: 1107.4996\n",
            "Epoch 374/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1104.5072 - val_loss: 1105.4652\n",
            "Epoch 375/500\n",
            "157/157 [==============================] - 0s 267us/step - loss: 1103.3199 - val_loss: 1108.3240\n",
            "Epoch 376/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.8636 - val_loss: 1106.4463\n",
            "Epoch 377/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1103.3046 - val_loss: 1106.6118\n",
            "Epoch 378/500\n",
            "157/157 [==============================] - 0s 199us/step - loss: 1103.2396 - val_loss: 1106.4156\n",
            "Epoch 379/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1103.0016 - val_loss: 1106.1350\n",
            "Epoch 380/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1102.7314 - val_loss: 1108.0343\n",
            "Epoch 381/500\n",
            "157/157 [==============================] - 0s 262us/step - loss: 1103.1136 - val_loss: 1104.7859\n",
            "Epoch 382/500\n",
            "157/157 [==============================] - 0s 298us/step - loss: 1106.1002 - val_loss: 1108.7999\n",
            "Epoch 383/500\n",
            "157/157 [==============================] - 0s 246us/step - loss: 1103.9990 - val_loss: 1105.4741\n",
            "Epoch 384/500\n",
            "157/157 [==============================] - 0s 264us/step - loss: 1102.4218 - val_loss: 1107.6257\n",
            "Epoch 385/500\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.7469 - val_loss: 1106.1265\n",
            "Epoch 386/500\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.7307 - val_loss: 1107.9407\n",
            "Epoch 387/500\n",
            "157/157 [==============================] - 0s 236us/step - loss: 1103.3830 - val_loss: 1109.3951\n",
            "Epoch 388/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1102.3190 - val_loss: 1108.0747\n",
            "Epoch 389/500\n",
            "157/157 [==============================] - 0s 193us/step - loss: 1105.1700 - val_loss: 1106.9974\n",
            "Epoch 390/500\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1103.7773 - val_loss: 1104.1781\n",
            "Epoch 391/500\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1103.4345 - val_loss: 1104.5596\n",
            "Epoch 392/500\n",
            "157/157 [==============================] - 0s 239us/step - loss: 1103.6091 - val_loss: 1107.0554\n",
            "Epoch 393/500\n",
            "157/157 [==============================] - 0s 210us/step - loss: 1103.0575 - val_loss: 1106.3628\n",
            "Epoch 394/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1102.8317 - val_loss: 1103.9548\n",
            "Epoch 395/500\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1102.7418 - val_loss: 1105.3169\n",
            "Epoch 396/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1102.1594 - val_loss: 1105.5789\n",
            "Epoch 397/500\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1102.9664 - val_loss: 1103.7343\n",
            "Epoch 398/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.6743 - val_loss: 1103.9088\n",
            "Epoch 399/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1103.7370 - val_loss: 1106.8578\n",
            "Epoch 400/500\n",
            "157/157 [==============================] - 0s 260us/step - loss: 1102.4487 - val_loss: 1106.1018\n",
            "Epoch 401/500\n",
            "157/157 [==============================] - 0s 200us/step - loss: 1104.1175 - val_loss: 1106.2384\n",
            "Epoch 402/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.2091 - val_loss: 1105.4971\n",
            "Epoch 403/500\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.3110 - val_loss: 1103.8225\n",
            "Epoch 404/500\n",
            "157/157 [==============================] - 0s 233us/step - loss: 1102.5185 - val_loss: 1104.6462\n",
            "Epoch 405/500\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1101.8878 - val_loss: 1102.9749\n",
            "Epoch 406/500\n",
            "157/157 [==============================] - 0s 213us/step - loss: 1103.1006 - val_loss: 1104.1919\n",
            "Epoch 407/500\n",
            "157/157 [==============================] - 0s 271us/step - loss: 1102.2666 - val_loss: 1105.1783\n",
            "Epoch 408/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1103.7856 - val_loss: 1103.9425\n",
            "Epoch 409/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1102.1920 - val_loss: 1105.5337\n",
            "Epoch 410/500\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1103.1201 - val_loss: 1103.9236\n",
            "Epoch 411/500\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.9834 - val_loss: 1104.7385\n",
            "Epoch 412/500\n",
            "157/157 [==============================] - 0s 289us/step - loss: 1103.4649 - val_loss: 1103.8505\n",
            "Epoch 413/500\n",
            "157/157 [==============================] - 0s 242us/step - loss: 1101.9212 - val_loss: 1106.4270\n",
            "Epoch 414/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1104.5170 - val_loss: 1104.7179\n",
            "Epoch 415/500\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1104.3075 - val_loss: 1103.8140\n",
            "Epoch 416/500\n",
            "157/157 [==============================] - 0s 231us/step - loss: 1102.5042 - val_loss: 1103.3944\n",
            "Epoch 417/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1104.3381 - val_loss: 1103.0342\n",
            "Epoch 418/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1101.6962 - val_loss: 1101.9424\n",
            "Epoch 419/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1102.7337 - val_loss: 1104.3472\n",
            "Epoch 420/500\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1103.4927 - val_loss: 1105.6281\n",
            "Epoch 421/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1104.3573 - val_loss: 1108.9813\n",
            "Epoch 422/500\n",
            "157/157 [==============================] - 0s 277us/step - loss: 1102.8964 - val_loss: 1104.9091\n",
            "Epoch 423/500\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1103.4891 - val_loss: 1103.2026\n",
            "Epoch 424/500\n",
            "157/157 [==============================] - 0s 321us/step - loss: 1103.2744 - val_loss: 1101.3605\n",
            "Epoch 425/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1101.7983 - val_loss: 1106.2637\n",
            "Epoch 426/500\n",
            "157/157 [==============================] - 0s 222us/step - loss: 1102.0942 - val_loss: 1104.6296\n",
            "Epoch 427/500\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1103.6245 - val_loss: 1104.7006\n",
            "Epoch 428/500\n",
            "157/157 [==============================] - 0s 228us/step - loss: 1102.2139 - val_loss: 1103.4167\n",
            "Epoch 429/500\n",
            "157/157 [==============================] - 0s 215us/step - loss: 1101.4252 - val_loss: 1104.8994\n",
            "Epoch 430/500\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1101.3176 - val_loss: 1102.6862\n",
            "Epoch 431/500\n",
            "157/157 [==============================] - 0s 208us/step - loss: 1102.7105 - val_loss: 1107.7120\n",
            "Epoch 432/500\n",
            "157/157 [==============================] - 0s 265us/step - loss: 1102.7255 - val_loss: 1108.3401\n",
            "Epoch 433/500\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1102.5114 - val_loss: 1104.0048\n",
            "Epoch 434/500\n",
            "157/157 [==============================] - 0s 234us/step - loss: 1102.3684 - val_loss: 1106.4207\n",
            "Epoch 435/500\n",
            "157/157 [==============================] - 0s 266us/step - loss: 1103.1032 - val_loss: 1102.2634\n",
            "Epoch 436/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1101.9473 - val_loss: 1104.4790\n",
            "Epoch 437/500\n",
            "157/157 [==============================] - 0s 254us/step - loss: 1102.9153 - val_loss: 1105.9106\n",
            "Epoch 438/500\n",
            "157/157 [==============================] - 0s 263us/step - loss: 1101.5903 - val_loss: 1105.6895\n",
            "Epoch 439/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.6286 - val_loss: 1105.9128\n",
            "Epoch 440/500\n",
            "157/157 [==============================] - 0s 255us/step - loss: 1102.6676 - val_loss: 1105.1820\n",
            "Epoch 441/500\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1103.2153 - val_loss: 1109.7632\n",
            "Epoch 442/500\n",
            "157/157 [==============================] - 0s 256us/step - loss: 1102.7317 - val_loss: 1109.7581\n",
            "Epoch 443/500\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1102.6823 - val_loss: 1107.4768\n",
            "Epoch 444/500\n",
            "157/157 [==============================] - 0s 201us/step - loss: 1102.6156 - val_loss: 1106.0002\n",
            "Epoch 445/500\n",
            "157/157 [==============================] - 0s 191us/step - loss: 1102.6070 - val_loss: 1105.3926\n",
            "Epoch 446/500\n",
            "157/157 [==============================] - 0s 203us/step - loss: 1102.0788 - val_loss: 1105.4790\n",
            "Epoch 447/500\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.8668 - val_loss: 1107.8217\n",
            "Epoch 448/500\n",
            "157/157 [==============================] - 0s 282us/step - loss: 1102.6374 - val_loss: 1105.0874\n",
            "Epoch 449/500\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1103.1108 - val_loss: 1105.8595\n",
            "Epoch 450/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1102.3795 - val_loss: 1105.7749\n",
            "Epoch 451/500\n",
            "157/157 [==============================] - 0s 243us/step - loss: 1101.1733 - val_loss: 1105.9457\n",
            "Epoch 452/500\n",
            "157/157 [==============================] - 0s 252us/step - loss: 1103.2170 - val_loss: 1104.0288\n",
            "Epoch 453/500\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1102.2162 - val_loss: 1103.7968\n",
            "Epoch 454/500\n",
            "157/157 [==============================] - 0s 197us/step - loss: 1101.6078 - val_loss: 1103.9225\n",
            "Epoch 455/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1103.0819 - val_loss: 1104.7471\n",
            "Epoch 456/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1101.8888 - val_loss: 1107.8220\n",
            "Epoch 457/500\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1102.1911 - val_loss: 1104.7000\n",
            "Epoch 458/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1103.0754 - val_loss: 1107.3169\n",
            "Epoch 459/500\n",
            "157/157 [==============================] - 0s 249us/step - loss: 1102.7396 - val_loss: 1109.6682\n",
            "Epoch 460/500\n",
            "157/157 [==============================] - 0s 235us/step - loss: 1101.6512 - val_loss: 1106.3668\n",
            "Epoch 461/500\n",
            "157/157 [==============================] - 0s 245us/step - loss: 1102.1761 - val_loss: 1107.6316\n",
            "Epoch 462/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1102.5704 - val_loss: 1105.8173\n",
            "Epoch 463/500\n",
            "157/157 [==============================] - 0s 268us/step - loss: 1104.1249 - val_loss: 1106.0703\n",
            "Epoch 464/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1102.1446 - val_loss: 1109.1514\n",
            "Epoch 465/500\n",
            "157/157 [==============================] - 0s 273us/step - loss: 1102.5728 - val_loss: 1107.2332\n",
            "Epoch 466/500\n",
            "157/157 [==============================] - 0s 211us/step - loss: 1102.3940 - val_loss: 1105.9612\n",
            "Epoch 467/500\n",
            "157/157 [==============================] - 0s 253us/step - loss: 1102.0927 - val_loss: 1107.8796\n",
            "Epoch 468/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1102.0514 - val_loss: 1106.3063\n",
            "Epoch 469/500\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1103.6801 - val_loss: 1104.8292\n",
            "Epoch 470/500\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1102.5027 - val_loss: 1104.1541\n",
            "Epoch 471/500\n",
            "157/157 [==============================] - 0s 251us/step - loss: 1103.4940 - val_loss: 1103.3306\n",
            "Epoch 472/500\n",
            "157/157 [==============================] - 0s 323us/step - loss: 1103.0648 - val_loss: 1105.7664\n",
            "Epoch 473/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1102.3870 - val_loss: 1104.1510\n",
            "Epoch 474/500\n",
            "157/157 [==============================] - 0s 229us/step - loss: 1102.2112 - val_loss: 1102.4586\n",
            "Epoch 475/500\n",
            "157/157 [==============================] - 0s 276us/step - loss: 1101.4512 - val_loss: 1101.7162\n",
            "Epoch 476/500\n",
            "157/157 [==============================] - 0s 240us/step - loss: 1103.0719 - val_loss: 1103.4548\n",
            "Epoch 477/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1101.4636 - val_loss: 1104.0117\n",
            "Epoch 478/500\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1102.5014 - val_loss: 1104.3989\n",
            "Epoch 479/500\n",
            "157/157 [==============================] - 0s 244us/step - loss: 1102.6069 - val_loss: 1103.9829\n",
            "Epoch 480/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1104.3021 - val_loss: 1105.5521\n",
            "Epoch 481/500\n",
            "157/157 [==============================] - 0s 216us/step - loss: 1101.5555 - val_loss: 1106.3734\n",
            "Epoch 482/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1103.0117 - val_loss: 1105.5131\n",
            "Epoch 483/500\n",
            "157/157 [==============================] - 0s 237us/step - loss: 1102.5057 - val_loss: 1104.4705\n",
            "Epoch 484/500\n",
            "157/157 [==============================] - 0s 209us/step - loss: 1101.2628 - val_loss: 1107.3584\n",
            "Epoch 485/500\n",
            "157/157 [==============================] - 0s 214us/step - loss: 1103.2028 - val_loss: 1103.7836\n",
            "Epoch 486/500\n",
            "157/157 [==============================] - 0s 300us/step - loss: 1102.3858 - val_loss: 1104.5620\n",
            "Epoch 487/500\n",
            "157/157 [==============================] - 0s 287us/step - loss: 1102.5500 - val_loss: 1103.2371\n",
            "Epoch 488/500\n",
            "157/157 [==============================] - 0s 247us/step - loss: 1102.8979 - val_loss: 1105.4269\n",
            "Epoch 489/500\n",
            "157/157 [==============================] - 0s 226us/step - loss: 1104.3569 - val_loss: 1102.2032\n",
            "Epoch 490/500\n",
            "157/157 [==============================] - 0s 230us/step - loss: 1103.6110 - val_loss: 1105.9429\n",
            "Epoch 491/500\n",
            "157/157 [==============================] - 0s 324us/step - loss: 1103.1001 - val_loss: 1105.6619\n",
            "Epoch 492/500\n",
            "157/157 [==============================] - 0s 232us/step - loss: 1103.3793 - val_loss: 1105.0200\n",
            "Epoch 493/500\n",
            "157/157 [==============================] - 0s 241us/step - loss: 1102.9876 - val_loss: 1103.4084\n",
            "Epoch 494/500\n",
            "157/157 [==============================] - 0s 258us/step - loss: 1102.0438 - val_loss: 1104.9144\n",
            "Epoch 495/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1103.0872 - val_loss: 1103.2903\n",
            "Epoch 496/500\n",
            "157/157 [==============================] - 0s 283us/step - loss: 1103.6379 - val_loss: 1105.3646\n",
            "Epoch 497/500\n",
            "157/157 [==============================] - 0s 218us/step - loss: 1101.4279 - val_loss: 1104.3837\n",
            "Epoch 498/500\n",
            "157/157 [==============================] - 0s 225us/step - loss: 1102.6695 - val_loss: 1106.5625\n",
            "Epoch 499/500\n",
            "157/157 [==============================] - 0s 250us/step - loss: 1101.6119 - val_loss: 1105.5562\n",
            "Epoch 500/500\n",
            "157/157 [==============================] - 0s 275us/step - loss: 1101.7888 - val_loss: 1103.8340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "kU_PGwVEhIfv",
        "outputId": "9faa30c7-99b7-4838-e8e5-6b49caaf205e"
      },
      "source": [
        "plt.figure()\n",
        "ax = vae.history_df.plot()\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('VAE Loss')\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"hist_plot_file.png\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RZIv-_zhJDG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}